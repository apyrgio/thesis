\chapter{Design of cached}\label{ch:cached-design}

In the previous chapters, we have addressed the need for tiering in terms of
scalability as well as performance. % Which are these chapters? Link them.

We have also evaluated current caching solutions and described why they couldn't 
be used as a cache tier in Archipelago. % Provide link to a table of comparisons

With the results of chapter ? in mind, we can provide some more strict 
requirements that our solution must have:

% List the requirements for our solution
\begin{enumerate}
	\item Requirement 1
	\item Requirement 2
	\item Requirement 3
	\item Requirement 4
\end{enumerate}

The following two chapters are the main bulk of this thesis and they present our 
own implementation that aims to fill the above requirements.

% Foretell what the following sections will contain
More specifically, this chapter provides an in-depth description of the design 
of cached.  Section ?  provides a general overview of cached. Sections ? - ?  
present the building blocks of cached and their design. Section ? presents the 
interaction of cached and its building blocks. Finally, in Section ? we 
illustrate the flow of requests for cached.

\section{General}

In order to provide a caching tier for Archipelago that would blabla our 
requirements, we had to create our own implementation. Its name is simply cached 
(\textbf{cache d}aemon) and is another XSEG peer with similar structure to those 
seen in chapter ? %link to chapter

For the creation of this peer, we have created some xtypes
% Link to where we define xtypes
that act as the building blocks for this peer. These xtypes are the following:

\begin{comment}
More specifically, cached consists of the cache provided by xcache aasfs as safa
asfasfnd a
pre-allocated number of objects. An object is divided in buckets and its size,
as well as bucket size, are defined by the user.

The fact that objects are pre-allocated means two things:

1) We don't need to care about memory fragmentation and system call overhead
2) We cannot index single buckets. <FILLME>
\end{comment}

% TODO: Link List to the definitions
\begin{itemize}
	\item \textbf{xcache} (for cache support)
	\item xwork (for job support)
	\item xworkq (for atomicity in execution of jobs)
	\item xwaitq (for conditional execution of jobs)
\end{itemize}

and their design will be discussed in-depth in the following sections.

\section{The xcache xtype}

xcache is the main component of cached. It is responsible for several key 
aspects of caching such as:

\begin{itemize}
	\item object indexing
	\item coherency (?) and
	\item eviction handling
	\item reference counting
\end{itemize}

Below we can see a design overview of xcache:

% Add figure for xcache with both hash tables, xq etc.

% While explaining, point to the diagram objects, a, b, c etc.
More specifically, xcache utilizes two hash tables. One hash table is 
responsible for indexing objects (or more generally speaking "cache entries") 
that are active in cache.  The other hash table is responsible for indexing 
evicted cache entries that have pending jobs.  Again, more generally speaking, 
evicted cache entries are entries whose refcount has not dropped to zero yet.

\subsection{Indexing}

In order to index the cached objects, xcache relies on another xtype, xhash, 
which is a hash table. What's more, it's actually the C implementation of the 
dictionary used in Python.
% explain that for sparce Python dictionaries, accesses are O(1)

We have chosen to use a hash table as our index because:

% List of pros and cons of hash tables and why cons are ok in our book

Finally, the xhash xtype gives provides us with the basic hash table functions, 
namely:

\begin{itemize}
	\item Insertion
	\item Look-up
	\item Deletion
\end{itemize}

\subsection{Eviction}

When xcache has reached its maximum capacity and is requested to index a new 
entry, we have to resort to the eviction of a cached entry. But, which cache 
entry we must evict? This is a well documented problem that was first faced 
when creating hardware caches (the L1, L2 CPU caches we are familiar with). In 
1966, Lazlo Belady % Link to the paper?
proved that the best strategy is to evict the entry that is going to be used 
more later on in the future. Although our implementation is promised to be 
fast, we can't say the same for its clairvoyance properties that are needed for 
this eviction strategy. So, our implementation (as well as all other 
implementation that index and evict entries) must choose between the following, 
more down-to-the-earth eviction strategies:

% Mention ehcache approach: 
% http://ehcache.org/documentation/apis/cache-eviction-algorithms
%
% Also look at the following papers
%L. Belady, “A Study of Replacement Algorithms for a
%Virtual-Storage Computer,” IBM Systems Journal, vol.5,
%no.2, pp.78-101, 1966.
%Also, check out this paper:
%http://dent.cecs.uci.edu/~papers/esweek06/cases/p234.pdf
\begin{itemize}
	\item \textbf{Random:} Simply, a randomly chosen entry is evicted. This 
		strategy, although it seems simplistic at first, is sometimes chosen 
		due to the ease and speed of each. It is preferred in random workloads 
		where getting free space for an object is more important than the 
		object that will be evicted.
	\item \textbf{FIFO (First-In-First-Out):} The entry that was first inserted 
		will also be the first to evict. This is also a very simplistic 
		approach as well as easy and fast. Interestingly, although it would 
		seem to produce better results than Random eviction, it is rarely used 
		though, since it assumes that cache entries are used only once, which 
		is not common in real-life situations.
	\item \textbf{LRU (Least-Recently-Used)}
	\item \textbf{LFU (Least-Frequently-Used)}
\end{itemize}

We have chosen the LRU strategy. What's more, our hash table approach allows us 
to create an O(1) LRU algorithm that you can see in the following figure:

% Insert figure for O(1) LRU and hash table

Our LRU implementation uses a doubly linked list blablabla.
This design allows us to do all of the following action in constant time:

\begin{itemize}
	\item Insert a new entry to the LRU list
	\item Evict the LRU entry
	\item Update an entry's access time (i.e. mark it as MRU)
	\item Remove an arbitrary entry
\end{itemize}

Another neat feature of xcache is that Cache entry eviction is done almost 
transparently from the user.  xcache has l
two
LRU algorithms, a linear LRU array or a binary heap (more at xbinheap) that can
be chosen at runtime and are responsible for deciding which is the least
recently used entry. Users do not explicitly interact with the LRU array.
Eviction occurs automagically during the insertion of a new cache entry and the
user is informed via a specific hook that is triggered upon eviction.

\subsection{Hooks}

The hooks that xcache provides to users are:

\begin{itemize}
	\item on\_ init: called on cache entry initialization.
	\item on\_put: called when the last reference to the cache entry is put
	\item on\_evict: called when a cache entry is evicted.
	\item on\_node\_init: called on initial node preparation.
	\item post\_evict: called after an eviction has occurred, with cache  
		lock held.
	\item on\_free: called when a cache entry is freed.
	\item on\_finalize: called to hint the user that the cache entry's ref 
		has dropped to zero.
	\item on\_reinsert: called when a cache entry has been in cache
\end{itemize}

\subsection{Refcounts and locks}

The refcount model in xcache should be familiar to most people:

\begin{itemize}
	\item When an entry is inserted in cache, the cache holds a reference 
		for it (ref = 1).
	\item Whenever a new lookup for this cache entry succeeds, the reference 
		is increased by 1 (ref++)
	\item When the request that has issued the lookup has finished with an 
		entry, the reference is decreased by 1. (ref--)
	\item When a cache entry is evicted by cache, the its ref is decreased 
		by 1. (ref--)
\end{itemize}

Some common refcount cases are:

\begin{itemize}
	\item active entry with pending jobs (ref > 1)
	\item active entry with no pending jobs (ref = 1)
	\item evicted entry with pending jobs (ref > 0)
	\item evicted entry with no pending jobs (ref = 0)
\end{itemize}

and, as always, the entry is freed only when its ref = 0.

Finally, xcache uses one lock for each hash table but when a cache entry shifts
from one hash table to the other, both locks are acquired.

\subsection{Re-insertion}

In xcache, there is a concept called "re-insertion". In order for an entry to
be re-inserted to the primary hash table (which will be called "entries" from
now on) it must first reside in the hash table that indexes the evicted cache
entries (which will be called "rm\_entries" from now on). As mentioned above,
an entry that is in rm\_entries has probably pending jobs that delay its
removal.

So, what happens if a lookup arrives for that entry while on this stage? In
this case, we re-insert it to entries and increase its refcount by 2, since
there is one reference by the hash table and one reference by the one who
requested the lookup.


\section{The xworkq xtype}

Every object has a workq. Whenever a new request is accepted/received for an
object, it is enqueued in the workq and we are sure that only one thread at a
time can have access to the objects data and metadata.

For more information, see the xworkq.

\section{The xwaitq xtype}

When a thread tries to insert an object in cache but fails, due to the fact
that cache is full, the request is enqueued in the xcache waitq, which is
signaled every time an object is freed.

For more information, see the xwaitq.

\section{Cached internals}

\subsection{Object states}

Every object has a state, which is set atomically by threads. The state list is
the following:

\begin{itemize}
	\item READY: the object is ready to be used
	\item FLUSHING: the object is flushing its dirty buckets
	\item DELETING: there is a delete request that has been sent to the 
		blocker for this object
	\item INVALIDATED: the object has been deleted
	\item FAILED: something went very wrong with this object
\end{itemize}

Also, object buckets have their own states too:

\begin{itemize}
	\item INVALID: the same as empty
	\item LOADING: there is a pending read to blocker for this bucket
	\item VALID: the bucket is clean and can be read
	\item DIRTY: the bucket can be read but its contents have not been
		written to the underlying storage
	\item WRITING: there is a pending write to blocker for this bucket
\end{itemize}

Finally, for every object there are bucket state counters, which are increased/
decreased when a bucket state is changed. These counters give us an O(1)
glimpse to the bucket states of an object.

\subsection{Per-object peer requests}

Reads and writes to objects are practically read/write request from other
peers, for which a peer request has been allocated. There are cases though
when an object has to allocate its own peer request e.g. due to a flushing of
its dirty buckets. Since this must be fast, there are pre-allocated requests
hard-coded in the struct of each object which can be used in such cases.

\subsection{Write policy}

The user must define beforehand what is the write policy of cache. There are
two options: writethrough and writeback. On a side note, as far as reads and
cache misses are concerned, cached operates under a write-allocate policy.

