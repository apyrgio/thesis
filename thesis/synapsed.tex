\chapter{The synapsed peer}\label{ch:synapsed}

On the previous sections, we have evaluated the design of cached and one of the 
conclusions that we have reached at is that it has heavy lock contention when 
more than one threads are used. The implications of this, however, are bigger, 
if we consider that Archipelago is running in the host machine, whose CPU's are 
already oversubscribed. This means that cached must compete for CPU time 
against the running VMs, essentially defeating the QoS purpose it serves.

On the other hand, on the RADOS nodes, the CPUs and RAM are not used to their 
full potential. If we could run cached or part of the Archipelago in these 
nodes, we would have the following benefits:

\begin{itemize}
	\item We would have access to a much larger amount of RAM
	\item We would make a big step forward in terms of creating a 
		distributed cache, since we would be able to replicate the data 
		to two or more cached peers that would ran in different nodes, 
		since these nodes are accessible from any host.
\end{itemize}

To this end, we have created a network peer called synapsed (from 
\textbf{synapse d}aemon) as a proof-of-concept, that should be able to accept 
read/write XSEG requests and send them through network to another Archipelago 
segment. We must note that this peer, given that it is proof-of-concept, has 
not been created with high-performance in mind but its main aim is to provide 
the functionality needed for our purposes.  As a consequence, we have not used 
tools such as ZeroMQ or libevent that would boost the performance of the 
implementation.

The structure of this chapter is the following. Section 
\ref{sec:design-synapsed} presents the design of the synapsed peer. Section 
\ref{sec:imp-synapsed} explains how we implemented synapsed as well as the 
challenges that we have faced. Finally, Section \ref{sec:plot-synapsed} 
provides the results of some preliminary benchmarks that we have conducted 
using synapsed.

\section{Design of synapsed}\label{sec:design-synapsed}

Given that currently Archipelago is not network-aware, peers from one segment 
cannot know the ports of peers of another segment. Moreover, they cannot send a 
request to the synapsed peer and simultaneously target a peer in another 
segment. So, we are faced with the problem of defining the limitations of 
synapsed.

We propose the following design: Each of these two segments must have at least 
one synapsed peer. So, when one synapsed accepts an XSEG request, it will 
translate it to a network request, send it to the synapsed peer of the other 
segment, who in turn will finally translate it back to an XSEG request.  
Moreover, each synapsed peer must be attached to a peer of its segment, which 
will serve as the request target when synapsed accepts a request.  

This means that synapsed does not actually connect two remote segments. More 
appropriately, it bridges two remote peers over network.

Another difficult task was to enable synapsed to listen from its port and its 
request queue simultaneously, using the request handling of Archipelago. We 
have tackled this problem in Section \ref{sec:poll-synapsed}.

Moreover, synapsed has been designed to send requests using the standard TCP 
protocol.  This means that it must handle the marshaling of buffers, 
send/receive errors as well as polling for new requests all by itself.

Finally, synapsed has been designed as a single-threaded peer.

\section{Implementation of synapsed}\label{sec:imp-synapsed}

In this section, we will explain how we implemented the main aspects of 
synapsed.

\subsection{Synapsed initialization}

Synapsed is a much simpler peer than cached and requires only the following 
arguments:

\begin{itemize}
	\item The network port where it will listen for requests.
	\item The remote address of the segment. \textbf{NOTE:} it can point to 
		the host address, which means that synapsed can also provide a 
		generic way to bridge two peers who reside in the same host but 
		in a different segment.
	\item The remote network port of the other synapsed peer.
	\item The XSEG port of the peer where synapsed will send the requests 
		that it accepts.
\end{itemize}

Once synapsed has the above arguments, it can create a socket, bind it and 
connect to its port. After that, it can listen for new connections and forward 
the requests that it receives to the synapsed of the other segment. That 
synapsed can in turn forward the request to the peer that it has been 
configured to target to.

\subsection{Request polling}\label{sec:poll-synapsed}

Synapsed typically uses the common peer polling and Archipelago IPC methods 
(see Section \ref{sec:arch-poll}) to check its XSEG ports for new requests.  
However, it simultaneously needs to listen for new requests on its socket. Yet, 
there is no way for a process to block on its socket \textbf{and} and 
synchronously wait for a queued signal.

The solution seems obvious; synapsed should instead block while polling for 
requests in its socket and, if a new request arrives at its XSEG port, its 
polling should be interrupted. This is a simple solution, but there is one
detail we must take into account; sleeping like so is unsafe because peers 
currently block the SIGIO signal in order to wake up synchronously.

Let's elaborate on that a bit: When a peer is initialized, it blocks the SIGIO 
signal. Using sigtimedwait, it can check fast and without races if a signal has 
been enqueued, and then it can sleep. This is not the case with \texttt{poll()} 
though, since if the peer sleeps with the SIGIO signal blocked, it will not 
wake up.  On the other hand, if we unblock the SIGIO signal, we have a new set 
of problems:

Consider the case where we receive a signal before we go to sleep. In order to 
be notified that a signal has been sent so as not to sleep, we would need to 
utilize a signal handler that would increment a global variable which would be 
checked right before we go to sleep. This approach is not only racy (what 
happens if SIGIO is received after this global variable is checked) but it is 
also very slow.

For this reason, we use the \texttt{ppoll()} alternative, which accepts a 
signal mask as an additional argument. \texttt{ppoll()} provides the guarantee 
that it will atomically: \begin{inparaenum}[i)]
\item replace the old signal mask of the process with the one provided,
\item check if there are pending signals and if so return,
\item block until a request has been received or a signal has been queued.
\end{inparaenum}

Solving the issue of waiting for XSEG requests while blocking on a socket is 
only part of the problem. We also have to solve its counterpart, which means we 
need to find a quick way to iterate the XSEG ports of synapsed for new 
requests, while listening for new requests on its socket.

We have countered this problem by checking alternately the XSEG port of 
synapsed, with the existing methods, and the network port of synapsed, by doing 
a poll with zero timeout, which returns immediately.

\subsection{Marshalling}

The XSEG request cannot be sent as is, since its data and target name are 
stored in two different buffers. Moreover, since these buffers can have 
variable length, the synapsed peer that reads from its socket needs a way to 
know when it has finished reading all the necessary data.

This is a common problem in computer science and its solution is to serialize 
(or "marshal") the object that must be transfered. The marshaling method we 
have chosen is the following:

\begin{enumerate}
	\item We send at the start of every transaction a fixed-length header, 
		whose size is known to all synapsed peers, and which has the 
		information about the length of the rest of the buffers.  The 
		header is presented on Listing \ref{lst:synapsed-header.h}

		\ccode{Synapsed header}{synapsed-header.h}

		The header is simply the necessary fields of the original 
		request that the remote synapsed peer needs to know. The most 
		important fields are \textbf{datalen} and \textbf{targetlen} 
		which indicate the length of the data and target buffers 
		respectively.
	\item Once the header has been sent and the remote peer has the 
		information that it needs, it allocates a new XSEG request from 
		its segment and fills it with the information provided by the 
		header, thereby creating to buffers with the appropriate 
		length.
	\item Finally, the remote synapsed peer reads the rest two buffers and 
		stores them in the XSEG request it has previously allocated.
\end{enumerate}

Marshaling however is usually not so simple. There are three caveats that one 
must take into account before attempting to serialize manually an object. They 
derive from the often overlooked fact that the host and the remote machine are 
not architecturally the same, which can lead to:

\begin{enumerate}
	\item \textbf{Different endianness}. This means that the two machines have 
		completely different byte order. This is commonly solved by converting 
		all of the data to network byte order (big endian) and then convert 
		them to the native endianess of the machine.
	\item \textbf{Different type representation}. Even if two architectures 
		have the same endianness, it is possible that they represent the same 
		types with different number of bytes. This is most common with 32-bit 
		and 64-bit architectures which, for example, use 4 bytes and 8 bytes 
		respectively to represent an int.
	\item \textbf{Padding}. Due to data alignment issues, the compiler may 
		need to pad the fields of a struct. The padding is depended on 
		the processor that is used to compile the program. A common 
		solution to sidestep padding issues from different processors 
		is to "pack" the structure, i.e.  to enforce that the structure 
		will have no padding.
\end{enumerate}

For synapsed, the first two caveats do not affect us, since the machines that 
are used are both 64-bit, little endian machines. The third caveat may also not 
affect us, but just to make sure, we have packed our header structure using the 
gcc pragma directive:

\ccode{GCC pragma pack directive}{synapsed-pack.h}

\subsection{Send/Receive}

As we have mentioned in the previous chapter, in order to send the data from 
one synapsed peer to the other, we must marshal them first and then unmarshal 
them. This commonly requires to merge all buffers into one and send that 
buffer.  

In synapsed however, we have chosen a different approach; we have employed the 
\texttt{readv()}/\texttt{writev()} functions, that allow us to do 
scatter/gather I/O.

This means that when we sent a request, we create an iovec vector that points 
to the data that we want to sent and their sizes (gather). Respectively, when 
we receive a request, we check its type and allocate the necessary XSEG buffers 
where the data can be copied (scatter).

\section{Evaluation of synapsed}\label{sec:plot-synapsed}

Currently, synapsed is in a functional but nascent state, meaning that 
important features such as mirroring of requests have not been implemented yet.  
Were these features implemented, they could also be evaluated and we would be 
able to quantify their performance cost.

This means that right now, synapsed's main purpose is to enable cached and 
other Archipelago peers to work in a remote environment with more resources, 
without encumbering the host machine. This kind of flexibility cannot be 
evaluated of course, but it should be considered an important feat, since it 
opens numerous possibilities for a previously network-unaware Archipelago.

There is however an interesting way we can evaluate synapsed. More 
specifically, we can benchmark its performance for each of the Archipelago 
configurations that are tested in Section \ref{sec:vm-plot}, minus the page 
cache test of course. 

The setup for our benchmarks is identical to the setup that is used in Chapter 
\ref{ch:cached-evaluation}. The only addition is that synapsed is in a 
different host, with similar specifications as our test-bed (see Section 
\ref{sec:test-bed}).  Additionally, the two hosts are connected with a 1Gbit 
Ethernet cable. Note that neither this connection type nor synapsed are tuned 
for high performance.  This is important in order to properly evaluate the 
following results.

We proceed with the performance results of our benchmarks. The bandwidth 
results are presented in Figure \ref{fig:bw-write-synapsed.pdf} and the latency 
results in Figure \ref{fig:lat-write-synapsed.pdf}.

\diagram{Bandwidth performance for writes through 
	synapsed}{bw-write-synapsed.pdf}
\diagram{Latency performance for writes through 
	synapsed}{lat-write-synapsed.pdf}

For the first round of comparisons, will be invoke the results of Section 
\ref{sec:sustained-plot} and more specifically Figures 
\ref{fig:bw-write-comp-truth.pdf} and \ref{fig:lat-write-comp-truth.pdf}.

The first impression that one gathers by comparing these results is that they 
seem very similar but clipped. This is expected since the theoretical maximum 
bandwidth of a 1Gbit ethernet cable is 128MB/s. The fastest that we seem to 
reach is 110MB/s, which is very close, if we consider that on the same cable 
both peers send data simultaneously. 

Most importantly however, besides the clipping in fast scenarios, there is 
practically little or no other effect on the rest of the scenarios, as they 
have approximately the same speed, with synapsed being only a little slower.

Moreover, we can see that cached-unlimited is performing similarly in both 
benchmarks and once again manages to outperform sosd, albeit being noticeably 
slower in the synapsed benchmark.

From the above comparisons we can observe that the network can be a bottleneck 
for an I/O intensive application, especially when it can achieve higher 
bandwidth than what the network can sustain.

We conclude this section with a comparison of our results with the results of 
Figures \ref{fig:bw-vm.pdf} and \ref{fig:lat-vm.pdf}. As we can see in these 
results, the latency that is introduced by the VM operations, hypervisor and 
Archipelago is currently far greater than the < 1ms latency of the 1Gbit 
connection. Moreover, if the connection between the two servers were 10G, the 
latency that is imposed by the network would be negligible.

What we try to infer with the above observation is that in the current 
situation, the network latency of synapsed is a small price to pay if we can 
manage to run cached or even Archipelago in a remote environment with more RAM.
