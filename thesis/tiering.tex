\chapter{The Holy Triad: Scalability, Tiering and Caching}\label{ch:tiering}

In this chapter we will discuss the challenges of today's data storage and will 
attempt to explain the role of scalability - a word that is being repeated like 
mantra in cloud circles - tiering and caching in mitigating costs and 
increasing performance.  Moreover, we present the current solutions for tiered 
storage and we evaluate if they can be used in conjunction with Archipelago.

\section{What is scalability?}

Scalability is a term that has different meaning for different contexts. For 
example, good scalablity in multi-threading context can mean that the program's 
performance increases linearly with the number of threads.

We approach this subject from the storage service's perspective, which is an 
integral part of a cloud service. In this context, scalability means the 
ability of the storage service to handle the increase of I/O requests in a way 
that each independent user does not experience any latency.

\section{What is tiering?}

Tiering is the organization of different storage types in levels (or tiers) 
depending on their performance.  The storage types usually differ in one of the 
following attributes: capacity, price or performance.  Tiered storage is 
analogous to the computer architecture model of memory hierarchy, which can be 
seen in Figure \ref{fig:mem-hier.pdf}.

\diagram{Computer Memory Hierarchy}{mem-hier.pdf}

Tiered storage is based on the same principles as memory hierarchy, in the 
sense that its objective is to keep data in the higher tiers, based on their 
frequency of reference.

\section{What is caching?}

In the context of I/O requests, caching is the addition of a fast medium in a 
data path, whose purpose is to transparently store the data that are intended 
for the slower medium of this data path. The benefits from caching is that 
later accesses to the same data will be faster than fetching them from the 
slower medium.

Caching is extensively used in computer architecture, as is evident from Figure 
\ref{fig:mem-hier.pdf}. Besides the memory hierarchy model, it is a widely 
employed concept in storage services where fast mediums are used as a journal 
for slower ones, essentially caching the data, or where dedicated servers are 
used to cache data, like in memcached's case (see more in Section 
\ref{sec:memcached-triad}).

\subsection{Caching policies}

Do we even care?

\begin{itemize}
	\item Write-through:
		This policy bla bla bla
	\item Write-back:
		This policy blu blu blu
\end{itemize}

\subsection{Caching limitations}

Given that fast mediums like RAM and SSD drives are always more expensive than 
slower mediums, caches always have smaller capacity than the mediums they 
cache. So, when a cache reaches its maximum capacity, it must evict one of its 
entries. 

And the big question arises: which entry?

This is a very old and well documented problem that still troubles the research 
community. It was first faced when creating hardware caches (the L1, L2 CPU 
caches we are familiar with). In 1966, Lazlo Belady proved that the best 
strategy is to evict the entry that is going to be used more later on in the 
future\cite{Belady}.  However, the clairvoyance needed for this strategy is a 
little difficult to implement, so we use one of the following, well-known 
strategies:

% Mention ehcache approach: 
% http://ehcache.org/documentation/apis/cache-eviction-algorithms
%
% Also look at the following papers
%L. Belady, “A Study of Replacement Algorithms for a
%Virtual-Storage Computer,” IBM Systems Journal, vol.5,
%no.2, pp.78-101, 1966.
\begin{itemize}
	\item \textbf{Random:} Evict a randomly chosen entry. This strategy, 
		although it seems simplistic at first, is sometimes chosen due 
		to the ease and speed of each. It is preferred in random 
		workloads where getting fast free space for an entry is more 
		important than the entry that will be evicted.
	\item \textbf{FIFO (First-In-First-Out):} Evict the entry that was 
		inserted first. This is also a very simplistic approach as well 
		as easy and fast.  Interestingly, although it would seem to 
		produce better results than Random eviction, it is rarely used 
		though, since it assumes that cache entries are used only once, 
		which is not common in real-life situations.
	\item \textbf{LRU (Least-Recently-Used)}
		Evict the entry that has
	\item \textbf{LFU (Least-Frequently-Used)}
\end{itemize}

Choosing the LRU strategy is usually a no-brainer. Not only does it 
\textit{seem} more optimal than the other algorithms, but it has also been 
proven, using a Bayesian statistic model, that no other algorithm that tracks 
the last K references to an entry can be more optimal.
%Also, check out this paper:
%An optimality proof of the LRU-K page replacement algorithm
%that proves that no algorithm that keeps track of the K most recent references 
%for a page can be more optimal than LRU.

\section{Current situation}

Caching is necessary in most medium or larger deployments, in order to bridge 
the performance gap between RAM and magnetic disks, which can be seen in Table 
\ref{tab:gap}

\begin{table}
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Medium & Access time (ns) \\ \hline \hline
		CPU registers and cache & < 10 \\ \hline
		RAM & < 10\textsuperscript{2}  \\ \hline
		SSD & < 10\textsuperscript{5} \\ \hline
		Hard Disk & < 10\textsuperscript{7} \\ \hline
	\end{tabular}
	\caption{Access times of storage mediums}
	\label{tab:gap}
\end{table}

For a computer which uses only hard-drives, it is easy to see that when data 
are not in RAM, the performance penalty is x10,000.

Usually, when a small deployment makes its first steps, it doesn't use SSDs due 
to management and hardware costs and since its an investment that is actually 
needed when the deployment has proved that it will attract traffic. Instead, 
the most common setup is an array of RAID-protected commodity hard disks or 
fast SAS drives.

When the storage needs start to increase and more users use that service, the 
OS caching system of the storage nodes will soon prove ineffective and the 
randomness in the requested data will skyrocket the access times. Soon, the 
infrastructure's access times will in the spectrum of many milliseconds.

At this point, the administrators are faced with the decision of buying or not 
battery-backed array controllers with volatile storage on-board to improve 
speeds but sooner or later, the quality of hardware will not be able to 
mitigate the cost of cache misses.

This is the turning point of medium deployments and where tiered storage helps


\section{How to scale?}

There are two methods of scaling, horizontal (scaling out) and vertical 
(scaling up). The horizontal scaling method requires that the service is a 
distributed one, i.e. that there are more than one storage nodes and the 
software can handle the addition 


\subsection{Block-devices}

There have been many block device tiers that have been built


\subsubsection{Bcache}

\paragraph{Overview}

Bcache has been designed by Kent Overstreet since 2011 and has been included in 
the Linux kernel (3.10) since the May of 2013.

Bcache allows one to use one or more fast mediums as a cache for slower ones.  
Typically, the slow medium is a RAID array of hard disks and the fast medium 
are SSD drives. Bcache has been specifically built for SSDs and has the 
following characteristics:

\begin{enumerate}
	\item The data are written sequentially and in erase block size 
		granularity, in order to avoid the costly read-erase-modify-write 
		cycle.
	\item It takes special care to mitigate wear-leveling by touching equally 
		all SSD cells
	\item It honors TRIM requests and uses them as hints for its garbage 
		collection.
\end{enumerate}

\paragraph{Installation}

Bcache is a kernel driver that needs a patched kernel and intrusive changes to 
the backing device, rendering previous data unreadable. On a nutshell, bcache 
edits the superblock of both the cache and backing devices in order to use 
them.  Then, it exposes to the users a virtual block device, which can be 
formatted to any filesystem.  This virtual device's request function actually 
points to the cache device and, when the cache device has reached its full 
utilization, it will flush dirty data to the backing device.

\paragraph{Features and Limitations}

The most striking bcache feature is that it uses a custom built B+tree as an 
index, which has the added benefit that dirty data can be coalesced and flushed 
sequentially to the slower spinning medium. This provides a considerable 
performance speed-up for hard disks since. Some of other noteworthy features of 
bcache are the following:

\begin{enumerate}
	\item It can be used to cache more than one devices
	\item It can operate in three modes, write-through, write-back and 
		write-around, which can be switched on/off arbitrarily during normal 
		usage or when the fast medium is congested.
	\item It utilizes a journal log of outstanding writes so that the data are 
		safe, even when an unclean shutdown occurs.
	\item It can bypass sequential IO and send it directly to the backing 
		device, since this workload is tailored for spinning disks.
\end{enumerate}

\subsubsection{Flashcache}

\paragraph{Overview}

Flashcache has been designed by Facebook and has been open-sourced in the April 
of 2010. It is a kernel module that is officially supported for kernels between  
2.6.18 and 2.6.38 and is based on the Linux Device Mapper, which is used to map 
a block device onto another.

\paragraph{Installation}

Flashcache's installation is not system-intrusive, in the sense that it needs 
only to compile the module against the kernel's source, modprobe it and then 
map the cache device upon the backing device, without making any changes to the 
latter.

\paragraph{Features and Limitations}

Flashcache uses a set-associative hash table for indexing. It has three modes 
of operation, writethrough, writeback and writearound, and some basic 
performance tuning options such eviction strategies and dirty data threshold.  
Also, it has the following limitations:

\begin{enumerate}
	\item It does not provide atomic write operations, which can lead to 
		page-tearing.
	\item It does not support the TRΙΜ command.
\end{enumerate}

\subsubsection{EnhanceIO}

\paragraph{Overview}

EnhanceIO has been developed by STEC Corp. and has been open-sourced in the 
December of 2012. It is a fork of Flashcache which does not use the Linux 
Device Mapper and has some major re-writes in parts of the code such as the 
write-back caching policy.

\paragraph{Installation}

The installation method is similar to the Flashcache's method. The source code 
is compiled again the kernel's source, which produces a module that can be 
modprobed. After that, the utilities provided can be used to map the cache 
device on the backing device.

\paragraph{Features and Limitations}

Similarly to Flashcache, EnhanceIO uses a set-associative hash table for 
indexing. It also has improvements upon the original Flashcache implementation 
in the following areas:

\begin{enumerate}
	\item The page-tearing problems have been solved.
	\item Dirty data flushing using background threads.
\end{enumerate}

\subsubsection{DM-cache}

\subsection{Key-value store}

\subsubsection{Memcached}\label{sec:memcached-triad}

\paragraph{Overview}

Memcached is a distributed memory caching system that is being widely employed 
by large sites such as Youtube, Facebook, Twitter, Wikipedia. It has been 
created in 2003 by Brad Fitzpatrick while working in LiveJournal and to date 
there have been numerous forks of the code, most notably including Twitter's 
twemcache and fatcache, Facebook's implementation etc.

When memcached came into existence, many social sites like LiveJournal were 
experiencing the following problem:

User pages would often have queries that would be executed hundreds of times 
per second or would span across the database due to a big SELECT, but whose 
nature would be less critical or would not change rapidly. Queries such as "Who 
are my friends and who of them are online?", "What are the latest news in my 
feed?" etc. which could be easily cached, crippled instead the database by 
adding a lot of load to it.

To tackle this problem, administrator can instruct memcached to utilize the 
unused RAM of the site's servers to cache these kinds of queries. Ten years 
later, memcached has become the defacto scale-out solution, and has use cases 
such as Facebook's whose dedicated memcached servers serve the 95\% of their 
queries (\fixme add presentation)

\paragraph{Installation/Usage}

Memcached adheres to the client server model, with N clients connecting to M 
servers. Memcached, which is a user space daemon, runs on every server and
listens for requests typically on port 11211. The installation is very easy 
since there are packages for most known distros. Once memcached has been 
installed, the administration needs to specify only the port and several 
performance options such as cache size and number of threads.

The clients on the other hand communicate with the memcached servers using 
native libraries. There are libraries that are written for most programming 
languages such as C, PHP, Python, Haskell etc. The clients can then specify 
which queries - or keys in general - want to be cached and the actual caching 
is done in runtime.

\paragraph{Features/Limitations}

Architecturally, memcached tries to do everything in O(1) time. Each memcached 
server consists of a hash table that indexes the keys and their data. Since the 
data size can vary from 1 byte to 1MB, they are organized in SLABs in order to 
prevent memory fragmentation.  Moreover, each memcached must be able to handle 
tens of thousands connections from clients, so it relies in libevent to do the 
asynchronous polling.

What's more interesting about memcached is that its main strength is actually 
its biggest limitation. Memcached has no persistence and in fact, data can be 
evicted in numerous ways:

\begin{enumerate}
	\item Cached data have an expiration time after which they are 
		garbage-collected.
	\item Data can be evicted before their expiration time, if the cache has 
		become full.
	\item When memcached is out of SLAB pages, it must evict one in order to 
		regain space. This leads to the eviction of more than one keys.
	\item When adding or removing memcached servers, the Ketama algorithm that 
		maps keys to servers will assign a portion of the existing keys to 
		other servers. This change in mapping, however, will not actually move 
		the existing keys to these servers and the data are essentially 
		invalidated.
\end{enumerate}

To sum up, the lack of persistence means that memcached will never hit the disk 
bottleneck due to flushes and will always be very fast, as long as the cache 
hit rate is high. On the other hand, its unreliable nature means that it is not 
a general purpose software and only specific workloads will be benefited from 
it.

\subsection{NoSQL Databases}

\subsubsection{Couchbase}

\paragraph{Overview}

Couchbase, which has been under active development since the January of 2012, 
is actually the product of the merge of two independent projects, CouchDB and 
Memebase, with CouchDB continuing as an Apache funded program. Couchbase aims 
to combine the scalability of memcached with the persistence of a database such 
as CouchDB. 

\paragraph{Installation/Usage}

Moreover, it tries to add some more features such as replication
to be alive as a projects.

\subsection{Honorary mentions}

\subsubsection{Repcached}

\subsubsection{Ramcloud}

\subsection{Summary}





