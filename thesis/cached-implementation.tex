\chapter{Implementation of cached}\label{ch:cached-implementation}

In the previous chapter, we have discussed in length the design of cached and 
its components. In this chapter, we will present how the above design has been
implemented. To aid us in this task, we will use code snippets from cached, and 
xcache and we will comment where necessary.

More specifically, Section \ref{sec:xcache-imp} presents the implementation of 
xcache, the main cached component. Next, section \ref{sec:cached-imp} presents 
the implementation of cached and demonstrates the structures and functions that 
are used.

\section{Implementation of xcache}\label{sec:xcache-imp}

For this section, we will attempt to provide a top-down view of the xcache 
implementation, starting from the functions that xcache exposes to peers and 
moving on to the more intrinsic details, such as the concurrency control.

\subsection{xcache initialization}

In order to use xcache, the peer must first initialize an xcache structure 
using \texttt{xcache\_init}, which can be seen in Listing 
\ref{lst:xcache-init.h}.

\ccode{Definition of \texttt{xcache\_init}}{xcache-init.h}

\texttt{xcache\_init} requests the following information from the peer:

\begin{description}
	\item[cache:] Simply, an allocated xcache struct.
	\item[xcache\_size:] The number of objects xcache will index.
	\item[ops:] The trigger functions for xcache's event hooks.
	\item[flags:] Optional flags that tune the following two things:
		\begin{enumerate}
			\item The LRU algorithm. For the cached implementation, 
				we use the O(1) LRU, but xcache also allows to 
				use two more LRU algorithms, a binary heap 
				(O(log(N))) or an LRU array (O(N)).
			\item The usage of the hash table for evicted entries.  
				Although our cached implementation relies 
				heavily on it, this does not account for all 
				the other peers that use xcache and by default 
				is not used.
		\end{enumerate}
	\item[priv:] A pointer (void *) to a structure that will be returned 
		when an event hook is triggered. As most priv fields, it is 
		irrelevant to the xcache struct and relevant only to the top 
		caller. We initialize it with the peer struct.
\end{description}

The purpose of xcache\_init is to process the above data, populate the xcache 
struct and create the necessary entities, such as the hash table, the cache 
entries etc. On Listing \ref{lst:xcache-struct.h}, we can view the xcache 
struct and its respective fields.

\ccode{Main \xcache struct}{xcache-struct.h}

Each of the above \xcache struct fields is used to implement a design feature 
that has already been discussed in Section \ref{sec:xcache-design}. In the 
following sections, we will revisit these design features and present their 
implementation.

\subsection{Cache entry preallocation}

When xcache is initialized, it preallocates the necessary cache entries. The 
relevant xcache fields for this purpose can be seen in Listing 
\ref{lst:xcache-prealloc.h}.

\ccode{\xcache struct fields relevant to preallocated 
	entries}{xcache-prealloc.h}

The \textbf{size} field is the number of entries. The \textbf{free\_nodes} is a 
stack where all entry indexes are initially pushed and subsequently popped when 
a new entry is inserted. Finally, \textbf{nodes} is the space allocated for the 
cache nodes and where the entry indexes point to.

Moreover, the definition of the \texttt{xcache\_entry} struct is shown in 
Listing \ref{lst:xcache-entry.h}.

\ccode{The \texttt{xcache\_entry} struct}{xcache-entry.h}

We will comment briefly on the relevant \texttt{xcache\_entry} fields for this 
section, which can be seen in Listing \ref{lst:xcache-entry-prealloc.h}. The 
rest of the fields will be discussed in the following sections.

\ccode{\texttt{xcache\_entry} fields, relevant to 
	preallocation}{xcache-entry-prealloc.h}

The description of the fields follows:

\begin{description}
	\item[ref:] The reference count of the entry, initially set to zero.
	\item[state:] The state of the entry. It can either be ACTIVE or 
		EVICTED and is initially set to the first.
	\item[name:] The name of the entry. Since we cannot know its length 
		beforehand, we allocate as much space as possible, typically 
		256 characters. During initialization, the entry name is 
		cleared out of junk values.
	\item[priv:] The private contents of the cache entry. On 
		initialization, the cache node creation hook is triggered and 
		cached initializes the private contents of cache entry with its 
		data (more on Section \ref{sec:xcache-hooks-imp})
\end{description}

\begin{comment}
	The rest willwhaLet's start by listing what \texttt{xcache entry} 
	consists of.  First of all, it must have a name.  Since we preallocate 
	the entries and cannot know in runtime their length, we must allocate 
	as much space as possible. The \texttt{char name[XSEG\_MAX\_TARGETLEN + 
		1]} field, which is 256 characters long, is long enough to hold the 
	target's name.  Also, as we have mentioned in Section 
	\ref{sec:entry-prealloc-design}, xcache must be agnostic of the cache 
	contents.  To this end, we use the generic \texttt{void *priv} field as 
	a pointer to the actual entry content.  The rest of the fields will be 
	explained in the following chapters.

	Let's continue now with the fields of Listing \ref{lst:xcache-prealloc.h}.  
	Since we preallocate the entries using \texttt{malloc}, they take up a 
	contiguous space in memory.  The start of this space is the where the 
	\texttt{*nodes} field points to. The \texttt{free\_nodes} field works similarly 
	to the free\_entries field in Section \ref{sec:get-req-archip} i.e. it is a 
	stack where indexes to unused nodes are pushed. These indexes will be seen in 
	various code excerpts in this chapter and have a specific name, 
	\texttt{xcache\_handler}\footnote{\#define xcache\_handler uint64\_t}.
\end{comment}

\subsection{Cache entry initialization}

Before a peer can index a new entry, it must first allocate it from the cache 
entry pool and then initialize it. Xcache has a special function for this 
purpose, which can be seen in Listing \ref{lst:xcache-alloc-init.c}.

\ccode{Allocation/initialization function for 
	\texttt{xcache\_entry}}{xcache-alloc-init.c}

This function attempts to claim an \texttt{xcache\_entry} index from the
\texttt{free\_nodes} stack.  Then it initializes it with the name given by the 
peer.  Moreover, it triggers the cache entry initialization hook which cached 
uses to further initialize the entry.

An added benefit of this function is that it doesn't need to acquire the main 
cache lock, so it does not slow down the indexing functions that rely on that 
lock.

\subsection{Cache entry indexing}

This is the core feature of xcache. In Listing \ref{lst:xcache-index.h}, we 
present the fields of the xcache struct that are relevant to the indexing task.

\ccode{xcache struct fields relevant to entry indexing}{xcache-index.h}

As we have mentioned in Section \ref{sec:xcache-index-design}, we utilize two 
hash tables, one for the active entries and one for the evicted entries. These 
hash tables can be accessed from xcache and are the \texttt{*entries} and 
\texttt{*rm\_entries} fields respectively.

More importantly, in Listing \ref{lst:xcache-index.c} we can see the functions 
that are related to indexing and xcache exposes to the peer:

\ccode{Indexing functions}{xcache-index.c}

All of these functions need a pointer to the xcache struct. Here's a brief 
description of them:

\begin{description}
	\item[xcache\_lookup:]
		Takes the target's name as an argument and searches for it in 
		cache.\\
		Returns on failure: NoEntry\\
		Returns on success: the requested handler.\\
		\textbf{Note:} Looks only in \texttt{entries}.
	\item[xcache\_insert:]
		Takes the handler of an allocated entry as an argument and uses 
		it to index that entry.\\
		Returns on failure: NoEntry.\\
		Returns on success:
		\begin{inparaenum}[i)]
		\item the same handler or,
		\item if the same entry already exists in cache, the handler of that 
			entry.
		\end{inparaenum}\\
		\textbf{Note:} It looks up first if the entry exist in 
		\texttt{entries} or \texttt{rm\_entries}. The later case can 
		lead to re-insertions.
		\item[xcache\_remove:]
			Takes the handler of an allocated entry as an argument and uses 
			it to remove that entry.\\
			Returns on failure: -1.\\
			Returns on success: 0.\\
			\textbf{Note:} Removes only active entries .
\end{description}

Moreover, in Listing \ref{lst:xcache-entry-index.h}, we show the 
\texttt{xcache\_entry} fields that are related to indexing, and comment on how 
they are used by each function.

\ccode{xcache entry struct relevant indexing}{xcache-entry-index.h}

\begin{description}
	\item[ref:] The reference counter of an object is increased on lookups 
		and inserts, since it is essentially referenced in these 
		operations.
	\item[state:] The state of the entry is set to ACTIVE.
	\item[older/younger]: These fields show the neighbors of the entry in 
		the LRU queue. The LRU queue is sorted by reference time order, 
		so the neighbors are essentially the entries that have been 
		referenced right before and right after our entry.
\end{description}

\subsection{Entry eviction}\label{xcache-evict-imp}

The relevant fields for this purpose can be seen in Listing 
\ref{lst:xcache-evict.h}.

\ccode{\xcache struct fields for eviction}{xcache-evict.h}

As we have mentioned in Section \ref{sec:xcache-eviction-design}, we resort to 
eviction when the cache is full and new entries can't be inserted. By xcache 
policy, we evict the least recently used entry. The necessary fields for the 
doubly-linked list that we maintain for this purpose can be seen below:

\ccode{Doubly-linked LRU list}{xcache-dlist.h}

The last entry of the list (oldest) is usually the LRU. When an object is 
referenced, it can be instantly transferred to the head of the list (MRU), 
since we know its position via the hash table (alternatively, we would need to 
search all entries, which would require O(N) time).

Another feature of this LRU queue is that it doesn't require timestamps, so we 
can avoid the unnecessary system call.

When we have found out which entry to evict, we migrate it from the hash table 
of active entries to the hash table of evicted entries. This process is 
explained in depth in Section \ref{sec:xcache-eviction-design}.

Finally, when a cache entry is evicted from the hash table, it triggers the 
cache entry eviction hook.

\begin{comment}
\begin{itemize}
	\item Insert a new entry to the LRU list
	\item Evict the LRU entry
	\item Update an entry's access time (i.e. mark it as MRU)
	\item Remove an arbitrary entry
\end{itemize}

Lets explain these fields a bit:

\begin{description}
	\item[lru:] Obviously, it's the least recently used entry. It can be 
		considered as the one end of the doubly linked list.
	\item[mru:] The entry that has just been used. It can be considered as 
		the other end of the doubly-linked list
	\item[younger:] This entry-specific field points to an entry used right 
		after our entry was used.
	\item[older:] Same as "younger", it points to the entry that has been 
		used right before our entry was used.
\end{description}

Finally, as we have explained in Section \ref{sec:xcache-evict-design}, the 
eviction internals should normally not bother the user. However, if the user 
wants to, \xcache exposes the following functions:

\begin{description}
	\item[xcache\_evict\_lru:] The name says it all, it evicts the recently 
		used item.
	\item[xcache\_peek\_and\_get\_lru:] This function allows the user to 
		atomically take a peek on the Least Recently Used entry and also 
		update its refcount.
\end{description}
\end{comment}

\subsection{Entry reinsertion}

We have seen in the previous section how an active entry is evicted. However, 
what happens when xcache receives a request for an evicted entry which hasn't 
been freed yet? 

In this case, the entry migrates back to the hash table of active entries.  
Also, its reference counter is incremented by 1, since the xcache can now 
reference this entry again.

The advantage of reinsertion is that we do not stall until the evicted entry 
has flushed all its data and instead, we can use it immediately.

\subsection{Concurrency control}\label{sec:xcache-conc-imp}

Concurrency control is an extremely important aspect of xcache, if we want to 
utilize an SMP system to its full potential. Although parts of the concurrency 
control have already been discussed in previous sections, in this section, we 
will provide an in-depth explanation of how xcache implements them.

The relevant fields for concurrency control can be seen in Listing 
\ref{lst:xcache-conc.h}, both for the xcache and \texttt{xcache\_entry} 
structs.

\ccode{Concurrency control fields}{xcache-conc.h}

There are three main techniques xcache uses for concurrency control. The first 
one is the usage of locks, the second one is reference counting and the third 
one is the tracking of parallel puts to an entry, a more esoteric method that 
counters the ABA problem.

\subsubsection{Locking}\label{sec:xcache-lock-imp}

With xcache, we have tried not to use a BKL
\footnote{BKL stands for Big Kernel Lock and was a giant lock in kernel space 
	that inhibited the performance of SMP systems and remained until the 
	late stages of the 2.6 Linux kernel}
type of lock, but instead use many smaller ones.

Specifically, we have used:
\begin{enumerate}
	\item a lock that protects the cache entry pool from concurrent 
		accesses. Since the only operation this lock protects is the 
		push and pop of cache entry indexes, we expect that there will 
		be no contention on it.
	\item the \texttt{lock} lock, as seen in the xcache struct, which is 
		our main lock as it protects the hash table of active entries 
		(\texttt{entries}) from concurrent accesses. This lock is used 
		during lookups, inserts and evictions, so it is the lock with 
		the most contention.
	\item the \texttt{rm\_lock}, which protects the hash table of evicted 
		entries (\texttt{rm\_entries}) from concurrent accesses and is 
		used during insertions, evictions and puts.
	\item a lock in every entry, which is specifically used when an entry 
		is put.
\end{enumerate}

Of major importance is also the issue of deadlocking. More specifically, during 
inserts or evictions, we need to have access on both hash tables. If a thread 
acquired the lock of one hash table and another thread acquired simultaneously 
the lock of the other, we would have a deadlock since both would need a lock 
that the other thread has.

To this end, xcache strictly acquires the locks in the following order: 
\texttt{lock} --> \texttt{rm\_lock} --> entry lock. With this policy we are 
sure that there will be no deadlocks.

\subsubsection{Reference counting}\label{sec:xcache-refcount-imp}

Each cache entry has a volatile uint64\_t field which is atomically get and 
put. The type is volatile to inform the compiler that it might be changed 
outside the current execution context and therefore do not cache it in a 
register.

Furthermore, the atomic gets and puts are executed using the GCC 
builtins\cite{gcc-atomic} which are shown in Listing \ref{lst:xcache-atomic.c}.

\ccode{Atomic operations of GCC}{xcache-atomic.c}

The refcount model in xcache should be familiar to most people:

% Turn this to figure
\begin{itemize}
	\item When an entry is inserted in cache, xcache holds a reference to 
		it (ref = 1).
	\item Whenever a new lookup for this cache entry succeeds, the reference 
		is increased by 1 (ref++)
	\item When a request has been completed and no longer needs the entry, 
		the reference is decreased by 1. (ref--)
	\item When a cache entry is evicted by xcache, its ref is decreased by 
		1, since it no longer holds a reference to it. (ref--)
\end{itemize}

Moreover, some common refcount cases are:

\begin{itemize}
	\item active entry with pending jobs (ref > 1)
	\item active entry with no pending jobs (ref = 1)
	\item evicted entry with pending jobs (ref > 0)
	\item evicted entry with no pending jobs (ref = 0)
\end{itemize}

\begin{comment}
\begin{table}
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Case & Refcount \\ \hline \hline
		active entry with pending jobs & ref > 1 \\ \hline
		active entry with no pending jobs & ref = 1 \\ \hline
		evicted entry with pending jobs & ref > 0 \\ \hline
		evicted entry with no pending jobs & ref = 0 \\ \hline
	\end{tabular}
	\caption{Reference counting of xcache}
	\label{tab:refcount}
\end{table}
\end{comment}

Unlike most refcount cases, however, the entry is not put when its refcount 
drops to zero. The reason is that the entry can be reinserted at any time. In 
the following section, we explain how we have handled that case.

\subsubsection{Parallel puts}

The scenario of putting the entry has proved the most tricky one and deserves 
its own section in the concurrency control implementation.

For this scenario, we aimed to avoid the usage of our two biggest locks: 
\texttt{lock} and \texttt{rm\_lock}. Avoiding the first one was easy since the 
hash table of active entries was not used. However, the same did not hold true 
for the \texttt{rm\_lock}, since the evicted entry must be reliably removed 
from the hash table of evicted entries. However, we have managed to reduce its 
usage to the absolute minimum.

The put procedure is the following:

\begin{enumerate}
	\item We decrement the refcount of the entry.
	\item If the refcount is more than zero, we leave.
	\item Else, we proceed by triggering the on\_finalize hook. With this 
		hook, the peer is given a chance to increment the refcount of 
		the entry if it deems it necessary (e.g. the entry is dirty).
	\item After we have returned from this hook, we check if the refcount 
		is zero.  This time, we use the \texttt{rm\_lock} to prevent 
		reinsertions and remove the entry from the hash table of 
		evicted entries.
	\item If the refcount is not zero, then someone in the meantime has 
		reinserted the entry and we can safely leave.
	\item Else, we remove the entry.
\end{enumerate}

This procedure looks simple at first glance. However, looks are deceiving and 
so is the ABA problem. In our case, consider the following:

\begin{enumerate}
	\item Tread 1 \textrightarrow\ evicts a clean entry \textbf{(ref 0)}
	\item Thread 1 \textrightarrow\ triggers the on\_finalize hook
	\item Thread 2 \textrightarrow\ reinserts the entry \textbf{(ref 2)}
	\item Thread 2 \textrightarrow\ makes the entry dirty and then puts it 
		\textbf{(ref 1)}
	\item Thread 3 \textrightarrow\ evicts the now dirty entry           
		\textbf{(ref 0)}
	\item Thread 1 \textrightarrow\ checks if the reference count is zero 
		and removes the entry
	\item Thread 3 \textrightarrow\ triggers the on\_finalize hook, 
		increments the refcount of the entry and issues a flush for its 
		contents
		\textbf{(ref 1)}
\end{enumerate}

Although the final refcount is correct, the entry has been removed midway the 
put procedure. The fault in our logic was that we expected that, if the 
refcount is zero before and after the on\_finalize hook, we could safely remove 
the entry. This is commonly referred to in the bibliography as the "ABA 
problem"\cite{aba}. Generally, it occurs when a thread reads twice a "guard" 
variable and gets the same value, but is unaware of the fact that it has 
changed in between these two checks.

The ABA problem is mostly encountered in lock-free structures and since xcache 
does not aim to be one of them, we can simply lock the put procedure with the 
\texttt{rm\_lock}. However, this can have a performance impact, especially if a 
peer plugs a slow on\_finalize trigger function.

We have chosen instead to use an extra \texttt{xcache\_entry} field called 
\texttt{parallel\_puts}.  This number is incremented when a thread enters the 
put function and has dropped the reference of the object to zero i.e. between 
steps one and two. This way, we can also check during step five the number of 
parallel puts if and leave if it is more than one.

\subsection{Event hooks}\label{sec:xcache-hooks-imp}

The hooks that xcache provides to users are stored in an xcache\_ops struct 
that can be seen in Listing \ref{lst:xcache-hooks.h}.

\ccode{xcache\_ops struct}{xcache-hooks.h}

The design of these hooks has been presented on Section 
\ref{sec:xcache-hooks-design}. The functions that are attached to each hook 
return two values:
\begin{inparaenum}
\item the private field of xcache (the peer structure in our case) and
\item the cache entry's private data (the object in our case) for which the 
	hook was triggered.
\end{inparaenum}

\section{Implementation of cached}\label{sec:cached-imp}

In this section, we will present the implementation of cached. A presentation 
of the design of cached is provided in Section \ref{sec:cached-design}.  
Similarly to xcache, we will begin with the initialization process, we will 
continue with the request handling and finish with presenting the challenges we 
faced and the solutions we implemented.

\subsection{Cached initialization}\label{sec:cached-init-imp}

We have mentioned in the previous chapters that cached can be multi-threaded, 
have different write policies, maximum number of objects, cache size etc. All 
these variables are given from command-line and used during cached 
initialization. The necessary arguments are:

\begin{itemize}
	\item Number of threads
	\item Max objects to cache
	\item Total cache size
	\item Object size
	\item Bucket size
	\item Blocker port
	\item Write policy
\end{itemize}

and the cached structure that is initialized is presented in Listing 
\ref{lst:cached.h}. 

\ccode{Main cached struct}{cached.h}

Moreover, on cached initialization we also initialize xcache as well as the 
necessary xworkqs and xwaitqs.

Some of the above cached fields are the same with the command-line arguments 
and are self explanatory. We will briefly comment on the less obvious fields, 
which will be discussed in length in their respective sections.

\begin{description}
	\item[cache:]
		The initialized xcache struct is stored here.
	\item[max\_req\_size:]
		The maximum request size that can be sent to the blocker.
	\item[workq:]
		A lockless xworkq where non-critical jobs from threads who are 
		in a critical section are enqueued.
	\item[pending\_waitq:]
		An xwaitq for jobs that need to allocate a cache entry to.
		continue
	\item[bucket\_waitq:]
		An xwaitq for jobs that need to allocate a bucket to continue.
	\item[req\_waitq:]
		An xwaitq for jobs that need to allocate a request to continue.
	\item[bucket\_data:]
		The bucket pool of cached, Its size is the total\_size of 
		cached.
	\item[bucket\_indexes:]
		The stack where bucket indexes are pushed.
\end{description}

Furthermore, during the xcache initialization that takes place inside the 
cached initialization, the cached node initialization hook is triggered and 
cached can create its objects, which are as many as the max objects.  
Programmatically, cached objects are called "ce"s and their structure can be 
seen in Listing \ref{lst:ce.h}.

\ccode{Cached entry struct}{ce.h}

The explanation of the above fields follows:

\begin{description}
	\item[status:] The object status, as seen in Section 
		\ref{sec:cached-states-design}.
	\item[lock:] The lock for the ce's and its buckets' data.
	\item[workq:] The xworkq that is used for concurrency control over 
		parallel access to the ce's and its buckets' data. It uses the 
		aforementioned lock.
	\item[pending\_waitq:] The xwaitq that is used when a request cannot be 
		executed due to the ce's state. It will allow job executions 
		only when the object is not in FLUSHING state.
\end{description}

We have intentionally left out the bucket related fields that will be discussed 
in length in Section \ref{sec:cached-bucket-imp}.

\subsection{Bucket pool}\label{sec:cached-bucket-imp}

The initialization of the bucket pool is covered in Section 
\ref{sec:cached-init-imp}. In this section, we will explain how this bucket 
pool is connected with the buckets of each ce.

When the cache node initialization hook is triggered, the ce's buckets are 
initialized. Essentially, this means we do (once only) the following:

\begin{enumerate}
	\item First, we allocate an array of struct buckets. The array has
		\texttt{buckets\_per\_object} length, which is typically 1024 
		(4MB objects / 4KB bucket size). The struct bucket is a very 
		simple struct and is presented in Listing 
		\ref{lst:bucket.h}.
		
		\ccode{Bucket implementation}{bucket.h}

	\item Second, we allocate two more arrays, the  
		\texttt{bucket\_alloc\_status\_counters} array and the
		\texttt{bucket\_data\_status\_counters} array, whose length is 
		the number of allocation states (2) and data states (5) 
		respectively.
	\item Third, we initialize each bucket's allocation state to FREE and 
		data state to INVALID. The allocation and data state are stored 
		in the \texttt{flags} section of struct bucket, which is 
		actually a custom bit-field with support for variable field 
		lengths.
	\item Finally, we initialize all the counters to zero, besides the 
		allocation counter for FREE buckets and the data counter for 
		INVALID buckets, which are set to \texttt{buckets\_per\_object} 
		(1024).
\end{enumerate}

None of the above operations, however, interact with the bucket pool. This is 
because we do not initially attach the bucket indexes to the ce's buckets.

The way buckets are attached to the object is analogous to the way a function 
maps to its address space a large memory chunk which has previously allocated; 
the chunk is internally divided to smaller chunks and when the function 
attempts to "touch" them, it is trapped and then the buckets are mapped to the 
function's address space.

Similarly, when cached accepts a request for a target, the request's range is 
translated to bucket range. If any of the buckets within that range are not 
attached to the ce, the request is "trapped" and the needed buckets are claimed 
from the bucket pool.

Finally, the bucket claiming and release procedure is the following:
\begin{enumerate}
\item We pop a bucket index from the \texttt{bucket\_indexes} stack,
\item We translate it to the actual data pointer and store it to the 
	\texttt{data} field of the struct bucket,
\item When the bucket is released, we translate the data pointer back to the 
	bucket index
\item We push the bucket index back to the bucket pool.
\end{enumerate}

\subsection{Request handling}

Cached uses the request polling scheme that we have described in Section 
\ref{sec:arch-poll}, with the addition of the following:

\begin{enumerate}
	\item Checks for the state of the bucket pool. If the bucket pool has 
		been depleted, we force flush the LRU entry to acquire its 
		buckets.
	\item Periodic signals to the cached's xworkq.
\end{enumerate}

When a request is accepted/received, it is forwarded to the appropriate handle 
function based on its xcache operation type.

More specifically, for accepted (new) requests, we index the request target 
(object) and store its xcache handler on the request's cache-io and we proceed 
according to its operation type. For received requests, the request's cache-io 
holds the xcache handler for the object, so we can proceed immediately 
according to its operation type. The way the request is handled next is 
documented in Section \ref{sec:cached-flow-design}.

