\chapter{Design of cached}\label{ch:cached-design}

In Chapter \ref{ch:triad}, we have addressed the importance of scalability, 
tiering and caching for the performance of a storage service. We have also 
presented various caching solutions and explained why we rejected them.

This situation ultimately lead to the design of a new caching entity for 
Archipelago.  This entity is called "cached", which simply means \textbf{cache 
	d}aemon. We have decided to invest time in creating our own implementation 
mainly for two reasons:
\begin{inparaenum}[(i)]
\item to create a peer that understands the Archipelago logic and can integrate 
	naturally with it and
\item to measure the best possible performance gain that we can achieve and 
	evaluate if it is worth to further pursue and improve upon it.
\end{inparaenum}

Thus, in addition to the observations of the previous chapter, we will provide 
some stricter requirements that our solution must have. These requirements are:

% List the requirements for our solution
\begin{enumerate}
	\item \textbf{Nativity:} Cached must be native to Archipelago i.e.  not 
		need any translation layers to communicate with it.
		%explain why
	\item \textbf{Pluggability:} Cached must be able to provide a caching 
		layer between peers that are already in operating mode without 
		restarting Archipelago. Also, it must be removed without 
		disturbing the service.
	\item \textbf{In-memory:}
		\begin{comment}
		Our solution must cache requests in RAM, since the next fastest 
		tier, SSDs, are already being used in RADOS as a 
		journal.
		\end{comment}
		Our main goal is to measure the maximum performance gain that 
		we can achieve, which means that caching in RAM is our best 
		option.
		
		This also means that we will temporarily hand-wave the issue of 
		data volatility and we will concern ourselves with it if cached 
		has promising results.
	\item \textbf{Low indexing overhead:} For performance reasons and since we 
		already cache data in memory, the overhead of the indexing mechanism 
		should be as small as possible.
\end{enumerate}

The following two chapters are the main bulk of this thesis as they present the 
design and implementation of cached that aims to fill the above requirements.

% Foretell what the following sections will contain
More specifically, this chapter provides an in-depth description of the design 
of cached. Section \ref{sec:rationale-design} provides the design rationale of 
cached and explains how its design meets the above requirements. Section 
\ref{sec:comp-design} presents the building blocks of cached while Sections 
\ref{sec:xcache-design}, \ref{sec:xworkq-design} and \ref{sec:xwaitq-design} 
provide a detailed explanation of their design. Moreover, Section 
\ref{sec:xcache-flow-design} illustrates the request flow of one of the most 
important components of cached, the xcache. Section \ref{sec:cached-design} 
explains how cached utilizes the aforementioned components and further 
introduces some unique ones that have been tailored specifically for cached.  
Finally, in Section \ref{sec:cached-flow-design} we illustrate the request flow 
for cached.

\section{Design rationale}\label{sec:rationale-design}

One of the first architectural decisions was to implement cached as an 
Archipelago user-space peer.  This choice was the most natural one since it 
provides the smallest possible communication overhead with the other 
Archipelago peers. Also, this design decision covers the \textbf{nativity} 
requirement we posed at the beginning of this chapter.

The above design choice has another advantage too; we can plug on-line the 
cached peer between the vlmcd and blocker and unplug it when we want to. This 
opens up numerous possibilities such as plugging cached for performance 
reasons, when there is a peak in I/O requests.  This also means that the 
\textbf{pluggability} requirement is also being met.

\begin{comment}
This is possible because, as we have mentioned in Section \ref{sec:arch-ipc}, 
XSEG ports can be registered on-line. Thus, during normal operation, the 
administrator can add the cached port to the request path between vlmcd and 
blocker, and all requests will seamlessly be intercepted by cached. This 
follows the same principle with bcache, which plugs its own request\_fn() 
function to the virtual device it creates.  Unlike bcache however, cached can 
be plugged on and off at any time.
\end{comment}

The next important design decision was what will cached index. Given that it 
will reside between the vlmcd and blocker, where the VM's requests have already 
been translated to object requests, the natural choice is to cache objects. To 
understand why our caching peer must be close to the Archipelago's logic, i.e.  
the translation of blocks to objects, consider the following two points:

\begin{itemize}
\item Like bcache, cached must not only cache object requests fast but also try 
	to coalesce them so that, when needed, they will be flushed to the 
	slower medium in a more sequential fashion. The fact, however, that a 
	VM's volume is partitioned into different objects, means that 
	sequential data (in volume context) which reside in different objects 
	will probably not be sequential in the storage backend too.
	
	Thus, in Archipelago it makes sense only to coalesce data in the object 
	range (commonly 4MBs). If our implementation was caching in block level 
	(e.g. bcache), it would be unaware of that fact.
\item We can use cached to further improve the performance of Copy-on-Write.  
	If cached receives a CoW request and has the object cached, we can
	acknowledge the copy on the fly, without waiting for the slower medium.  
	Note that this is a feature that has not been implemented yet, but will be 
	in the future.
\end{itemize}

Having decided that cached will cache objects, the next step is to decide
\begin{inparaenum}[(i)]
\item on the index mechanism and
\item on \textbf{what} exactly will we index.
\end{inparaenum}

As for what we will index, it would be an overkill to further partition the 
objects and index the regions within them. This would probably be more 
appropriate for larger objects (e.g. volumes).  So, we index object names 
solely.

As for the index mechanism, we have chosen to use a very fast in-memory hash 
table that will keep the data in a preallocated space in RAM. This covers the 
\textbf{in-memory} requirement that we have set above. Also, the choice of the 
hash table is one of the reasons that our implementation has \textbf{low 
	indexing overhead}.

Finally, another important decision was whether cached would be a 
multi-threaded peer. We have decided that we will implement it this way and 
then evaluate the performance of the implementation to find out if we are 
benefited by multi-threading or not.

Thus, cached must be able to work with multiple threads which will accept 
requests from cached's request queue and serve them concurrently with the other 
threads. Of course, multi-threading can be very tricky, especially when we are 
dealing with I/O requests and simultaneous accesses to the same object.  So, in 
order to achieve a balance between safety and speed, we use a
fine-grained locking scheme in critical sections, which is discussed in detail 
in Section \ref{sec:xworkq-design}.

\section{Cached components}\label{sec:comp-design}

At this point, we must do an intermission before we show the design of cached.  
Specifically, we will show first the design of the cached's components, since 
many cached operations rely on them and the reader needs prior knowledge of 
them to grasp the cached design.

\subsection{Overview}

In this section, we will list the main components that cached relies on. Per 
Archipelago policy, most of these components have been written in the xtypes 
fashion. Xtypes can be considered as modules for Archipelago that are designed 
to be reusable and reside in the shared memory segment.

The components of cached can be seen below:
 
\begin{itemize}
	\item xcache, an xtype that provides indexing support, amongst many other 
		functionalities.
	\item xworkq, an xtype that guarantees atomicity for execution of jobs on 
		the same object.
	\item xwaitq, an xtype that allows conditional execution of jobs.
\end{itemize}

and their design will be discussed in-depth in the following sections.

Also, we must note that the above components predate our cached implementation 
and are not a contribution of this thesis\footnote{xcache is an exception since 
	we have extended its functionalities for our purposes}. They are 
presented however in this thesis for clarity reasons. 

\subsection{The xcache xtype}\label{sec:xcache-design}

xcache is the most important component of cached. It is responsible for several 
key aspects of caching such as:

\begin{itemize}
	\item entry indexing,
	\item entry eviction,
	\item concurrency control and
	\item event hooks
\end{itemize}

In Figure \ref{fig:xcache-design.pdf}, we can see the design of xcache:

\diagram{Xcache design}{xcache-design.pdf}

Xcache consists of two hash tables. In the first hash table (a), xcache keeps 
an index of the currently active entries, whereas on the second hash table (b) 
it keeps an index of the entries that have been recently evicted but not 
removed, meaning entries that are in the process of removal but still have 
pending jobs.

The cache entries are located in a contiguous space (c) where each hash table 
points to. In order to acquire or release an entry, all we need is to pop or 
push an entry index from a special purpose stack (d). A more closer inspection 
of their design is presented in Figure \ref{fig:xcache-entry.pdf}. In this 
figure, each entry is marked with a different color for clarity reasons.

\diagram{Xcache entry design}{xcache-entry.pdf}

Every entry has the following important fields:
\begin{inparaenum}[(i)]
	\item Pointers to the previous (O) and next (Y) entries in the LRU 
		list,
	\item a reference counter and
	\item the entry's name.
\end{inparaenum}

Moreover, each entry has its own lock as well as a pointer to the entry's data, 
as set by the peer.

On the following subsections, we present the features of xcache as well as 
their design.

\subsubsection{Entry Preallocation}\label{sec:xcache-entry-design}

Since xcache indexes a bounded number of entries, there is no need to allocate 
them on-the fly using malloc/free. Considering that we are caching at RAM level 
and not at SSD level, the system call overhead will have a considerable impact 
on performance. Thus, in our case, we preallocate the necessary space in 
advance.

Also, note that we will use the term "cache node" from now on when we refer to 
a chunk of the preallocated space and "cache entry" when we refer to a cache 
node that has been associated with an object.

\subsubsection{Entry indexing}\label{sec:xcache-index-design}

The index mechanism that xcache uses is a hash table named xhash, which is also 
an xtype. The reasons why we have chosen to use a hash table are:

\begin{enumerate}
	\item Given that volume names are created pseudorandomly, we expect 
		that the hash values of the object names will have 
		approximately uniform distribution and by association, 
		practically zero hash collisions, as in most real-life 
		scenarios. If an adversary, on the other hand, was allowed to 
		choose its own names, that would be a different case.

		Thus, the insert, lookup and delete operations should occur 
		averagely in constant time.
	\item We don't need to do substring matches (which is an advantage of 
		tries).
	\item We don't need to traverse the entries sequentially (which is an 
		advantage of B-trees).
\end{enumerate}

The hash table that is used is heavily based on dictobject\cite{dictobject},
the Python dictionary implementation in C. What is interesting about 
dictobject, and is a feature of other hash tables too, is that it dynamically
resizes once it has reached the two thirds (2/3) of its capacity. We will 
attempt to explain what is resizing and how it affects us.

Resizing occurs to prevent performance degradation from the hash collisions 
that will likely ensue once the hash table reaches its maximum capacity. On 
resizing, the entries are typically rehashed, reinserted and the hash table is 
reallocated to double its capacity.

Resizing may seem as an expensive operation, but it is actually not, if we 
consider the following:
\begin{quotation}
Suppose that we insert \(n\) entries in a hash table that was created to hold 
only a fraction of \(n\), one (\(1\)) entry in the worst case.  Now, consider 
the case where the \(nth\) entry has triggered a resize:

In this case, the total number of computed hashes will be \(n\) plus the 
previous rehashes, which are \(n + n/2 + n/4 + n/8 + ...  + 1 = 2n\).  This 
means that the total rehashes are \(n + 2n = 3n\), while their 
\textbf{amortized} cost will be \(3n/n = 3\) hashes for each item, which is 
practically O(1) and thus negligible. The same amortized cost applies to the 
reinsertion of the entries, since the insertion of \(n\) entries in a hash 
table requires O(n) time.\cite{hash-resize}
\end{quotation}

Besides the hash table, which answers to the question "Where is the entry?" we 
also need another mechanism to answer the question "Is the entry still 
referenced?". xcache has such a mechanism which is commonly called "reference 
counting". Specifically, each entry has a counter that is 
incremented/decremented when a user accesses/releases an entry.

To sum up, when an entry is inserted, we use its name as a key and we update 
its refcount to 2, one reference from the user and one standard reference from 
the hash table. When we lookup for an entry, we use the entry's name as a key 
and then increment by 1 its refcount. 

\subsubsection{Entry eviction}\label{sec:xcache-eviction-design}

The decision to have xcache index a bounded number of entries means that when 
it reaches its maximum capacity and is requested to index a new entry, it has 
to resort to the eviction of a previously cached entry. Evicted entries are not 
removed immediately from xcache. They are instead set in an "evicted" state, 
their refcount is decremented by 1 and they are indexed by a special-purpose 
hash table until the user confirms that they can be removed. 

Xcache handles evictions in an interesting way. More specifically, evictions 
occur implicitly and not explicitly, meaning that the user (peer) does not have 
to evict entries manually. For example, when a user tries to insert a new entry 
to an already full cache, the insertion will succeed and the user will not be 
prompted to evict an entry manually. What has happened in the background, 
however, is that an entry has been evicted and the user will be notified about 
this eviction via a specific xcache event hook.

The scheme of implicit evictions and later on notification of the user, has the 
advantage that lookups, inserts and evictions can occur atomically by xcache.  
This would not be the case if the user was responsible for the evictions.

As for the eviction strategy, we have utilized an LRU queue. We have mitigated 
the cost of keeping the last reference for each entry by creating a simple LRU 
algorithm, which has O(1) complexity for all update actions.

The LRU queue that we have created is a very simple structure. It is a doubly 
linked list whose pointers reside in the xcache entry and its head (MRU) and 
tail (LRU) entries are stored in xcache.

There are four operations that are supported by this LRU:

\begin{enumerate}
	\item \textit{Insertions}, where we simply append the new entry to the
		head of the list.
	\item \textit{Evictions}, where we remove the tail of the list.
	\item \textit{Updates}, in which case we already know the entry we want 
		to update and have instant access to its pointers.  We can thus 
		unlink this entry from its neighbors and append it to the head 
		of the list.
	\item \textit{Removals}, which are similar to \textit{updates}. In this 
		case we also know the entry that is going to be removed, 
		meaning that we can unlink it and remove it safely.
\end{enumerate}

An illustration of this list can be seen in Figure \ref{fig:xcache-entry.pdf}.  
In this example, the references order is the following:

The least recently used entry is \textbf{obj9}, next is \textbf{obj1}, then 
\textbf{ob7}, then \textbf{obj5} and finally the entry that has been mostly 
recently used is \textbf{obj3}.

More about the implementation of the LRU algorithm can be found in Section 
\ref{xcache-evict-imp}.

\subsubsection{Concurrency control}

The concept of concurrency control has been discussed in Section 
\ref{sec:conc-theory}.  The goal of xcache is to handle safely - and preferably 
fast - simultaneous accesses to the shared memory.

In order to do so, we must first identify which are the critical sections of 
xcache, to wit, the sections where a thread modifies a shared structure. These 
sections are the following:

\begin{itemize}
	\item
		\textbf{Most indexing operations:} Inserts and removals can 
		modify the hash table (e.g. they can resize it, add more 
		entries or delete existing ones). This also means that lookups 
		must not run simultaneously with the above two operations.
	\item
		\textbf{Cache node claiming:} Before an entry is inserted, it 
		must	acquire one of the pre-allocated nodes from the 
		cache-node pool and we must ensure that this can happen 
		concurrently from all threads.
	\item
		\textbf{Entry migration:} An entry can migrate from one hash 
		table to the other e.g. on cache eviction. This migration 
		involves a series of xhash operations; removal from one hash 
		table and subsequent insertion to the other. These two 
		operations must occur atomically.
	\item
		\textbf{Reference counting:} Every entry must have a reference 
		counter.  Reference counters provide a simple way to determine 
		when an entry can be safely removed. Since many threads can 
		have access to the same entry, we must provide a way to update 
		the reference counters atomically.
	\item
		\textbf{LRU updates:} Most actions that involve cache entries 
		must subsequently update the LRU queue. These updates must also 
		occur atomically.
\end{itemize}

Let's see what guarantees we provide for each of the above scenarios:

\begin{itemize}
	\item
		\textbf{xhash operations:} We provide a lock for each hash 
		table. Only one thread can access each hash table at any time.
	\item
		\textbf{Cache node claiming:} The cache-node queue is also 
		protected by a lock.
	\item
		\textbf{Entry migration:} When an entry is migrated from one 
		hash table to the other, we always acquire the lock of the hash 
		table of active entries first and then the lock of the hash 
		table of the evicted entries. The order on which we take the 
		locks is very strict to avoid deadlocks.
	\item
		\textbf{Reference counting:} For the atomic increases and 
		decreases of a counter, we don't need a lock and its added 
		overhead. Instead, we can use the atomic get and atomic put 
		operations that the CPU provides.
	\item
		\textbf{LRU updates:} Since the majority of LRU updates take
		place when a new entry is inserted in the hash table, we can 
		protect our LRU under the same cache lock.
\end{itemize}

\subsubsection{Event hooks}\label{sec:xcache-hooks-design}

Since xcache is created to provide core caching functionalities for other 
peers, it must also notify them when it takes an implicit action that the peer 
is not aware of. In Section \ref{sec:xcache-eviction-design} we have seen one 
implicit action that xcache takes when a user inserts an entry, namely 
eviction. 

Besides this event, there are others. The complete list is the following:

\begin{description}
\item[cache node initialization:]
	This hook is triggered when a cache node is initialized. It is 
	triggered once only for each node, during the initialization phase of 
	xcache.
\item[cache entry initialization:]
	This hook is triggered when a cache entry has been inserted in the 
	cache.
\item[cache entry eviction:]
	This hook is triggered when a cache entry has been evicted from the 
	cache.
\item[cache entry reinsertion:]
	This hook is triggered when an evicted entry has been reinserted in the 
	cache.
\item[cache entry finalization:]
	This hook is triggered when an evicted entry's refcount has dropped to 
	0. This serves as a warning for the user who has the opportunity to let 
	the cache entry go or increment its refcount.
\item[cache entry put:]
	This hook is triggered when an evicted entry has been totally removed 
	from the cache.
\item[cache entry free:]
	This hook is triggered when a removed entry's cache node has been sent 
	back to the cache node pool.
\end{description}

For each of the above events, we have created the respective event hook. The 
peer that uses xcache may choose, if it wants, to use them and if so, it can 
plug its own event function for each hook which will be called when the event 
is triggered.

\subsection{The xcache flow}\label{sec:xcache-flow-design}

To make the way xcache works a bit more clearer, we will see the flow for four 
of the main xcache operations; insertion of a new entry, lookup of an entry, 
eviction and removal of an entry:

\subsubsection{Insertion}

The insertion process is described below:

\begin{enumerate}
	\item In order to insert a new entry, we must first allocate the space 
		for it. This means that we need to pop an entry index and use 
		it to initialize the entry it points to. If there are no 
		indexes left, we inform the peer.
	\item Next, we search the active entries to ensure that no other thread 
		has already inserted our entry. If however our entry is there, 
		we return and free the entry we have previously allocated.
	\item Then, we search the evicted entries to likewise ensure that our entry 
		has not been just evicted. In this case, we reinsert it and free the 
		entry we have previously allocated.
	\item If our entry is not in any of these two hash tables, we try to insert 
		it to the hash table of active entries.
	\item If the hash table is full, we resort to the eviction of another 
		entry and inform the peer.
	\item In either case, we update the reference count of our entry by 2, 
		one for the hash table and one for the request.
\end{enumerate}

\subsubsection{Lookup}

The lookup process is a lot simpler. To lookup an entry, we only need its name.  
If this name exists in the hash table of active entries, we update the 
reference count of the entry by 1 and we return with the entry index.

\subsubsection{Eviction}

When a cache entry is evicted, we do the following:

\begin{enumerate}
	\item Having acquired the lock of both hash tables, we remove the entry 
		from the active entries and put it to the evicted entries.
	\item We release these locks and inform the peer using the eviction hook.
	\item Once the peer has been informed, we decrease the refcount of the
		entry by 1, since it is no longer referenced by the hash table of 
		active entries.
\end{enumerate}

\subsubsection{Removal}

The removal of an entry occurs only when its refcount has reached 0. More 
specifically:

\begin{enumerate}
	\item We inform the peer that the entry is about to be removed with the
		finalization hook.
	\item We then proceed to check again the refcount of the entry.
	\item If it is more than zero, this means that the entry has been 
		reinserted or that the peer has taken an action in the 
		finalization hook. In this case, we can leave.
	\item Else, we can safely remove that entry from the evicted entries and 
		inform the peer.
\end{enumerate}

\subsection{The xworkq xtype}\label{sec:xworkq-design}

The xworkq xtype is a useful abstraction for concurrency control. Its purpose 
is to enqueue "jobs" (protected by a lock) and ensure that only one thread will 
execute them. There is no distinction as to which thread this will be, as well 
as no execution condition. The execution thread is simply the one that acquires 
the lock first.

xworkq is generally used when multiple threads want simultaneous access to a 
critical section. Instead of spinning indefinitely, waiting for a thread to 
finish, they can enqueue their job in the xworkq and resume processing other 
requests. xworkq is also generic by nature, since the "job" is simply a target 
function and its input data.

On Figure \ref{fig:xworkq-design.pdf}, we can see the design of xworkq:

\diagram{Xworkq design}{xworkq-design.pdf}

It consists of a queue where jobs are enqueued.  The thread that enqueues a job 
can attempt to execute it too, by acquiring a lock for the xworkq. If the lock 
is unheld%yes, the word exists...
, the thread will acquire it and will be able to execute the enqueued job.  
Else, it can safely leave and its job will be executed by the thread that holds 
the lock.

In cached context, every object has an xworkq. Whenever a new request is 
accepted/received for an object, it is enqueued in the xworkq and we are thus 
ensured that only one thread at a time can have access to the object's data and 
metadata.

\subsection{The xwaitq xtype}\label{sec:xwaitq-design}

The xwaitq xtype bears some similarities to the xworkq xtype. Like xworkq, it 
is also an abstraction where "jobs" are enqueued and dequeued later on to be 
executed. Unlike xworkq though, jobs are executed only when a predefined 
condition is met. Another distinction is that the jobs in xwaitq are assumed to 
be thread-safe and can be executed concurrently by any thread.  

xwaitq is commonly used in non-critical code sections that can be executed only 
under specific, predefined circumstances. The "jobs" that are enqueued in 
xwaitq are the same as the jobs of xworkq.

The design of xwaitq is illustrated in Figure \ref{fig:xwaitq-design.pdf}.

\diagram{Xwaitq design}{xwaitq-design.pdf}

Unlike xworkq, before a job enters the waitq, the thread can attempt to execute 
it by checking the execution condition. Only if the condition is \textbf{not} 
met does the thread enqueue the job to the queue. Before the thread leaves, it 
"signals" the queue and essentially rechecks the condition to ensure that it 
can't be executed. It can then safely leave since its job will be executed when 
another thread signals the queue successfully.

In cached context, xwaitqs are used to enqueue jobs which cannot be executed 
immediately. Common cases are when we have run out of space, when we have run 
out of requests etc.

\section{Cached Design}\label{sec:cached-design}

At this point, we have discussed in depth the design of the cached components.  
Having the above sections in mind, we can present how cached has been designed. 
We will illustrate the design of cached from two different perspectives: the 
operational perspective, which can be seen in Figure 
\ref{fig:cached-design.pdf}, and the component perspective, which is properly 
discussed in Section \ref{sec:cached-flow-design} and can be seen in Figure 
\ref{fig:cached-design-comp2.pdf}.

\diagram{Cached design - operations}{cached-design.pdf}

Cached has been been designed mainly as the orchestrator, a peer that utilizes 
several different components to handle various tasks such as indexing (xcache), 
concurrency (xworkq) and deferred/conditional execution (xwaitq). Cached 
however is not limited to the above role as these components do not cover all 
of the needed tasks. There are several other key tasks that cached must 
undertake, namely:

\begin{itemize}
	\item \textit{Request handling}, which is how cached handles requests 
		from other peers and sends its own.
	\item \textit{Write policy enforcing}, which is the enforcing of a 
		cache write policy (write-through, write-back).
	\item \textit{Data propagation}, which controls how data changes are 
		propagated to the slower medium.
\end{itemize}

Moreover, cached extends its repertoire using some unique components that have
been specifically crafted for it:

\begin{enumerate}
	\item Bucket pool, which is a preallocated space from where cached can draw 
		resources for the object's data.
	\item Utilities for asynchronous task execution.
	\item Utilities for various book-keeping purposes.
\end{enumerate}

We will further explain how cached manages the above new components and tasks 
in the following sections.

\subsection{Request handling}

Cached operates as a peer that receives requests from the vlmcd. The majority of 
these requests will be read/write requests, but there are other types of 
requests too such as copy requests (sent when an object is copied-on-write) and 
info requests i.e. queries on what is the size of an object. Each of these 
requests must be handled independently, using special-purpose functions.

Furthermore, cached will also issue requests to the blocker mainly on two 
occasions: when it flushes a dirty object and when it operates in write-through 
mode.

This means that cached must be able to categorize requests and send them to the 
appropriate functions. Moreover, it must be able to create requests of its own, 
as well as handle cases such as running out of requests.

\subsection{Write policy enforcing}\label{sec:cached-wcp-design}

The user defines beforehand the write policy of cached. There are two options: 
write-through and write-back. These policies aren't new and have already been 
discussed in Section \ref{sec:wp-triad}, but let's see what these policies 
translate to in cached context.

\begin{itemize}
	\item
		In \textbf{write-back} mode, cached caches writes, immediately 
		serves the request back and marks the data as dirty. When a read 
		arrives, it either serves the request with the dirty data 
		(read-hit) or forwards the request to the storage peer and 
		caches the answer (read-miss).

		This policy is used when we want to improve read and write speed 
		and can sacrifice data safety.
	\item
		In \textbf{write-through} mode, cached forwards writes to 
		blocker, servers the request when blocker replies, caches the 
		data and marks them as valid.  When a read arrives, it either 
		serves the request with the valid data (read-hit) or forwards 
		the request to the storage peer and caches the answer 
		(read-miss).

		This policy is used when we want to improve read speed and want 
		to make sure that no data will be lost.
\end{itemize}	

These policies are specified once during cached's deployment and cannot be 
switched on/off later on.

\subsection{Data propagation}

In order to ensure that we have no data corruption, cached must provide the 
following two guarantees:
\begin{inparaenum}[(i)]
\item consistency of cached data and
\item correct propagation of data updates to the storage backend.
\end{inparaenum}

\begin{comment}
Cache coherence is a concept that is closely connected with concurrency 
control, as is evident in Figure \ref{fig:cached-design.pdf}. It is commonly 
used as a term in the literature on the subject of caches for multiprocessor 
systems or distributed caches and it refers to:

\begin{enumerate}
	\item the consistency of data that are stored in the cache and
	\item how each change is propagated through the system, to the other 
		storage tiers.\label{list:second-coherence}
\end{enumerate}

If we wanted to rephrase the above to match our case, we could say that it 
refers to the consistency of the data that are stored in cached and how each 
change is propagated to the blocker.
\end{comment}

As for the first guarantee, we have explained in section 
\ref{sec:xworkq-design} that the consistency of the cached data is greatly 
secured using the xworkq as the guard of parallel accesses to the object's 
data.

\begin{comment}
The problem is evident if you consider the eviction of an object with dirty 
data. xcache does not operate on object level and thus is unaware of the 
contents of the cache entry. Thus, cached must use the cache entry finalization 
hook to increment the refcount of the object so that it can not be removed 
until all data have been flushed to the blocker. Essentially, cached overrides 
the xcache's book-keeping to maintain cache coherency.
\end{comment}

As for the second guarantee, one might consider that flushing the dirty data of 
an object would be sufficient to correctly propagate data changes to the 
blocker.  However, consider the scenario when a dirty object is evicted and 
then subsequently reinserted, overwritten and evicted again. The result would 
be two write requests for the same data sent to the blocker, that we cannot be 
sure about the order they will be executed.

To solve this problem, flush jobs must be deferred (using an xwaitq) if another 
flush for the same object is in flight. 

\subsection{Bucket pool}

% Explain what buckets are:
% a) That they are bounded
% b) That they have states
% c) That they are preallocated and every thread enqueues and dequeues an index 
% to them

We have already explained in Section \ref{sec:rationale-design} the reason why 
cached caches objects. There is, however, one important design issue that we 
must address. This issue is how will cached perceive the object's data. To make 
it a bit clearer, given that an object typically has 4MB of size, what should 
cached do when a user requests e.g. a 16KB chunk of it?

If we perceived the object's data as a monolithic chunk, we would need to read 
and cache the whole object just to reply to the user's request. If the user 
then requests a chunk from another object, we would have to cache that object 
too and in the end, we would thrash our cache
\footnote{
	cache thrashing occurs when we aggressively cache data that is only 
	used once and effectively leads to a snowball of evictions
}.

The solution we propose is to further divide objects to the next and final 
logical entity, \emph{buckets} (typically 4KB of size). Each bucket consists of 
its data and metadata and cannot be half-empty, or half-allocated. This way, we 
can also know which parts of the cached object are actually written, or are in 
the process of being read etc.

Thus, our solution to the hypothetical problem we have posed above is to 
request 16KB from the blocker, store the result in 4 buckets and then respond 
the request to the user.

The above answer, however, is not entirely complete. It implies that buckets 
are something readily available or attached to the object. Although each object 
\textit{could} have its buckets pre-allocated, this would limit the objects 
that we can cache since that, even if the user requested only a small chunk, 
each object would statically need 4MB of space.

Ideally, we would like to be able to cache thousands of objects but
\begin{inparaenum}[(i)]
\item allocate a much smaller amount of buckets and
\item strictly when the user requests to.
\end{inparaenum}
To make things even faster, we would also like the buckets to be preallocated 
(like cache nodes in Section \ref{sec:xcache-entry-design}) to avoid the 
overhead of malloc/free system calls.

We have achieved the above by creating a bucket pool. The design of the bucket 
pool is the same as the design of cache entries, as shown in Figure 
\ref{fig:xcache-design.pdf}. More specifically, the bucket pool size is static 
and has been set during initialization by the administrator. After the 
necessary space has been allocated, it is divided in buckets and all the bucket 
indexes are pushed in a lock-protected stack. Then, each thread can pop bucket 
indexes from the pool and attach them to an object when needed. When that 
object is evicted, its attached buckets indexes are pushed back to the pool.

\subsection{Asynchronous task execution}

There are cases when a request cannot be processed immediately e.g. when it 
needs to allocate the necessary buckets for the object's data. In these cases, 
we use a utility called \texttt{cache-io}, which
\begin{inparaenum}[(i)]
\item holds the original request and
\item describes the asynchronous task that will be executed.
\end{inparaenum}. In order to execute a task, we simply need a function pointer 
to the entry point of this task and the necessary arguments.

Using Cache-IOs, the request can break in more than two flows of execution, 
since the asynchronous task can allocate new requests for its own purposes.  In 
order to know when the asynchronous task has finished, we keep track of the 
number of \emph{pending} requests that it has spawned.

\subsection{Book-keeping utilities}\label{sec:cached-states-design}

In order to know information such as when an object is in flushing state, when 
a bucket has been allocated or when a bucket is being read etc., cached must 
employ some sort of book-keeping of their states. The entities that require to 
track their states are:

\begin{enumerate}
\item \textbf{Buckets:} Each bucket has two different states that must be 
	tracked. The first is its allocation state:
	\begin{enumerate}[i)]
		\item free, meaning that the bucket is not allocated.
		\item claimed, meaning that the bucket is allocated.
	\end{enumerate}
	The second is its data state:
	\begin{enumerate}[i)]
		\item invalid, meaning that it currently holds no data.
		\item valid, meaning that it has data that are in sync with the
			storage backend.
		\item loading, meaning that a read request that includes this bucket 
			has been sent to the blocker.
		\item writing, meaning that the bucket contents are being 
			written/flushed to the blocker.
		\item dirty, meaning that its data are newer than the data of 
			the storage backend.
	\end{enumerate}
\item \textbf{Objects:} The bucket states of an object provide a good 
	indication of the object's status, yet not a complete one. The statuses 
	we keep for the objects are:
	\begin{enumerate}[i)]
		\item ready, meaning that it is ready to accept data.
		\item invalidated, meaning that it is not ready to accept data.
		\item flushing, meaning that it is currently flushing dirty data to the 
			blocker.
		\item failed, meaning that a request has failed for this object and we 
			must stop using it.
	\end{enumerate}
\item \textbf{Cache-IOs:} Cache-IOs also have states. They are the following:
	\begin{enumerate}[i)]
		\item reading, meaning that the task wants to read data.
		\item writing, meaning that the task wants to write data.
		\item served, meaning that the task has been served the data it needed.
		\item failed, meaning that the task has failed.
	\end{enumerate}
\end{enumerate}

Moreover, we keep global and per-object counters of every object's bucket 
states. The benefits from this approach is that we can know at any time if an 
object (or cached in general) has a bucket in a certain state.

\section{Cached Flow}\label{sec:cached-flow-design}

In this section, we will discuss how cached handles read/write requests.  
Through the explanation of the cached flow, we hope to reassemble in the 
reader's mind the puzzle pieces that we have laid out in the previous sections 
of this chapter.

In Figure \ref{fig:cached-design.pdf}, we have essentially assorted the flow of 
cached in a set of logical operations. Now that we have explained all cached 
components, we can link these operations to the components that we have 
designed. The connection between cached operations and components is 
illustrated in Figure \ref{fig:cached-design-comp2.pdf}.

\diagram{Cached design - components}{cached-design-comp2.pdf}

We will explain each step of the cached flow, as is presented in Figures 
\ref{fig:cached-design.pdf} and \ref{fig:cached-design-comp2.pdf}. Note that we
will focus on the read/write requests and not the copy/info requests.

\subsection{Main steps}

\subsubsection{Request handling}

Initially, cached receives an XSEG request from vlmcd. Cached then checks the 
type of the request and it forwards it to the right channels.

\subsubsection{Indexing}

Next, cached uses the name of the request's target to check if it has been 
indexed (lookup). 

\begin{itemize}
	\item If the object is in the cache, it stores the returned handler for 
		later references to the object data.
	\item If not, it allocates a new entry handler and indexes this handler 
		in xcache (insert).
\end{itemize}

In either case, the returned handler is used for later references to the 
object's data.

From this point on, there are three paths that the request may follow:

\begin{enumerate}
	\item The \textit{"Conditional execution"} path. This usually occurs 
		when an outside situation does not allow our request to 
		continue. Typically, this outside condition is the depletion of 
		cache entries, buckets or XSEG requests.
	\item The \textit{"Data propagation"} path. This usually occurs when 
		the indexing of our object has resulted in an eviction of 
		another object, whose data must be flushed.
	\item The \textit{"Concurrency control"} path. Concurrency control 
		refers to the access to the object's data and is the default 
		next step for a request.
\end{enumerate}

\subsubsection{Concurrency control}

After cached has managed to acquire a valid entry handler for the object, it 
enqueues the appropriate read or write work to the object's workq.  Once this 
job is executed, it runs exclusively and can modify safely the object's data.

The first thing that cached attempts to do is to claim the necessary buckets 
for the request. Knowing the following:
\begin{inparaenum}[(i)]
\item bucket size,
\item request size and
\item request offset,
\end{inparaenum}
cached can determine the bucket range that it must claim.

To claim a bucket, cached pops bucket handlers from the bucket pool. If the 
pool has been depleted, the request cannot proceed and it must wait until a
bucket is freed. Again, this is covered in the "Conditional execution" step.

Once cached has claimed the necessary buckets, it can perform the requested 
operation on them. The way an operation is executed however, is tightly coupled 
with the cache write policy. Moreover, depending on the cache write policy, 
cached may need to issue a new request for the object's data. Thus, the request 
handling step inevitably blends in this step.

There are four possible combinations, which are in part explained in Section 
\ref{sec:cached-wcp-design}:

\begin{enumerate}
	\item \textbf{\{Write operation, write-back policy:\}} \hfill \\
		The data are written to the object, they are marked as dirty 
		and the request is completed.
	\item \textbf{\{Write operation, write-through policy:\}} \hfill \\
		Cached forwards the write request to the blocker. Once the 
		request returns, it fills the claimed buckets with the object 
		data and completes the request.
	\item \textbf{\{Read operation, write-back policy:\}} \hfill \\
		\textbf{\{Read operation, write-through policy:\}} \hfill \\
		The behavior for reads is the same, regardless of the policy.
		Essentially, if the requested data are cached, cached can 
		instantly complete the request. Else, it creates a new request, 
		using cache-io, and sends it to the blocker. After the request 
		is replied, it loads the missing data in the claimed buckets 
		and then it completes the original request.
\end{enumerate}

\subsection{Optional steps}

\subsubsection{Conditional execution}

As we have seen in the previous steps, since resources are finite, the requests 
cannot always be processed immediately. The common way that cached handles 
these cases is through the usage of xwaitqs.

More specifically, when a resource has been depleted, we store the original 
request in a Cache-IO struct and we enqueue a job in the appropriate xwaitq.  
This job will be executed once the resource becomes replenished and our request 
will resume.

\subsubsection{Data propagation}

When an object is evicted, cached does the following:

\begin{enumerate}
	\item It quickly checks if the object is dirty or not (using the cache 
		entry finalization hook)
	\item If the object is dirty, cached updates its reference count and it 
		checks if there is a pending flush for it.
		\begin{enumerate}[i)]
			\item If there is a pending flush, we wait until that 
				flush is finished (waitq).
			\item Else, we mark the buckets as \textbf{WRITING} and 
				we flush their contents to the blocker.
		\end{enumerate}
	\item After the flush has finished, we once again check if the object 
		is dirty (the cache entry finalization hook is once more 
		triggered):
		\begin{enumerate}[i)]
			\item If the object is clean, then we do nothing and 
				the object will be freed.
			\item Else, we go back to step 2.		
		\end{enumerate}
\end{enumerate}

