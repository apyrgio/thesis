\chapter{Necessary theoretical background}\label{ch:theory}

In this typically boring section, we will explain the basic system facilities 
and programming concepts that are used by our implementation and Archipelago in 
general. More specifically, in Section ?...

\section{Multithreading}

Mutlithreading is a programming concept that has been the subject of research 
long before the emerge of SMP systems
\footnote{Symmetric multiprocessing systems, commonly systems with multiple 
	processors}
. More specifically, temporal multithreading has been introduced in the 1950s 
whereas Simultaneous Multithreading (SMT), which is the current invocation of 
multithreading programming, was first researched by IBM in 1968\cite{mt}.

Before threads, programs could take advantage of SMP systems using forked 
processes that would communicate with each other. The introduction of threads 
did not render this practice obsolete, but instead provided an alternative 
technique to speed up applications.

Threads and process also have some fundamental differences, which are shown in 
the following list:

\begin{itemize}
	\item Threads are always parts of a process, whereas processes are 
		independent from each other and may only have a parent-child connection 
		between them.
	\item Forked processes have their own address space and resources, which 
		are inherited by the parent process with CoW semantics.  Multiple 
		threads, on the other hand, usually share the same memory and resources 
		with the other threads in the same process.
\end{itemize}

From the above differences, we can see that there are no clear advantages of a 
multithreaded approach over a multiprocess one. To better demonstrate our 
point, we will present the advantages and disadvantages of multithreading 
programming in the following lists:

The advantages are:

\begin{itemize}
	\item Context switching is generally faster between threads, due to the 
		fact that the TLB
		\footnote{Translation Lookaside Buffer, a hardware cache that speeds up 
			the translation of virtual addresses to physical RAÎœ pages.}
		cache does not need to be flushed
	\item Sharing data between threads is easier, due to the fact that they use 
		the same memory by default (however, this is also a disadvantage, as we 
		will explain below)
\end{itemize}

Whereas the disadvantages are:

\begin{itemize}
	\item Processes are more isolated that threads, which means that there is 
		no need to ensure that functions are thread-safe. Moreover, the whole 
		process can come to a halt, when one of the threads encounters an 
		error.
	\item Processes can update their structures independently and then lazily 
		inform the other processes about these changes. Threads on the other 
		hand need to be careful and employ some form of costly concurrency 
		control.
\end{itemize}

Regardless of the chosen method, at some point the programmer will have face 
two of the biggest challenges of multithreading/multiprocess programming;
interprocess communication, which is discussed in Section \ref{sec:ipc-theory}, 
and concurrency control, which is discussed in Section \ref{sec:conc-theory}.

\section{Interprocess Communication - IPC}\label{sec:ipc-theory}

Interprocess Communication is a concept that predates the SMP systems that we 
all use nowadays. It is a set of methods that an OS uses to allow processes and 
threads to communicate with each other. Archipelago for example, uses 
extensively IPC methods to synchronize its different components.

The full list of Linux's IPC methods is presented below:

\begin{itemize}
	\item \textbf{Signals:} they are sent to a process to notify it that an 
		event has occurred.
	\item \textbf{Pipes:} one-way channel that transfers information from 
		one process to the other.
	\item \textbf{Sockets:} bidirectional channels that can transfer 
		information between two or more processes either locally or 
		remotely through the network.
	\item \textbf{Message queues:} asynchronous communication protocol that 
		is used to exchange data packets between processes.
	\item \textbf{Semaphores:} Special purpose pipes that are used mainly 
		for process synchronization.
	\item \textbf{Shared memory:} a memory space that can be accessed and 
		edited by more than one process.
\end{itemize}

We will concentrate on the following IPC methods:
\begin{inparaenum}[i)]
\item signals,
\item sockets,
\item message queues and
\item shared memory,
\end{inparaenum}
since these are the methods that Archipelago and our implementation have used.

\subsection{Signals}

Signals are notifications that are sent to processes and can be considered as 
software interrupts. The signals' purpose is to interrupt the execution of the 
processes and inform it that an event has occurred.

Given that there more than one events and exceptions that can occur in a 
system, there are also various signals that correspond to each one of these
events. For more information about the signals that Linux supports as well as 
the conditions on which they are raised, the reader is prompted to consult the 
man pages for signal(7) or read the POSIX.1-1990, SUSv2 and POSIX.1-2001 
standards.
  
Moreover, the above standards dictate the standard behavior of a process when a 
signal is received. The standard actions that a process can take fall roughly 
in the following categories:

\begin{enumerate}
	\item ignore the signal,
	\item pause its execution,
	\item restart its execution or
	\item stop its execution and/or dump its core
\end{enumerate}

However, a process is not limited to this set of actions. It can instead do one 
of the following things, regardless of the signal that has been raised and its 
severity:

\begin{itemize}
	\item ignore the signal
	\item install a custom signal handler function, which essentially 
		passes the signal handling task to the process.
\end{itemize}

Finally, the process can block the signal, by setting the SIG\_BLOCK flag for 
this signal in the process's signal mask. This is used in Archipelago and is 
described in in Section \ref{sec:poll-archip}.

\subsection{Sockets}

Socket are a bidirectional means of sending data between processes. The 
processes can be in the same host but most commonly, they are in remote hosts 
and the data are sent over the network. Furthermore, from all the IPC methods 
that we have described above, sockets are the only method that enables remote 
communication. 

There are many socket implementations for different purposes, which are divided 
in several communication domains, most of which are rather obscure. The three 
communication domains, however, that are supported by most UNIX and UNIX-like 
operating systems are:

\begin{itemize}
	\item \textit{IPv4} domain, which allows communication between 
		processes over the Internet Protocol version 4 network.
	\item \textit{IPv6} domain, which allows communication between 
		processes over the Internet Protocol version 6 network.
	\item \textit{UNIX} domain, which allows communication between 
		processes in the same host
\end{itemize}

The above three communication domains are further divided in two types, based 
on the transport layer protocol that they use.

\begin{itemize}
	\item \textit{Stream sockets}, which use the Transmission Control 
		Protocol (TCP) or Stream Control Transmission Protocol (SCTP),
	\item \textit{Datagram sockets}, which use the User Datagram Protocol 
		(UDP),
\end{itemize}

The TCP/UDP protocols are only one layer out of the four layers that the TCP/IP 
protocol suite defines, and we will explain them in detail in the following 
sections. Although a thorough explanation of the TCP/IP protocol is out of the 
scope of this thesis and is not needed to understand the following sections, we 
will mention some of its most important aspects for the sake of completeness.

The TCP/IP protocol suite is the basis for the World Wide Internet and the most 
used form of networking. It specifies all the stages of the data processing 
that need to happen in various levels and entities, such as operating systems, 
network cards, routers etc. in order to connect two machines over the network.  
For this reason, the data that are sent are encapsulated in layers, which can 
be seen in Figure \ref{fig:data-encapsulation.pdf}.

\diagram{Data encapsulation for a UDP packet}{data-encapsulation.pdf}

We now continue with an presentation of TCP and UDP protocols.

\subsubsection{TCP}

The Transmission Control Protocol is connection-oriented, i.e. it provides 
unique connection between two sockets, and has the following key features:

\begin{description}
	\item[Reliability] The data will arrive to the receiver with no packet 
		loss, or they will not arrive at all. In the latter case, the 
		receiver may receive sparious packets but it will not 
		acknowledge them until it has received all of them.
	\item[Ordered transfer] The data will arrive in the same order that 
		they were sent.
	\item[Error-checking] The data are checksummed to allow the receiving 
		end to check if there was any data corruption.
	\item[Rate-limiting] When the receiver accepts packets with slower rate 
		than the sender, the sender will adjust its rate to ensure 
		packet delivery and less congestion.
	\item[Byte-stream] The data that are sent do not have a boundary.
\end{description}

\subsubsection{UDP}

The User Datagram Protocol on the other hand has far less restrictions than 
TCP. First, it is connectionless, meaning that the socket can receive requests 
from anyone. Second, it provides no guarantees about the delivery of the 
messages. Third, the messages can arrive in other order than the one they were 
sent. Fourth, there is no rate-limiting, meaning that the congestion control 
must be handled in the application level. Finally, it sends datagrams instead 
of bytes, which have definite boundaries.

Although there might seem that there is no reason for one to choose the 
unreliable UDP protocol, the lack of the TCP overhead makes it an ideal choice 
for applications that value speed over packet loss.

\subsection{Shared memory}

When two or more processes share the same memory segment, they can exchange 
data by placing it in a region of the segment. The data then becomes instantly 
visible to the other processes too, since their page-table entries for this 
segment point to the same RAM pages.

The way shared memory is created is analogous to a disk file creation:

\begin{enumerate}
	\item The process uses \texttt{shm\_open()}, instead of a regular 
		\texttt{open()}, in order to create a new file in /dev/shm or 
		open an existing one.
	\item Then, in both cases, it can use \texttt{ftruncate()}, which 
		resizes the file to the desirable size.
\end{enumerate}

At this point, the process is allowed to seek into the file and write to it. In 
the shared memory case however, it can also use \texttt{mmap()}, to map this 
file to its address space. There are two types on mapping:

\begin{itemize}
	\item \textit{Private mapping}, in which case the mapping contents will 
		not be visible to other processes that have mapped the same 
		file and
	\item \textit{Shared mapping}, in which case the mapping contents will 
		be visible to all processes that map this file and changes to 
		the mapping will be propagated to the file.
\end{itemize}

Finally, an issue with mappings is that the start of the shared memory is not 
always mapped in the same virtual address for all processes. For this reason, 
when processes want to share data, they should not pass direct pointers to 
them, but relative pointers (i.e. offsets) from the start of the segment, which 
are common for all processes and can be translated to the correct direct 
pointers.

\section{Concurrency control\label{sec:conc-theory}}

Concurrency control is the set of methods that a program uses to ensure that 
concurrent accesses to the same data will leave them in a consistent state.

There are several techniques that are used for concurrency control and are 
listed below:

\begin{itemize}
	\item \textit{Spinlocks}, which are locks that protect a critical segment.  
		Typically, a thread acquires a lock at the start of the critical 
		segment and releases it at the end of it. Threads that are waiting for 
		the lock essentially "spin", i.e. they busy-loop until the lock is 
		released.
	\item \textit{Mutexes}, which are locks that protect a critical segment in 
		the same fashion as spinlocks. Their difference from spinlocks, however 
		is that if a thread cannot get the lock, it will block instead of 
		busy-loop.
	\item \textit{Semaphores}, which are also an IPC method. In concurrency 
		control context, they are pipes that restrict the number of 
		simultaneous accesses to a resource or a critical segment. When the 
		number of times is one (1), they essentially degenerate to mutexes, 
		with the main difference that they can also be used by processes,
		besides threads.
	\item \textit{Atomic operations}, which are hardware-assisted operations 
		whose purpose is to atomically update a value as fast as possible and 
		atomically. The atomicity is usually achieved with implicit hardware 
		locks on the bus or cache-line. Atomic operations come in many flavors 
		such as "add-and-fetch", "compare-and-swap" etc.
\end{itemize}

Concurrency control - and locking in particular - have three important caveats 
that the programmer needs to know before he/she decides on the techniques that 
will be used:

\begin{description}
	\item[Lock overhead]
		Lock overhead is the overhead that the locking mechanism introduces.  
		For example, semaphores are a mechanism with big overhead, since they 
		must be read and written to using system calls. If the critical segment 
		they protect is simply the update of a variable, then the programmer is 
		probably better off using a spinlock or atomic operations.
	\item[Lock contention]
		Lock contention can be considered as the "design overhead" of 
		concurrency control, and has to do with the coarseness of the lock.  
		There is contention for a lock when it is requested by many threads, to 
		the point that the waiting time is longer than the execution time.
		This has a big performance impact to implementation, which usually 
		needs to be redesigned in order to break such locks into smaller ones.
	\item[Deadlocks]
		Deadlock is a situation in a multi-lock scenario where each process in 
		a group of processes needs to acquire a lock which is held by another 
		process in the same group. Since no process can continue, the operation 
		of the group is essentially stalled.
		As a rule of thumb, the circular dependency of the locks can break if 
		the locks are acquired in a predefined order. This however is only 
		possible in less complex scenarios and in general, a more well-though 
		design is required.
\end{description}

\section{Networking}

Explain TCP/UDP, sockets, polling
