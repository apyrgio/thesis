\chapter{Performance evaluation of cached}\label{ch:cached-evaluation}

\begin{flushright}
	\textit{"There are three kinds of lies:\\
		lies, damned lies, \\
		and \sout{statistics} benchmarks."}	\\

	Mark Twain (modernized)
\end{flushright}

It may seem as an ironic statement, considering that we are about to provide 
benchmark results for cached, but it's actually is a valid one. What Mr.  Twain 
tries to say here
\footnote{
	and that's a phrase usually not heard in programming contexts...
}
is that the presentation of partials facts for something can be used to 
fabricate a plausible truth for it.
%This is shit, fix it
In science's case, it so often happens that promising results for an experiment 
can seem more important to the researcher's eye than negative ones due to 
positive reinforcement.

In out case, we will try not to merely smear the next pages with diagrams but 
first explain the benchmarking methodology behind them.

The skeleton of this chapter is the following: Section \ref{sec:perf-meth} 
explains the methodology behind our measurements. Section \ref{sec:perf-plot} 
presents the results of the benchmarks that we have done and provides in-depth 
explanations about each of them. Finally, Section ? is undefined.

\section{Benchmark methodology}\label{sec:perf-meth}

The benchmarks that have been executed and whose results are presented in this 
chapter, will be split in two categories, both of which have their own distinct 
goals:

The first category is the comparison between using cached on top of the sosd 
(sosd has been discussed here ?) and using solely the sosd as the Archipelago 
storage. The category's goal is to "defend" one of the core thesis arguments, 
that tiering is a key element that will improve the performance of Archipelago. 

In order to compare effectively the performance of cached and sosd, we must 
consider the following: 

\begin{enumerate}
	\item The comparison of the two peers should try to focus on what is 
		the best performance that these peers can achieve for a series 
		of tough workloads.
	\item The circumstances under which both peers will be tested need not 
		be thorough but challenging. For example, it may be interesting 
		to test both peers against sequential requests, but
		\begin{inparaenum}[i)]
		\item such patterns are rarely a nuisance for production 
			environments
		\item they do not stress the peers enough to provide something 
			conclusive
		\item they are out of the scope of this section as there can be 
			many of them and adding them all here will impede the 
			document's readability.
		\end{inparaenum}
		
	\item Both peers must be tested under the same, reasonable workload, 
		i.e a workload that can be encountered in production 
		environments.
	\item If the peer doesn't show a consistent behavior for a workload, it 
		must be depicted in the results.
\end{enumerate}

Having the above in mind, the next step is to choose a suitable workload.  This 
choice though is fairly straight-forward; in production environments, the most 
troublesome workload is the stampede of small random reads/writes and is 
usually the most tested scenario.  

One may ponder however, how many requests can be considered as a "stampede" or 
which block size is considered as "small". Of course, there is not only one 
answer to this question so, we will work with ranges. For our workload, we will 
use block sizes ranging from 4KB to 64KB and parallel requests ranging from 4 
to 16.

The second category deals solely with the inner-workings of cached and its 
behavior on different operation modes or states. Its aim is not to capture the 
performance against a tough workload, but to explain \textbf{why} this 
performance is observed and how each of the options affect it. For example, we 
will measure things (blarg?) such as writethrough mode vs writeback mode, 
single-threaded vs multi-threaded etc.

Also, here is the following list is the options of cached that affect the 
measurements: 

\begin{enumerate}
	\item Bucket size
	\item IOdepth
	\item Cache size
	\item Max cached objects
	\item Write policy
	\item ...
\end{enumerate}

Finally, in the following sections for brevity reasons we will talk about 
comparing cached and sosd. What the reader must keep in mind however is that 
cached is the cache layer above sosd.  

\section{Specifications of test-bed}

The specifications of the server on which we conducted our benchmarks is the 
following.

\begin{table}[H]
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Component & Description \\ \hline \hline
		CPU &  2 x Intel(R) Xeon(R) CPU E5645 @ 2.40GHz \cite{e5645} \\
		 & Each CPU has six cores with Hyper-Threading enabled, which equals to 
		 24 threads. \\ \hline
		RAÎœ & 2 banks x 6 DIMMs PC3-10600 \\
		& Peak transfer rate: 10660 MB/s \\ \hline
	\end{tabular}
	\caption{dev100 specs}
	\label{tab:specs}
\end{table}

Also, mention that we evaluated both peers by sending requests directly at 
their request queues

\section{Performance comparison between cached and sosd}

As mentioned above, for our first test, we will evaluate the read and write 
performance of cached and sosd for a random workload with parallel requests of 
small size. In order to measure accurately their performance, we will use two 
different metrics:

\begin{description}
	\item[Bandwidth:] We will measure the bandwidth for the bulk of our 
		requests. This is a metric from the peer's perspective that reflects 
		how much request size requests our peers can handle per second.
	\item[Latency:] We will measure the average time the requests need to be 
		served. Unlike bandwidth, this metric reflects the responsiveness of 
		the implementation. For example...
	\item[IOPS:] (Should we include them?)
\end{description}

Let's see now the bandwidth performance of our peers. The write performance can 
be seen in Figure \ref{fig:bw-write-comp-lie.pdf} while the read performance 
can be seen in Figure \ref{fig:bw-read-comp-lie.pdf}.

\diagram{bw-write-comp-lie.pdf}{Comparison of bandwidth performance for writes}
\diagram{bw-read-comp-lie.pdf}{Comparison of bandwidth performance for reads}

Before we proceed with the interpretation of the diagram results, we will 
briefly comment on the diagram structure. Due to the fact that the performance 
of the two peers differs in at least two orders of magnitude, the results would 
look too flat in a conventional diagram that would scale from 0 to 11000. To 
amend this, we have broken the y-axis of our diagrams in two parts with 
different scales and starting values, in order to make the comparison easier to 
the eye.

We will begin the diagram interpretation with the bandwidth performance of the 
two peers. First, let's see the speedups of write and read requests with the 
addition of cached. For write requests, the speedup for very small block sizes 
(4KB - 16KB) is approximately 100x whereas for larger ones (32KB - 64KB) it 
ranges from 50x to 200x. For read requests, the speedup for very small block 
sizes is approximately 50x, whereas for larger block sizes it ranges between 
20x - 75x.

From these first two diagrams we can extract the following points:

\textbf{Why is there such a vast improvement?}

(Explain that it's because we don't write past cache's size)

\textbf{Where is the speedup difference between read and writes attributed to?}

The speedup difference is attributed to three factors:

\begin{enumerate}
	\item The performance of cached is almost the same for writes and reads.  
		This is expected behavior as the read and write paths for cached have 
		many common parts (see Figure ? and ?). However, if we go one step 
		further we will see that under closer inspection, the reads seem 
		consistently a bit faster than writes. So, how can these happen if both 
		paths are the same? \\
		\\
		This is actually typical RAM behavior. Reads are reportedly faster than 
		writes (find paper) due to the fact that the update of a bit of an 
		SDRAM is slower than the read (arg, it's dumb).
	\item Cached doesn't scale much past the 16KB block size. This is an 
		interesting observation with an unexpected answer. It may seem 
		implausible at first, but what happens is that we are actually hitting 
		the bandwidth limit of the server's RAM. You can see in Table 
		\ref{tab:specs} that the bandwidth limit is ~10.7GB/s. This limit is 
		approached asymptotically as the block size increases and the CPU 
		overhead decreases (more about the CPU overhead later on). Once more, 
		the experienced eye may see that in reads we surpass this limit, which 
		is logical given the fact that we have a multi channel RAM setup that 
		reportedly increases marginally the RAM's performance (why reportedly 
		and how marginally? Explain...)
	\item On reads, sosd is benefited from the existence of caches in various 
		levels: on OSD level, on RAID controller level and thus is faster.
\end{enumerate}

\textbf{Why is cached's performance increased along with block size?}

This is another interesting observation but first, why doesn't it apply on 
sosd? This is because sosd's primary storage are hard disk drives, which have a 
major drawback; their seek time is not constant and is affected by the location 
of the contents in the disk platters. Cached however stores data in RAM and we 
would expect that writing 16 x 4k blocks and 1 x 64k block to take 
approximately the same time.

From this observation, we can extract that the indexing-related stuff (job 
enqueuing, lock spinning, hash table indexing) dominate the cached's 
performance. We can make sure this is the case if the latency results are in
microseconds instead of nanoseconds, which is typical for RAM.

\textbf{Why isn't the performance of cached improved proportionally as the 
	parallel requests increase?}

The reason why we see a minor increase in the performance of cached, even 
though it's multi-threaded, is because our locking scheme is not fine-grained 
enough. We have a single lock for our request queue, a single lock for most of 
the hash table accesses and this inevitably causes a lot of threads to spin.  
This slight improvement we see is mainly due to the fact that requests are 
effectively being pipelined while waiting each other to release locks. (explain 
better)

Let's proceed now to the latency results. The write performance can be seen in 
Figure \ref{fig:lat-write-comp-lie.pdf} while the read performance can be seen 
in Figure \ref{fig:lat-read-comp-lie.pdf}.

\diagram{lat-write-comp-lie.pdf}{Comparison of latency performance for writes}
\diagram{lat-read-comp-lie.pdf}{Comparison of latency performance for reads}

We will measure the speedup of the write and read performance of cached. It is 
bla-bla for writes and bla-bla for reads.

We can also see that the latency results confirm our previous observations. The 
latency is increased proportionally with the iodepth, which indicates once more 
that we need a more fine-grained locking scheme. Also, latency results are in 
the order of microseconds instead of nanoseconds which further supports the 
assertion that the results are dominated by index-related stuff (argh, think of 
something prettier)

We now proceed to the second part of the comparison between cached and sosd.  
On this part, we will once again evaluate their performance against a random 
workload with many parallel requests. Unlike the first part though, where the 
cache size was the same as the workload's, on this part the cache size will be 
only a fraction of it. This is after all the projected usage of cached in
production environments.

For this test, there are two main parameters we must take into account: the 
cache size and the maximum objects. These parameters have been decoupled in our 
implementation and we expect different results for each combination. We have 
chosen to measures cache sizes that start from the 1/64th of the workload's 
size and reach up to the 1/8th of the workload's size. The maximum objects are 
chosed differently (oversubscriptions on cache size etc.) they start from a 1/1 
and reach up to 8/1.

In Figure \ref{fig:bw-write-comp-truth.pdf} and Figure 
\ref{fig:bw-read-comp-truth.pdf} we can see how cached performs for the above 
scenario. sosd's result are of-course un-affected from the cache size and 
maximum cache objects, and that's will be used as indicative lines for 
slowness, fastness, does this word even exist?

Comparing these results with the results Figure \ref{fig:bw-write-comp-lie.pdf} 
and \ref{fig:bw-read-comp-lie.pdf}, there is a vast drop in the performance. 
Writes specifically cannot outperform the sosd. On the other hand, reads are 
generally faster than sosd, about 1.5x.

\diagram{bw-write-comp-truth.pdf}{Comparison of bandwidth performance for 
	writes}
\diagram{bw-read-comp-truth.pdf}{Comparison of bandwidth performance for reads}

\textbf{Why adding more objects makes us slower?}

\textbf{Why reads manage to remain faster?}

We will accompany the above diagrams with latency results. You can see them 
below:

\diagram{lat-write-comp-truth.pdf}{Comparison of latency performance for 
	writes}
\diagram{lat-read-comp-truth.pdf}{Comparison of latency performance for reads}


