\chapter{Design of cached}\label{ch:cached-design}

In the previous chapters, we have addressed the need for tiering in terms of
scalability as well as performance. % Which are these chapters? Link them.

We have also evaluated current caching solutions and described why they couldn't 
be used as a cache tier in Archipelago. % Provide link to a table of comparisons

With the results of chapter ? in mind, we can provide some more strict 
requirements that our solution must have:

% List the requirements for our solution
\begin{enumerate}
	\item \textbf{Nativity:} Our solution must be native to Archipelago i.e.  
		not need any translation layers to communicate with it.
		%explain why
	\item \textbf{Pluggability:} Our solution must be able to provide a 
		caching layer between peers that are already in operating mode 
		without restarting Archipelago.
	\item \textbf{Object awareness:} Our solution must be able to operate on 
		object level, which are the data entities Archipelago 
		understands.
	\item \textbf{In-memory:} Our solution must cache requests in RAM, since 
		the next fastest tier, SSDs, are already being used in RADOS as 
		a journal.
\end{enumerate}

For the following chapters, we will drop the \textit{"solution"} moniker and we 
will use instead the proper name of our implementation, \emph{"cached"}, which 
simply means \textbf{cache d}aemon).

The following two chapters are the main bulk of this thesis and they present our 
own implementation that aims to fill the above requirements.

% Foretell what the following sections will contain
More specifically, this chapter provides an in-depth description of the design 
of cached.  Section ? provides a general overview of cached. Sections ? - ?  
present the building blocks of cached and their design. Section ? presents the 
interaction of cached and its building blocks. Finally, in Section ?  we 
illustrate the flow of requests for cached.

\section{Design overview}

First of all, cached has been designed as an Archipelago user-space peer (see 
Section \ref{sec:arch-peer} about Archipleago peers). This design decision 
covers the nativity requirement we posed at the beginning of this chapter.

Also, cached's purpose is to provide a caching layer between the vlmc and 
blocker. Due to the fact that cached is a peer, this is easily achievable even 
under normal operation because:

\begin{enumerate}
	\item 
		As we have mentioned in Section \ref{sec:arch-ipc}, XSEG ports 
		can be registered on-line.
	\item
		During normal operation, the administrator can add the cached 
		port to the request path between vlmc and blocker, and all 
		requests will seamlessly be intercepted by cached. This follows 
		the same principle with bcache, which plugs its own 
		request\_fn() function to the virtual device it creates.  Unlike 
		bcache however, cached can be plugged on and off at any time.
\end{enumerate}

Thus, the pluggability requirement has also been covered.

Furthermore, one of the most important aspects of cached's design is the index 
mechanism. Specifically, we have utilized an in-memory hash table to index our 
cached objects.

Moreover, cached has been designed to be object aware. This means that we search 
in the hash table with the object name as key.%Anything else?


Finally, an important aspect of cached is its multi-threading support.  
Specifically, cached can work with multiple threads that can accept requests 
from cached's request queue and serve them concurrently with the other threads.  
Of course, multi-threading can be very tricky, especially when we are dealing 
with I/O requests and simultaneous accesses to the same object blocks. So, in 
order to achieve a balance between safety and speed, we use a
fine-grained locking scheme in critical sections that can be seen is discussed 
in detail in Section \ref{sec:xworkq-design}.

\subsection{Cached components}

Let's see now the design of cached in detail. The cached peer consists of a 
number of building blocks.  Per Archipelago policy, most of these building 
blocks have been written in the xtypes fashion (see Section \ref{arch-xtypes} 
about xtypes). Also, we must note that these components were not created by the 
author and the contribution to them was limited.

The components of cached can be seen below:
 
% TODO: Link List to the definitions
\begin{itemize}
	\item xcache, an xtype that provides indexing support, amongst many 
		other things
	\item xworkq, an xtype that guarantees atomicity for execution of jobs 
		on the same object
	\item xwaitq, an xtype that allows conditional execution of jobs
\end{itemize}

and their design will be discussed in-depth in the following sections.

Also, we must note that the above components predate our cached implementation 
and are not a contribution of this thesis\footnote{xcache is an exception since 
	we have extended its functionalities for our purposes}. They are 
presented however in this thesis for clarity reasons. 

\section{The xcache xtype}\label{sec:xcache-design}

xcache is the main component of cached. It is responsible for several key 
aspects of caching such as:

\begin{itemize}
	\item entry indexing,
	\item entry eviction, and
	\item concurrency control
\end{itemize}

We have to note here that xcache pre- our contribution to xcache 

Below we can see a design overview of xcache:

% Add figure for xcache with both hash tables, xq etc.

% While explaining, point to the diagram objects, a, b, c etc.
As we can see above, xcache utilizes two hash tables. One hash table is 
responsible for indexing entries (or more generally speaking "cache entries") 
that are active in cache.  The other hash table is responsible for indexing 
evicted cache entries that have pending jobs.  Again, more generally speaking, 
evicted cache entries are entries whose refcount has not dropped to zero yet.

\subsection{Entry Preallocation}\label{sec:entry-prealloc-design}

Since xcache has a bounded number of entries that will allocate, there is no 
need to allocate them on-the fly using malloc/free. Considering that we are 
caching at RAM level and not at SSD level, the system call overhead will have a 
considerable impact on performance.

Thus, the best thing to do in our case would be to pre-allocate the necessary 
space.

\subsection{Entry indexing}\label{sec:xcache-index-design}

In order to index the cached entries, xcache relies on another xtype, xhash, 
which is a hash table. Moreover, it's actually the C implementation of the 
dictionary used in Python.
% explain that for sparse Python dictionaries, accesses are O(1)

We have chosen to use a hash table as our index because:
% List of pros and cons of hash tables and why cons are ok in our book

Finally, the xhash xtype gives provides us with the basic hash table functions, 
namely:

\begin{itemize}
	\item Insertion
	\item Look-up
	\item Deletion
\end{itemize}

\subsection{Entry eviction}

As we can see in figure ?, xcache has been designed to index a pre-defined 
number of entries. That means that when xcache reaches its maximum capacity and 
is requested to index a new entry, it has to resort to the eviction of a 
previously cached entry. We have chosen the LRU strategy 

Also, an added bonus is that we won't need to sacrifice speed over optimality, 
since that, our hash table approach allows us to create an O(1) LRU algorithm 
which you can see in the following figure:

% Insert figure for O(1) LRU and hash table

In a nutshell, our LRU implementation uses a doubly linked list and utilize the 
hash table to jump to the element (instead of traversing the list linearly).
This design allows us to do all of the following action in constant time:

\begin{itemize}
	\item Insert a new entry to the LRU list
	\item Evict the LRU entry
	\item Update an entry's access time (i.e. mark it as MRU)
	\item Remove an arbitrary entry
\end{itemize}

% TODO: Explain what is the interesting thing here, that evictions happen 
% implicitly
Another interesting feature of xcache is that evictions occur implicitly and not 
explicitly. The user doesn't need to interact with the LRU queue.

For example, when a user tries to insert a new entry to an already full cache, 
the insertion will succeed and the user will not be prompted to evict an entry 
manually. Also, the user will be notified via specific event hook that is 
triggered upon eviction that an entry has been evicted.

More about hooks can be seen in the following subsection.

\subsection{Concurrency control}

The concept of concurrency control has been discussed in chapter ?. The goal of 
xcache is to handle safely - and preferably fast - simultaneous accesses to 
shared memory.

In order to do so, we must first identify which are the critical sections of 
xcache, that is the sections where a thread can modify a shared structure. These 
sections are the following

\begin{itemize}
	\item
		All xhash operations: Two of the three xhash operations (inserts 
		and removals) can modify the hash table (e.g. they can resize 
		it, add more entries or delete existing ones).  This means that 
		the third one (lookups) must not run concurrently with the 
		other.
	\item
		Cache node claiming: Before an entry is inserted, it must	
		acquire one of the pre-allocated nodes and we must ensure that 
		this can happen from all threads.
	\item
		Entry migration: An entry can migrate from one hash table to 
		the other e.g. on cache eviction. This migration involves a 
		series of xhash operations; removal from one hash table and 
		subsequent insertion to the other. This a scenario that must be 
		handled properly.
	\item
		Reference counting: Every entry must have a reference counter.  
		Reference counters provide a simple way to determine when an 
		entry can be safely removed. You can see more about reference 
		counting in chapter ?
	\item
		LRU updates: Most actions that involve cache entries must 
		subsequently update the LRU queue. Being a doubly linked list, 
		if two threads update the LRU simultaneously, we can lead to 
		segfaults.
\end{itemize}

Let's see what guarantees we provide for each of the above scenarios:

\begin{itemize}
	\item
		xhash operations: We provide a lock for each hash table
	\item
		Cache node claiming: The free node queue is protected by a fast 
		lock
	\item
		Entry migration: We always take fist the lock for entries and 
		then for rm\_entries
	\item
		Reference counting: Another important guarantee is the 
		reference counting of entrys. xcache uses atomic gets and puts 
		to update the reference count of an entry.
	\item
		LRU updates: Since all LRU operations take place for entries in 
		"entries" hash table and LRU updates are blazing fast we can 
		secure our LRU with the cache->lock.
\end{itemize}

% TODO: Yada yada about refcount

\subsection{Re-insertion}

We have previously mentioned that in xcache, there can be data migration 
between hash tables. This is easy to see why in case of evictions: an entry 
that previously was in "entries" must now be migrated to "rm\_entries" until 
its reference count falls to zero and can be freed.

However, what happens when xcache receives a request for an evicted entry? 

%Explain, explain, explain the re-insertion
there is a concept called "re-insertion".  In order for an entry to
be re-inserted to the primary hash table (which will be called "entries" from
now on) it must first reside in the hash table that indexes the evicted cache
entries (which will be called "rm\_entries" from now on). As mentioned above,
an entry that is in rm\_entries has probably pending jobs that delay its
removal.

So, what happens if a lookup arrives for that entry while on this stage? In
this case, we re-insert it to entries and increase its refcount by 2, since
there is one reference by the hash table and one reference by the one who
requested the lookup.

\begin{comment}
\subsection{Hooks}


The hooks that xcache provides to users are:

\begin{itemize}
	\item on\_ init: called on cache entry initialization.
	\item on\_put: called when the last reference to the cache entry is put
	\item on\_evict: called when a cache entry is evicted.
	\item on\_node\_init: called on initial node preparation.
	\item post\_evict: called after an eviction has occurred, with cache  
		lock held.
	\item on\_free: called when a cache entry is freed.
	\item on\_finalize: called to hint the user that the cache entry's ref 
		has dropped to zero.
	\item on\_reinsert: called when a cache entry has been in cache
\end{itemize}

\end{comment}

\subsection{xcache flow}

Below we will see three important scenarios

\textbf{Insertion}

Figure

\textbf{Lookup}

Figure

\textbf{Put}

Figure

\section{The xworkq xtype}\label{sec:xworkq-design}

The xworkq xtype is a useful abstraction for concurrency control. It's purpose 
is to enqueue "jobs" and ensure that only one thread will execute them at a 
time. There is no distinction as to which thread this will be, as well as no 
execution condition. The winner is simply the one that acquires first the lock.

This means that xworkq is generally used when multiple threads want atomic 
access to the same memory segment. xworkq is also generic by nature, since the 
"jobs" are simply a set of functions and their input data.

In cached context, every object has a workq. Whenever a new request is 
accepted/received for an object, it is enqueued in the workq and we are sure 
that only one thread at a time can have access to the object's data and 
metadata.

\begin{comment}
	This is wrong, xworkq understands only jobs.
on object level.  It is important to distinguish between cache level operations 
and object level operations. Cache level operations include insertions, lookups, 
removals, allocations and refcount handling. On object level, there is a 
different set of operations that must be synchronized across threads. Namely, we 
have bucket claiming, read/write operations and object flushes.

The above distinction makes it easy to see that provided that operations on 
object level need not worry about interactions with other objects. Each object 
is "sandboxed", so to speak.
\end{comment}

Let's see the design of the xworkq xtype. 

% Design of xworkq

It consists of a queue where jobs (e.g.  read from block, write to block) are 
enqueued. The thread that enqueues a job can attempt to execute it to, by 
acquiring a lock for the workq. If the lock is free, the thread will be able to 
execute the enqueued job. Also, other threads can enqueue their jobs, so the 
thread that has the lock can do those too. There is an xworkq for every object.


\section{The xwaitq xtype}

The xwaitq xtype bears some similarities to the xworkq xtype. Like xworkq, it is 
also an abstraction where "jobs" are enqueued and dequeued later on to be 
executed. Unlike xworkq though, in xwaitq the "jobs" that are being enqueued are 
considered to be thread-safe and can be executed concurrently by any thread.  
Moreover, the execution scheme is different. Specifically, jobs are executed 
only when a specific condition is true. This way, xwaitq provides an abstraction 
to...

In cached context...

Let's see the design of the xwaitq xtype. 

% Design of xwaitq

\section{Cached internals}

The components that have been discussed in the previous sections provide core 
functionalities for cached. Also, besides bridging these components, cached also 
has some internal bla bla that are crucial for cached's operation.

Namely, these are:

\begin{enumerate}
	\item Bucket pool
	\item Per-object request queues
	\item Bucket/Object/cio states
	\item Write policy
\end{enumerate}

\section{Bucket pool}

There is however a problem when operating solely on object level. Objects have 
typically 4MB of size. What would happen if a user requested e.g. a 16KB chunk 
of an object?

In this case, we would need to read and cache the whole object just to reply to 
the user's request. If the user then requests a chunk from another object, we 
would have to cache that object too and in the end, we would thrash our 
cache\footnote{cache thrashing occurs when we aggressively cache data that is 
	only used once and effectively leads to a snowball of evictions}.

The solution to this is to further divide objects to the next and final logical 
entity, \emph{buckets} (typically 4KB of size). Each bucket consists of its data 
and metadata and cannot be half-empty, or half-allocated. This way, we can also 
know which parts of the cached object are actually written, or are in the 
process of being read etc.

The buckets are pre-allocated, which means two things:

\begin{enumerate}
	\item We don't need to care about memory fragmentation and system call 
		overhead
	\item We cannot index single buckets. <FILLME>
\end{enumerate}

\subsection{Per-object peer requests}

Reads and writes to objects are practically read/write request from other
peers, for which a peer request has been allocated. There are cases though
when an object has to allocate its own peer request e.g. due to a flushing of
its dirty buckets. Since this must be fast, there are pre-allocated requests
hard-coded in the struct of each object which can be used in such cases.

\subsection{Bucket/Object states}

In order to know on how to operate on an object/bucket/cio, we must have some 
sort of book-keeping. The book-keeping we use is to check the state of the 
above. (arggh, silly)

\subsection{Write policy}\label{sec:wcp-design}

The user must define beforehand what is the write policy of cache. There are
two options: write-through and write-back. These policies aren't new and have 
been discussed extensively in chapter ?, but let's see what these policies 
translate to in cached context.

\begin{itemize}
	\item
		In \textbf{write-back} mode, cached caches writes, immediately 
		serves the request back and marks the data as dirty. When a read 
		arrives, it either serves the request with the dirty data 
		(read-hit) or forwards the request to the storage peer and 
		caches the answer (read-miss).

		This policy is used when we want to improve read and write speed 
		and can sacrifice data safety.
	\item
		In \textbf{write-through} mode, cached forwards writes to 
		blocker, servers the request when blocker replies, caches the 
		data and marks them as valid.  When a read arrives, it either 
		serves the request with the valid data (read-hit) or forwards 
		the request to the storage peer and caches the answer 
		(read-miss).

		This policy is used when we want to improve read speed and want 
		to make sure that no data will be lost.
\end{itemize}	

These policies are specified once during cached's deployment and cannot be 
switched on/off later on.

\section{Cached Operation}\label{sec:op-design}

\begin{comment}
Let's attempt to make the above a bit clearer. When cached receives a request, 
it first checks the request target (i.e. the object name and then calculates 
which bucket objects are within the request's range. It is easy to see that this 
is a 1:1 mapping to the object's data.
\end{comment}

\subsection{Write-through mode}

Here we will see how cached operates in write-through mode.

\subsubsection{Write}

This is the flow for the write path:

\subsubsection{Read}

This is the flow for the read path:

\subsection{Write-back mode}

Here we will see how cached operates in write-back mode.

\subsubsection{Write}

This is the flow for the write path:

\subsubsection{Read}

This is the flow for the read path:

