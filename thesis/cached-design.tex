\chapter{Design of cached}\label{ch:cached-design}

In the previous chapters, we have addressed the need for tiering in terms of
scalability as well as performance. % Which are these chapters? Link them.

We have also evaluated current caching solutions and described why they couldn't 
be used as a cache tier in Archipelago. % Provide link to a table of comparisons

With the results of chapter ? in mind, we can provide some more strict 
requirements that our solution must have:

% List the requirements for our solution
\begin{enumerate}
	\item Requirement 1
	\item Requirement 2
	\item Requirement 3
	\item Requirement 4
\end{enumerate}

The following two chapters are the main bulk of this thesis and they present our 
own implementation that aims to fill the above requirements.

% Foretell what the following sections will contain
More specifically, this chapter provides an in-depth description of the design 
of cached.  Section ?  provides a general overview of cached. Sections ? - ?  
present the building blocks of cached and their design. Section ? presents the 
interaction of cached and its building blocks. Finally, in Section ? we 
illustrate the flow of requests for cached.

\section{General}

In order to provide a caching tier for Archipelago that would blabla our 
requirements, we had to create our own implementation. Its name is simply cached 
(\textbf{cache d}aemon) and is another XSEG peer with similar structure to those 
seen in chapter ? %link to chapter

For the creation of this peer, we have created some xtypes
% Link to where we define xtypes
that act as the building blocks for this peer. These xtypes are the following:

\begin{comment}
More specifically, cached consists of the cache provided by xcache aasfs as safa
asfasfnd a
pre-allocated number of objects. An object is divided in buckets and its size,
as well as bucket size, are defined by the user.

The fact that objects are pre-allocated means two things:

1) We don't need to care about memory fragmentation and system call overhead
2) We cannot index single buckets. <FILLME>
\end{comment}

\begin{itemize}
	\item \textbf{xcache} (for cache support)
	\item xwork (for job support)
	\item xworkq (for atomicity in execution of jobs)
	\item xwaitq (for conditional execution of jobs)
\end{itemize}

and their design will be discussed in-depth in the following sections.

\section{The xcache xtype}

xcache is the main component of cached. It handles the indexing, reference 
counting, coherency and eviction jobs all by itself. A basic overview of xcache 
can be seen below

% Add figure for xcache with both hash tables, xq etc.

More specifically, xcache consists of two hash tables. One hash table is
responsible for indexing objects (or more generally speaking "cache entries")
that are active in cache. The other hash table is responsible for indexing
evicted cache entries that have pending jobs. Again, more generally speaking,
evicted cache entries whose refcount has not dropped to zero yet.

\subsection{Indexing}

In order to index the cached objects, xcache relies on another xtype, xhash, 
which is a hash table. What's more, it's actually the C implementation of the 
dictionary used in Python.

xhash is one of the main reasons our implementation is guaranteed to be O(1).
% TODO: Explain why

Finally, the xhash xtype gives provides us with the basic hash table functions, 
namely:

\begin{itemize}
	\item Insertion
	\item Look-up
	\item Deletion
\end{itemize}

\subsection{Eviction}

Cache entry eviction is done almost transparently from the user. xcache has two
LRU algorithms, a linear LRU array or a binary heap (more at xbinheap) that can
be chosen at runtime and are responsible for deciding which is the least
recently used entry. Users do not explicitly interact with the LRU array.
Eviction occurs automagically during the insertion of a new cache entry and the
user is informed via a specific hook that is triggered upon eviction.

\subsection{Hooks}

The hooks that xcache provides to users are:

\begin{itemize}
	\item on init: called on cache entry initialization.
	\item on\_put: called when the last reference to the cache entry is put
	\item on\_evict: called when a cache entry is evicted.
	\item on\_node\_init: called on initial node preparation.
	\item post\_evict: called after an eviction has occurred, with cache  
		lock held.
	\item on\_free: called when a cache entry is freed.
	\item on\_finalize: called to hint the user that the cache entry's ref 
		has dropped to zero.
	\item on\_reinsert: called when a cache entry has been in cache
\end{itemize}

\subsection{Refcounts and locks}

The refcount model in xcache should be familiar to most people:

\begin{itemize}
	\item When an entry is inserted in cache, the cache holds a reference 
		for it (ref = 1).
	\item Whenever a new lookup for this cache entry succeeds, the reference 
		is increased by 1 (ref++)
	\item When the request that has issued the lookup has finished with an 
		entry, the reference is decreased by 1. (ref--)
	\item When a cache entry is evicted by cache, the its ref is decreased 
		by 1. (ref--)
\end{itemize}

Some common refcount cases are:

\begin{itemize}
	\item active entry with pending jobs (ref > 1)
	\item active entry with no pending jobs (ref = 1)
	\item evicted entry with pending jobs (ref > 0)
	\item evicted entry with no pending jobs (ref = 0)
\end{itemize}

and, as always, the entry is freed only when its ref = 0.

Finally, xcache uses one lock for each hash table but when a cache entry shifts
from one hash table to the other, both locks are acquired.

\subsection{Re-insertion}

In xcache, there is a concept called "re-insertion". In order for an entry to
be re-inserted to the primary hash table (which will be called "entries" from
now on) it must first reside in the hash table that indexes the evicted cache
entries (which will be called "rm\_entries" from now on). As mentioned above,
an entry that is in rm\_entries has probably pending jobs that delay its
removal.

So, what happens if a lookup arrives for that entry while on this stage? In
this case, we re-insert it to entries and increase its refcount by 2, since
there is one reference by the hash table and one reference by the one who
requested the lookup.


\section{The xworkq xtype}

Every object has a workq. Whenever a new request is accepted/received for an
object, it is enqueued in the workq and we are sure that only one thread at a
time can have access to the objects data and metadata.

For more information, see the xworkq.

\section{The xwaitq xtype}

When a thread tries to insert an object in cache but fails, due to the fact
that cache is full, the request is enqueued in the xcache waitq, which is
signaled every time an object is freed.

For more information, see the xwaitq.

\section{Cached internals}

\subsection{Object states}

Every object has a state, which is set atomically by threads. The state list is
the following:

\begin{itemize}
	\item READY: the object is ready to be used
	\item FLUSHING: the object is flushing its dirty buckets
	\item DELETING: there is a delete request that has been sent to the 
		blocker for this object
	\item INVALIDATED: the object has been deleted
	\item FAILED: something went very wrong with this object
\end{itemize}

Also, object buckets have their own states too:

\begin{itemize}
	\item INVALID: the same as empty
	\item LOADING: there is a pending read to blocker for this bucket
	\item VALID: the bucket is clean and can be read
	\item DIRTY: the bucket can be read but its contents have not been
		written to the underlying storage
	\item WRITING: there is a pending write to blocker for this bucket
\end{itemize}

Finally, for every object there are bucket state counters, which are increased/
decreased when a bucket state is changed. These counters give us an O(1)
glimpse to the bucket states of an object.

\subsection{Per-object peer requests}

Reads and writes to objects are practically read/write request from other
peers, for which a peer request has been allocated. There are cases though
when an object has to allocate its own peer request e.g. due to a flushing of
its dirty buckets. Since this must be fast, there are pre-allocated requests
hard-coded in the struct of each object which can be used in such cases.

\subsection{Write policy}

The user must define beforehand what is the write policy of cache. There are
two options: writethrough and writeback. On a side note, as far as reads and
cache misses are concerned, cached operates under a write-allocate policy.

