\chapter{Performance evaluation of cached}\label{ch:cached-evaluation}

\begin{flushright}
	\textit{"There are three kinds of lies:\\
		lies, damned lies, \\
		and \sout{statistics} benchmarks."}	\\

	Mark Twain (modernized)
\end{flushright}

It may seem as an ironic statement, considering that we are about to provide 
benchmark results for cached, but it's actually is a valid one.
\begin{comment}
What Mr.  Twain tries to say here
\footnote{
	and that's a phrase usually not heard in programming contexts...
}
is that the presentation of partials facts for something can be used to 
fabricate a plausible truth for it.
%This is shit, fix it
In science's case, it so often happens that promising results for an experiment 
can seem more important to the researcher's eye than negative ones due to 
positive reinforcement.
\end{comment}
In our case, we will try not to merely smear the next pages with diagrams but 
first explain the benchmarking methodology behind them and then provide a 
concrete depiction of cached's performance under various workloads.

The skeleton of this chapter is the following: Section \ref{sec:perf-meth} 
explains the methodology behind our measurements. Section \ref{sec:test-bed} 
provides details about the hardware on which we have conducted our benchmarks.  
Section \ref{sec:perf-plot} presents the results of the benchmarks that we have 
conducted and provides in-depth explanations about each of them.  Finally, 
Section ?  is reserved for cached on a VM (\fixme take these measurements).

\section{Benchmark methodology}\label{sec:perf-meth}

The benchmarks that have been executed and whose results are presented in this 
chapter, will be split in two categories, both of which have their own distinct 
goals:

The first category is the comparison between using cached \textbf{on top} of 
the sosd peer (sosd has been discussed here ? \fixme add section) and using 
solely the sosd.

\begin{comment}
The category's goal is to "defend" one of the core thesis arguments, that 
tiering is a key element that will improve the performance of Archipelago.  
\end{comment}

In order to compare effectively the performance of cached and sosd, we must 
consider the following: 

\begin{enumerate}
	\item The comparison of the two peers should try to focus on what is 
		the best performance that these peers can achieve for a series 
		of tough workloads.
	\item The circumstances under which both peers will be tested need not 
		be thorough but challenging. For example, it may be interesting 
		to test both peers against sequential requests, but
		\begin{inparaenum}[i)]
		\item such patterns are rarely a nuisance for production 
			environments
		\item they do not stress the peers enough to provide something 
			conclusive
		\item they are out of the scope of this section as there can be 
			many of these kinds of tests and adding them all here 
			will impede the document's readability.
		\end{inparaenum}
		
	\item Both peers must be tested under the same, reasonable workload, 
		i.e a workload that can be encountered in production 
		environments.
	\item If the peer doesn't show a consistent behavior for a workload, it 
		must be depicted in the results.
\end{enumerate}

Having the above in mind, the next step is to choose a suitable workload.  This 
choice though is fairly straight-forward; in production environments, the most 
troublesome workload is the burst of small random reads/writes and is usually 
the most common one that is benchmarked.  

One may ponder however, how many requests can be considered as a "burst" or 
which block size is considered as "small". Of course, there is not only one 
answer to this question so, we will work with ranges. For our workload, we will 
use block sizes ranging from 4KB to 64KB and parallel requests ranging from 4 
to 16.

The second category deals solely with the inner-workings of cached and its 
behavior on different operation modes or states. Its aim is not to capture the 
performance against a tough workload, but to explain \textbf{why} this 
performance is observed and how each of the options affect it. In this 
category, we measure how threads impact the performance of cached or what 
impact (if any) does our index mechanism have.

Finally, in the following sections, for brevity reasons, we will talk about 
comparing cached and sosd. What the reader must keep in mind however is that 
cached is essentially the cache layer above sosd, which means that we actually 
test sosd vs cached over sosd.

\section{Specifications of test-bed}\label{sec:test-bed}

The specifications of the server on which we conducted our benchmarks is the 
following.

\begin{table}[H]
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Component & Description \\ \hline \hline
		CPU &  2 x Intel(R) Xeon(R) CPU E5645 @ 2.40GHz \cite{e5645} \\
		 & Each CPU has six cores with Hyper-Threading enabled, which equals to 
		 24 threads. \\ \hline
		RAÎœ & 2 banks x 6 DIMMs PC3-10600 \\
		& Peak transfer rate: 10660 MB/s \\ \hline
	\end{tabular}
	\caption{Test-bed hardware specs}
	\label{tab:specs}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Software & Version \\ \hline \hline
		OS &  Debian Squeeze \\ \hline
		Linux kernel & 3.2.0-0 (backported) \\ \hline
		GCC & Debian 4.4.5-8 \\ \hline
	\end{tabular}
	\caption{Test-bed software specs}
	\label{tab:specs}
\end{table}

\todo add configuration of RADOS cluster (journal, backing storage, network 
connection)

\fixme Mention that the peers were evaluated by sending requests directly at 
their ports.

\section{Performance comparison between cached and sosd}
\label{sec:perf-plot}

As mentioned above, for our first test, we will evaluate the read and write 
performance of cached and sosd for a random workload with parallel requests of 
small size. In order to measure accurately their performance, we will use two 
different metrics:

\begin{description}
	\item[Bandwidth:] \hfill \\
		Bandwidth measures the maximum throughput that the application 
		can sustain. This metric is usually used to indicate how much 
		data (from various inputs) can an application process within a 
		second.
	\item[Latency:] \hfill \\
		Latency is the converse of bandwidth. It is a measurement from 
		the viewpoint of the issuer of requests and indicates the 
		responsiveness of the tested application. It is commonly 
		calculated as the average reply time for a series of requests.
\end{description}

On the following sections, we present the benchmarks that we conducted for the 
first category. The first benchmark, which is shown in Section 
\ref{sec:peak-plot} attempts to depict the behavior of cached during a peak of 
I/O requests. The second benchmark, which is shown in Section 
\ref{sec:consistent-plot}, illustrates the behavior of cached under continuous 
load.

\subsection{Workload smaller than cache size - Peak behavior}
\label{sec:peak-plot}

We begin with the bandwidth performance of our peers. The write performance can 
be seen in Figure \ref{fig:bw-write-comp-lie.pdf} while the read performance 
can be seen in Figure \ref{fig:bw-read-comp-lie.pdf}.

Before we proceed with the interpretation of the diagram results, we will 
briefly comment on the diagram structure. Due to the fact that the performance 
of the two peers differs in at least two orders of magnitude, the results would 
look too flat in a conventional diagram that would scale from 0 to 11000. To 
amend this, we have broken the y-axis of our diagrams in two parts with 
different scales and starting values, in order to make the comparison easier to 
the eye.

\diagram{Comparison of bandwidth performance for writes}{bw-write-comp-lie.pdf}
\diagram{Comparison of bandwidth performance for reads}{bw-read-comp-lie.pdf}

The initial results look very promising. For write requests, the speedup for 
very small block sizes (4KB - 16KB) is approximately \textbf{100x} whereas for 
larger ones (32KB - 64KB) it ranges from \textbf{50x to 200x}. For read 
requests, the speedup for very small block sizes is approximately \textbf{50x}, 
whereas for larger block sizes it ranges between \textbf{20x - 75x}.

These results not only illustrate the performance gap between RAM and
HDDs but also show that our implementation manages to keep its bandwidth 
consistently over 1 GB/s in stress scenarios. However, we must keep in mind 
that they only show a part of the full picture and, since we do not test how 
cached behaves past its cache size.

Moreover, upon closer inspection the following questions arise:

\textbf{1) Where is the speedup difference between read and writes attributed 
	to?}

We have mentioned that the speedup for writes is between 50x and 200x whereas 
for reads between 20x and 75x. Although it is a large speedup in both cases, it 
also shows some of the shortcomings of our implementation.

We attribute the speedup difference to three factors:

\begin{enumerate}
	\item The performance of cached is almost the same for writes and 
		reads.  This is expected behavior as the read and write paths 
		for cached have many common parts (\fixme show write/read 
		path).
		\footnote{To be more strict, reading seems a bit faster than 
			writing, but that is probably attributed to CPU caches 
			and the fact that reading from RAM is not a distractive 
			operation in contrast to writing.}
	\item Cached doesn't scale much past the 16KB block size. This is an 
		interesting observation with an unexpected answer. It may seem 
		implausible at first, but what happens is that we are actually 
		hitting the bandwidth limit of the server's RAM. If we consult 
		the table \ref{tab:hardware-specs}, we can see that the 
		bandwidth limit of the RAM is about \~10.7GB/s.
		\footnote{A more careful look shows that we surpass this limit, 
			which is expected given the fact that we have a multi 
			channel RAM setup}
		This limit is approached asymptotically as the block size 
		increases and the index overhead decreases (more about the 
		index overhead later in the measurements of Section ? \fixme 
		add section).
	\item On reads, sosd is benefited from the existence of caches in 
		various levels e.g. on OSD level, on RAID controller level.
\end{enumerate}

To sum up, the cached's performance remains relatively the same in both reads 
and writes, it's merely the sosd that is getting faster due to caches. 

\textbf{2) Why is cached's performance increased along with block size?}

This is another interesting observation but first, why don't we ask the same 
about sosd? This is because sosd's primary storage are hard disk drives, which 
have a major drawback; their seek time is not constant and is affected by the 
location of the contents in the disk platters. Cached however stores data in 
RAM and we would expect that writing 16 x 4k blocks and 1 x 64k block to take 
approximately the same time.

From this observation, we can extract that the indexing-related stuff (job 
enqueuing, lock spinning, hash table indexing) dominate the cached's 
performance. We can make sure this is the case if the latency results are in
microseconds instead of nanoseconds, which is typical for RAM.

\textbf{Why isn't the performance of cached improved proportionally as the 
	parallel requests increase?}

The reason why we see a minor increase in the performance of cached, even 
though it's multi-threaded, is because our locking scheme is not fine-grained 
enough. We have a single lock for our request queue, a single lock for most of 
the hash table accesses and this inevitably causes a lot of threads to spin.  
This slight improvement we see is mainly due to the fact that requests are 
effectively being pipelined while waiting each other to release locks. (explain 
better)

Let's proceed now to the latency results. The write performance can be seen in 
Figure \ref{fig:lat-write-comp-lie.pdf} while the read performance can be seen 
in Figure \ref{fig:lat-read-comp-lie.pdf}.

\diagram{Comparison of latency performance for writes}{lat-write-comp-lie.pdf}
\diagram{Comparison of latency performance for reads}{lat-read-comp-lie.pdf}

We will measure the speedup of the write and read performance of cached. It is 
bla-bla for writes and bla-bla for reads.

We can also see that the latency results confirm our previous observations. The 
latency is increased proportionally with the iodepth, which indicates once more 
that we need a more fine-grained locking scheme. Also, latency results are in 
the order of microseconds instead of nanoseconds which further supports the 
assertion that the results are dominated by index-related stuff (argh, think of 
something prettier)

\subsection{Workload larger than cache size - Consistent behavior}

We now proceed to the second part of the comparison between cached and sosd.  
On this part, we will once again evaluate their performance against a random 
workload with many parallel requests. Unlike the first part though, where the 
cache size was the same as the workload's, on this part the cache size will be 
only a fraction of it. This is after all the projected usage of cached in
production environments.

For this test, there are two main parameters we must take into account: the 
cache size and the maximum objects. These parameters have been decoupled in our 
implementation and we expect different results for each combination. We have 
chosen to measures cache sizes that start from the 1/64th of the workload's 
size and reach up to the 1/8th of the workload's size. The maximum objects are 
chosen differently (oversubscriptions on cache size etc.) they start from a 1/1 
and reach up to 8/1.

In Figure \ref{fig:bw-write-comp-truth.pdf} and Figure 
\ref{fig:bw-read-comp-truth.pdf} we can see how cached performs for the above 
scenario. sosd's result are of-course un-affected from the cache size and 
maximum cache objects, and that's will be used as indicative lines for 
slowness, fastness, does this word even exist?

Comparing these results with the results Figure \ref{fig:bw-write-comp-lie.pdf} 
and \ref{fig:bw-read-comp-lie.pdf}, there is a vast drop in the performance. 
Writes specifically cannot outperform the sosd. On the other hand, reads are 
generally faster than sosd, about 1.5x.

\diagram{Comparison of bandwidth performance for writes} 
{bw-write-comp-truth.pdf}
\diagram{Comparison of bandwidth performance for reads}{bw-read-comp-truth.pdf}

\textbf{Why adding more objects makes us slower?}

\textbf{Why reads manage to remain faster?}

We will accompany the above diagrams with latency results. You can see them 
below:

\diagram{Comparison of latency performance for writes}
{lat-write-comp-truth.pdf}
\diagram{Comparison of latency performance for reads}{lat-read-comp-truth.pdf}


\section{Performance evaluation of cached parameters}

On this part, we will see what impact do different cached parameters have on 
its performance. We will test the following:

\begin{enumerate}
	\item Impact of different number of threads
	\item Impact of cold cache vs. hot cache
	\item Impact of writeback vs. writethrough mode
\end{enumerate}

Note that the tests above are run with the following parameters:

\begin{description}
	\item[Mode] Writeback
	\item[Block size] 4k
	\item[Cache size] Always larger than benchmark size
\end{description}

The above options have been chosen to isolate cached of any other factors that 
may alter it's performance. This way, we will be able to see more clearly the   
that in any other

\subsubsection{Threads}

Checking the performance impact of multi-tasking is meaningless without issuing
parallel requests. Therefore, for each number of threads, we will use different 
IOdepth and measure its performance.

The bandwidth results can be seen in Figure \ref{fig:bw-write-threads.pdf} 
whereas the latency results can be seen in Figure 
\ref{fig:lat-write-threads.pdf}.

\diagram{Bandwidth performance per number of threads}{bw-write-threads.pdf}
\diagram{Latency performance per number of threads}{lat-write-threads.pdf}

From these results, we derive the following conclusions:

\begin{enumerate}
	\item Our implementation is benefited from multi-threading. We achieve 
		a major performance improvement of up to 75\% when using two 
		threads, as well as lower performance improvement for up to 
		four threads, as the number of parallel requests increases.
	\item We don't scale well past the two threads and four parallel 
		requests.
	\item Adding more than two threads degenerates significantly the 
		performance when the number of parallel requests is small.
\end{enumerate}

Finally, these results, along with the results of the first part, clearly show 
the dark spot of our implementation; it needs a more fine-grained locking 
scheme else most of the thread's time will be spent spinning for a lock.

\subsubsection{Cold cache vs Hot cache}

This scenario will attempt to evaluate the overhead of cache misses in cached
against cache hits for \textbf{write} operations. Theoretically, this should 
account to the overhead of adding new entries to cached and consecutively, an 
indication of the complexity of our index mechanism.

For this reason, we have written 128K (where \textbf{K} is \textbf{*1024} and 
\textbf{M} is \textbf{*1024\^2}), 256K, 512K and 1M objects and have measured 
their latency performance. We expect that the experimental results will verify 
the claim that our implementation is O(1).

To get the most accurate results and since we want to test just the performance 
of our indexing mechanism, we have also used only 1 thread and only 1 IOdepth. 

On Figure \ref{fig:cold1.pdf} we can see the results we were talking about. The 
major point in these results is that we can see that write latency, either of 
cold or warm cache, remains practically the same as the number of objects 
increases.

\diagram{Latency performance of cold/warm cache for variable sizes}{cold1.pdf}

As a side note, we observe a constant decrease in latency as the number of 
objects increase this is not something that should be attributed to our 
implementation. (explain that we have used a hash table that holds 2million 
objects, so it is not mapped to our process's address space. When more objects 
are indexed, the hash table becomes fuller and the latency of mmap()s is 
equally distributed to the objects. Else, the hash table is more scarce but the 
same blocks are hit, albeit not fully written, and thus the mmap latency is the 
same but distributed to less objects.)
