\chapter{Design of cached}\label{ch:cached-design}

In the previous chapters, we have addressed the need for tiering in terms of
scalability as well as performance. % Which are these chapters? Link them.

We have also evaluated current caching solutions and described why they couldn't 
be used as a cache tier in Archipelago. % Provide link to a table of comparisons

With the results of chapter ? in mind, we can provide some more strict 
requirements that our solution must have:

% List the requirements for our solution
\begin{enumerate}
	\item \textbf{Nativity:} Our solution must be native to Archipelago i.e.  
		not need any translation layers to communicate with it.
		%explain why
	\item \textbf{Pluggability:} Our solution must be able to provide a 
		caching layer between peers that are already in operating mode 
		without restarting Archipelago. Also, it must be removed 
		without disturbing the service.
	\item \textbf{In-memory:} Our solution must cache requests in RAM, 
		since the next fastest tier, SSDs, are already being used in 
		RADOS as a journal.
\end{enumerate}

For the following chapters, we will drop the \textit{"solution"} moniker and we 
will use instead the proper name of our implementation, "cached", which simply 
means \textbf{cache d}aemon).

The following two chapters are the main bulk of this thesis and they present our 
own implementation that aims to fill the above requirements.

% Foretell what the following sections will contain
More specifically, this chapter provides an in-depth description of the design 
of cached. Section \ref{sec:rationale-design} provides the design rationale of 
cached and explains how its design meets the above requirements. Section 
\ref{sec:comp-design} presents the building blocks of cached while Sections 
\ref{sec:xcache-design}, \ref{sec:xworkq-design} and \ref{sec:xwaitq-design} 
provide a detailed explanation of their design. Section \ref{sec:cached-design}  
presents the interaction between cached and its  as well as the unique 
components that cached consists of.  Finally, in Section ?  we illustrate the 
flow of requests for cached.

\section{Design rationale}\label{sec:rationale-design}

One of the first architectural decisions was to implement cached as an 
Archipelago user-space peer (see Section \ref{sec:arch-peer} about Archipelago 
peers). This choice was the most natural one since it provides the smallest 
possible communication overhead with the other Archipelago peers. Also, this 
design decision covers the nativity requirement we posed at the beginning of 
this chapter.

The above design choice has another advantage too; we can register on-line the 
cached peer between the vlmc and blocker and unregister it when we want to.  
This opens up numerous possibilities such as plugging cached for 
QoS\footnote{Quality of Service} reasons when there is a peak in I/O requests.  
This is possible because, as we have mentioned in Section \ref{sec:arch-ipc}, 
XSEG ports can be registered on-line. Thus, during normal operation, the 
administrator can add the cached port to the request path between vlmc and 
blocker, and all requests will seamlessly be intercepted by cached. This 
follows the same principle with bcache, which plugs its own request\_fn() 
function to the virtual device it creates.  Unlike bcache however, cached can 
be plugged on and off at any time.

This also means that the pluggability requirement is also being met.

The next important design decision was what will cached index. Given that it 
will reside between the vlmc and blocker, where the VM's requests have already 
been translated to object requests, the natural choice is to cache objects.  

The decision to cache objects not only is the most natural one, but also is 
closer to the way our storage (RADOS) handles data. To understand the 
importance of it, consider the following:

Like bcache, our implementation must not only cache object requests fast but 
also try to coalesce them so that, when needed, they will be flushed to the 
slower medium in a more sequential fashion. The fact however that the VMs' 
volumes are partitioned into different objects, means that sequential data (in 
volume context) which reside in different objects will probably not be 
sequential in the storage backend too.

Thus, unlike bcache which expects that the backing device is also the physical 
device and coalesces date accordingly, our implementation is limited only in 
coalescing data in the object range (commonly 4MBs). If our implementation was 
caching in block or volume level, it would be unaware of that fact. 

Having decided that cached will cache objects, the next step is to decide
\begin{inparaenum}[i)]
\item on the index mechanism and
\item on what \textbf{exactly} would we index.
\end{inparaenum}

As for what we would index, it would be an overkill to further partition the 
objects and index regions within them. Moreover, this would make sense only if 
the objects where large (e.g. like volumes). So, our index mechanism should 
index object names solely. As for the index mechanism, we have chosen to use a 
very fast in-memory hash-table for this job. This covers the in-memory 
requirement we have set above. Also, this choice is one of the main reasons 
that our implementation is O(1).

Finally, another important decision was whether cached would be a 
multi-threaded peer. We have decided that we will implement it this way and 
then evaluate the performance of the implementation to find out if we are 
benefited by multi-threading or not.

Thus, cached must be able to work with multiple threads which will accept 
requests from cached's request queue and serve them concurrently with the other 
threads. Of course, multi-threading can be very tricky, especially when we are 
dealing with I/O requests and simultaneous accesses to the same object blocks.  
So, in order to achieve a balance between safety and speed, we use a
fine-grained locking scheme in critical sections that can be seen is discussed 
in detail in Section \ref{sec:xworkq-design}.

\section{Cached components}\label{sec:comp-design}

At this point, we must do an intermission before we show the design of cached.  
Specifically, we will show first the design of the cached's components, since 
many cached operations rely on them and the reader needs prior knowledge of 
them to grasp the cached design.

\subsection{Overview}

In this section, we will list the main components that cached relies on. Per 
Archipelago policy, most of these components have been written in the xtypes 
fashion (see Section \ref{arch-xtypes} about xtypes).  

The components of cached can be seen below:
 
% TODO: Link List to the definitions
\begin{itemize}
	\item xcache, an xtype that provides indexing support, amongst many 
		other things
	\item xworkq, an xtype that guarantees atomicity for execution of jobs 
		on the same object
	\item xwaitq, an xtype that allows conditional execution of jobs
\end{itemize}

and their design will be discussed in-depth in the following sections.

Also, we must note that the above components predate our cached implementation 
and are not a contribution of this thesis\footnote{xcache is an exception since 
	we have extended its functionalities for our purposes}. They are 
presented however in this thesis for clarity reasons. 

\subsection{The xcache xtype}\label{sec:xcache-design}

xcache is the most important component of cached. It is responsible for several 
key aspects of caching such as:

\begin{itemize}
	\item entry indexing,
	\item entry eviction,
	\item concurrency control and
	\item event hooks
\end{itemize}

Below, we can see a design graph of xcache:

\fixme add Figure here

% While explaining, point to the diagram objects, a, b, c etc.
\fixme add better design explanation \\
As we can see above, xcache utilizes two hash tables. One hash table is 
responsible for indexing entries (or more generally speaking "cache entries") 
that are active in cache.  The other hash table is responsible for indexing 
evicted cache entries that have pending jobs.  Again, more generally speaking, 
evicted cache entries are entries whose refcount has not dropped to zero yet.

On the following subsections, we present the features of xcache as well as 
their design.

\subsubsection{Entry Preallocation}\label{sec:xcache-entry-design}

Since xcache indexes a bounded number of entries, there is no need to allocate 
them on-the fly using malloc/free. Considering that we are caching at RAM level 
and not at SSD level, the system call overhead will have a considerable impact 
on performance. Thus, in our case, we pre-allocate the necessary space in 
advance and store them in a cache-node pool (note that this is different from 
the bucket pool).

\subsubsection{Entry indexing}\label{sec:xcache-index-design}

The index mechanism that xcache uses is a hash table named xhash, also an 
xtype. The reason why a hash table is used as an index mechanism is because:

\begin{enumerate}
	\item Given that we index only a certain number of entries, we expect 
		the that the insert, lookup and delete operations are in 
		constant time (see below for an explanation why)
	\item Hash tables can preallocate the space needed whereas 
		tries/b-trees/bst will allocate nodes as new entries are 
		inserted. Again, the fact that we index a certain number of 
		entries means that we expect that we will have many evictions 
		and insertions.
	\item We don't need to do substring matches (advantage of tries)
	\item We don't need to traverse the entries sequentially (advantage of 
		B-trees and BSTs)
\end{enumerate}

The hash table that is used is heavily based on dictobject\cite{dictobject},
the Python dictionary implementation in C. Distobject has been created to 
minimize the collisions and the hops (\fixme Explain that better). Its only 
drawback is that it needs to resize when the table's entry history has reached 
the 2/3 of its capacity.

Besides the hash table, which answers to the question "Where is the entry?" we 
also need another mechanism to answer the question "Is the entry still 
referenced?". xcache has such a mechanism which is commonly called "reference 
counting". Specifically, each entry has a counter that is 
incremented/decremented when a user accesses/releases an entry.

To sum up, when an entry is inserted, we use its name as a key and we update 
its refcount to 2, one reference from the user and one standard reference from 
the hash table. When we lookup for an entry, we use the entry's name as a key 
and then increment by 1 its refcount. 

\subsubsection{Entry eviction}\label{xcache-eviction-design}

The decision to have xcache index a bounded number of entries means that when 
it reaches its maximum capacity and is requested to index a new entry, it has 
to resort to the eviction of a previously cached entry. Evicted entries are not 
removed immediately from xcache. They are instead set in an "evicted" state and 
they reside in a special-purpose hash table until the user confirms that they 
can be removed. 

xcache handles evictions in an interesting way. More specifically, evictions 
occur implicitly and not explicitly, meaning that the user (peer) doesn't have 
to evict entries manually. For example, when a user tries to insert a new entry 
to an already full cache, the insertion will succeed and the user will not be 
prompted to evict an entry manually. Moreover, the user will be notified via 
specific event hook that is triggered upon eviction.

The scheme of implicit evictions and later on notification of the user has the 
advantage that lookups, inserts and evictions can occur atomically by xcache.  
This wouldn't be the case if the user was responsible for the evictions.

As for the eviction strategy, we have utilized an LRU queue. Not only it's 
optimal (\fixme verify it) for our purposes, but we have also mitigated the 
cost of keeping the last references for each entry by creating a simple LRU 
algorithm, which has O(1) complexity for all update actions. More about the 
implementation of the LRU algorithm can be found in Section 
\ref{xcache-evict-imp}.

\subsubsection{Concurrency control}

The concept of concurrency control has been discussed in chapter ?. The goal of 
xcache is to handle safely - and preferably fast - simultaneous accesses to the 
shared memory.

In order to do so, we must first identify which are the critical sections of 
xcache, to wit, the sections where a thread modifies a shared structure. These 
sections are the following:

\begin{itemize}
	\item
		\textbf{Most xhash operations:} Inserts and removals can modify 
		the hash table (e.g. they can resize it, add more entries or 
		delete existing ones). This also means that lookups must not 
		run simultaneously with the above two operations.
	\item
		\textbf{Cache node claiming:} Before an entry is inserted, it 
		must	acquire one of the pre-allocated nodes from the 
		cache-node pool and we must ensure that this can happen 
		concurrently from all threads.
	\item
		\textbf{Entry migration:} An entry can migrate from one hash 
		table to the other e.g. on cache eviction. This migration 
		involves a series of xhash operations; removal from one hash 
		table and subsequent insertion to the other. These two 
		operations must occur atomically.
	\item
		\textbf{Reference counting:} Every entry must have a reference 
		counter.  Reference counters provide a simple way to determine 
		when an entry can be safely removed. Since many threads can 
		have access to the same entry, we must provide a way to update 
		the reference counters atomically.
	\item
		\textbf{LRU updates:} Most actions that involve cache entries 
		must subsequently update the LRU queue. The updates at the LRU 
		queue must also occur atomically.
\end{itemize}

Let's see what guarantees we provide for each of the above scenarios:

\begin{itemize}
	\item
		\textbf{xhash operations:} We provide a lock for each hash 
		table. Only one thread can access each hash table at any time.
	\item
		\textbf{Cache node claiming:} The cache-node queue, is also 
		protected by a lock.
	\item
		\textbf{Entry migration:} When an entry is migrated from one 
		hash table to the other, we always acquire the lock of the hash 
		table of active entries and then the lock of the hash table of 
		the evicted entries. The order on which we take the locks is 
		very strict to avoid deadlocks.
	\item
		\textbf{Reference counting:} For the atomic increases and 
		decreases of a counter, we don't need a lock and its added 
		overhead. Instead, we can use the atomic get and atomic put 
		operations that the CPU provides.
	\item
		\textbf{LRU updates:} Since the majority of LRU updates take
		place when a new entry is inserted in the hash table, we can 
		protect our LRU under the same cache lock.
\end{itemize}

\subsubsection{Re-insertion}

We have previously mentioned that in xcache, there can be data migration 
between hash tables. Most commonly, an entry that is evicted from the active 
cache entries migrates to the hash table of the evicted cache entries, until 
its reference count falls to zero and can be freed.

However, what happens when xcache receives a request for an evicted entry which 
hasn't been freed yet? 

In this case, the entry switches state again and is inserted back to the hash 
table of active entries. Also, its reference counter is incremented accordingly 
in order not to be freed amidst this process.

This way, we can avoid waiting for an entry that has just been evicted to flush 
its data. 

\subsubsection{Event hooks}\label{sec:xcache-hooks-design}

Since xcache is created to provide core caching functionalities for other 
peers, it must also notify them when it takes an implicit action that the peer 
is not aware of. In Section \ref{xcache-eviction-design} we have seen one 
implicit action that xccahe takes when a user inserts an entry, namely 
eviction. 

Besides this event, there are others. The complete list is the following:

\begin{description}
\item[cache node initialization:]
	This hook is triggered when a cache node is initialized. It is 
	triggered once only for each node, during the initialization phase of 
	xcache.
\item[cache entry initialization:]
	This hook is triggered when a cache entry has been inserted in the 
	cache.
\item[cache entry eviction:]
	This hook is triggered when a cache entry has been evicted from the 
	cache.
\item[cache entry reinsertion:]
	This hook is triggered when an evicted entry has been reinserted in the 
	cache.
\item[cache entry finalization:]
	This hook is triggered when an evicted entry's refcount has dropped to 
	0. This serves as a warning for the user who has the opportunity to let 
	the cache entry go or increment its refcount.
\item[cache entry put:]
	This hook is triggered when an evicted entry has been totally removed 
	from the cache.
\item[cache entry free:]
	This hook is triggered when a removed entry's cache node has been sent 
	back to the cache node pool.
\end{description}

For each of the above events, we have created the respective event hook. The 
peer that uses xcache may choose, if it wants, to use them and if so, it can 
plug its own event function for each hook which will be called when the event 
is triggered.

\subsection{xcache flow}

To make the way xcache works a bit more clearer, we will see the flow for three 
of the main xcache operations; lookup of an entry; insertion of a new entry and 
removal of an entry:

\subsubsection{Insertion}

\fixme add figure and explanation

\subsubsection{Lookup}

\fixme add figure and explanation

\subsubsection{Put}

\fixme add figure and explanation

\subsection{The xworkq xtype}\label{sec:xworkq-design}

The xworkq xtype is a useful abstraction for concurrency control. Its purpose 
is to enqueue "jobs" (protected by a lock) and ensure that only one thread will 
execute them. There is no distinction as to which thread this will be, as well 
as no execution condition. The executive thread is simply the one that acquires 
the lock first.

xworkq is generally used when multiple threads want simultaneous access to a 
critical section. Instead of spinning indefinitely, waiting for a thread to 
finish, they can enqueue their job in the xworkq and resume processing other 
requests. xworkq is also generic by nature, since the "job" is simply a target 
function and its input data.

On the following figure, we can see the design of xworkq:

% Design of xworkq
\fixme add figure

It consists of a queue where jobs are enqueued.  The thread that enqueues a job 
can attempt to execute it too, by acquiring a lock for the xworkq. If the lock 
is unheld, the thread will acquire and will be able to execute the enqueued 
job. Else, it can safely leave and its job will be executed by the thread that 
has holds the lock.

In cached context, every object has an xworkq. Whenever a new request is 
accepted/received for an object, it is enqueued in the xworkq and we are thus 
ensured that only one thread at a time can have access to the object's data and 
metadata.

\subsection{The xwaitq xtype}\label{sec:xwaitq-design}

The xwaitq xtype bears some similarities to the xworkq xtype. Like xworkq, it 
is also an abstraction where "jobs" are enqueued and dequeued later on to be 
executed. Unlike xworkq though, jobs are executed only when a predefined 
condition is met. Another distinction is that the jobs in xwaitq are considered 
to be thread-safe and can be executed concurrently by any thread.  

xwaitq is commonly used in non-critical code sections that can be executed only 
under specific, predefined circumstances. The "jobs" that are enqueued in 
xwaitq are the same as the jobs of xworkq.

% Design of xwaitq
\fixme add figure

Unlike xworkq, before a job is enqueued, the thread can attempt to execute it 
by checking the execution condition. Only if the condition is \textbf{not} met 
does the thread enqueue the job to the queue. Before the thread leaves, it 
"signals" the queue and essentially rechecks the condition to ensure that it 
can't be executed. It can then safely leave since its job will be executed when 
another thread signals the queue successfully.

In cached context, xwaitqs are used to enqueue jobs which cannot be executed 
immediately. Common cases are when we have run out of space, when we have run 
out of requests etc.

\section{Cached Design}\label{sec:cached-design}

At this point, we have discussed in length the design of the cached components.  
Having the above sections in mind, we can proceed with presenting how cached 
has been designed.

Cached has been been designed mainly as the orchestrator, a peer that utilizes 
several different components to handle various tasks such as indexing (xcache), 
concurrency (xworkq) and deferred/conditional execution (xwaitq). Cached 
however is not limited to the above role as these components do not cover all 
of the needed tasks. There are several other key tasks that cached must 
undertake, namely:

\begin{enumerate}
	\item Request handling
	\item Write policy enforcing
	\item Cache coherence
\end{enumerate}

Moreover, cached extends its repertoire using some unique components, namely:

\begin{enumerate}
	\item Bucket pool
	\item Cache-IO
	\item Book-keeping
\end{enumerate}

We will illustrate the design of cached from two different perspectives: the 
operational perspective, which can be seen in Figure 
\ref{fig:cached-design.pdf}, and the component perspective, which can be seen 
if Figure ?. Moreover, we will further explain how cached manages the above new 
components and tasks in the following sections.

\diagram{cached-design.pdf}{Cached design}

\fixme add component diagram for cached (xcache, xworkq, objects, buckets etc.)

\subsection{Request handling}

Cached operates as a peer that receives requests from the vlmc. The majority of 
these requests will be read/write requests, but there are other types of 
requests too such as copy requests (sent when an object is copied-on-write) and 
info requests i.e. queries on what is the size of an object. Each of these 
requests must be handled independently, using special-purpose functions.

Furthermore, cached will also issue requests to the blocker mainly on two 
occasions: when it flushes a dirty object and when operating in write-through 
mode.

This means that, cached must be able to categorize requests and send them to 
the appropriate functions. Moreover, it must be able to create requests of its 
own, as well as handle cases such as running out of requests.

\subsection{Write policy enforcing}\label{sec:cached-wcp-design}

The user defines beforehand what will the write policy of cached be. There are 
two options: write-through and write-back. These policies aren't new and have 
already been discussed in chapter ?, but let's see what these policies 
translate to in cached context.

\begin{itemize}
	\item
		In \textbf{write-back} mode, cached caches writes, immediately 
		serves the request back and marks the data as dirty. When a read 
		arrives, it either serves the request with the dirty data 
		(read-hit) or forwards the request to the storage peer and 
		caches the answer (read-miss).

		This policy is used when we want to improve read and write speed 
		and can sacrifice data safety.
	\item
		In \textbf{write-through} mode, cached forwards writes to 
		blocker, servers the request when blocker replies, caches the 
		data and marks them as valid.  When a read arrives, it either 
		serves the request with the valid data (read-hit) or forwards 
		the request to the storage peer and caches the answer 
		(read-miss).

		This policy is used when we want to improve read speed and want 
		to make sure that no data will be lost.
\end{itemize}	

These policies are specified once during cached's deployment and cannot be 
switched on/off later on.

\subsection{Cache coherence}

Cache coherence is a concept that is closely connected with concurrency 
control, as is evident in Figure \ref{fig:cached-design.pdf}. It is commonly 
used as a term in the literature on the subject of caches for multiprocessor 
systems or distributed caches and it refers to:

\begin{enumerate}
	\item the consistency of data that are stored in the cache and
	\item how each change is propagated through the system, to the other 
		storage tiers.\label{list:second-coherence}
\end{enumerate}

If we wanted to rephrase the above to match our case, we could say that it 
refers to the consistency of the data that are stored in cached and how each 
change is propagated to the blocker.

The consistency of the cached data is greatly secured using the xworkq as the 
guard of parallel accesses to the object's data. However, the guarantee that 
data updates are conducted serially does not suffice for the consistency 
requirement.

The problem is evident if you consider the eviction of an object with dirty 
data. xcache does not operate on object level and thus is unaware of the 
contents of the cache entry. Thus, cached must use the cache entry finalization 
hook to increment the refcount of the object so that it can not be removed 
until all data have been flushed to the blocker. Essentially, cached overrides 
the xcache's book-keeping to maintain cache coherency.

The propagation of the data to the blocker is solely cached's job. One might 
consider that flushing the dirty data of an object would be sufficient to 
propagate the changes to the blocker. However, consider the scenario when an 
object is evicted and then subsequently reinserted, overwritten and evicted 
again. The result would be two flush requests, for the same data, sent to the 
blocker. Although the blocker guarantees that there will be no page-tearing, we 
cannot be sure about the order on which the data will be written.

To solve this problem, flush jobs must be deferred (using an xwaitq) if another 
flush for the same object is in flight. 

\subsection{Bucket pool}

% Explain what buckets are:
% a) That they are bounded
% b) That they have states
% c) That they are preallocated and every thread enqueues and dequeues an index 
% to them

We have already explained in Section \ref{sec:rationale-design} the reason why 
cached caches objects. There is, however, one important design issue that we 
must address. The issue is how will cached perceive the object's data. To make 
it a bit clearer, given that an object typically has 4MB of size, what should 
cached do when a user requests e.g. a 16KB chunk of it?

If we perceived the object's data as a monolithic chunk, we would need to read 
and cache the whole object just to reply to the user's request. If the user 
then requests a chunk from another object, we would have to cache that object 
too and in the end, we would thrash our cache
\footnote{
	cache thrashing occurs when we aggressively cache data that is only 
	used once and effectively leads to a snowball of evictions
}.

The solution we propose is to further divide objects to the next and final 
logical entity, \emph{buckets} (typically 4KB of size). Each bucket consists of 
its data and metadata and cannot be half-empty, or half-allocated. This way, we 
can also know which parts of the cached object are actually written, or are in 
the process of being read etc.

Thus, our solution to the hypothetical problem we have posed above is to 
request 16KB from the blocker, store the result in 4 buckets and then respond 
the request to the user.

The above answer, however, is not entirely complete. It implies that buckets 
are something readily available or attached to the object. Although each object 
\textit{could} have its buckets pre-allocated, this would limit the objects 
that we can cache since that, even if the user requested only a small chunk, 
each object would statically need 4MB of space.

Ideally, we would like to be able to cache thousands of objects but
\begin{inparaenum}[i)]
\item allocate a much smaller amount of buckets and
\item strictly when the user requests to.
\end{inparaenum}
To make things even faster, we would also like the buckets to be preallocated 
(like cache nodes in Section \ref{sec:xcache-entry-design}) to avoid the 
overhead of malloc/free system calls.

We have achieved the above by creating a bucket pool. The design of the bucket 
pool is the following:

\fixme add diagram

The bucket pool size is static and has been set during initialization by the 
administrator. After the necessary space has been allocated, it is divided in 
buckets and all the bucket indexes are pushed in a lock-protected stack (xq).  
Then, each thread can pop bucket indexes from the pool and attach them
to an object when needed. When that object is evicted, its attached buckets 
indexes are pushed back to the pool.

\subsection{Cache-IO}

When a request is accepted, all Archipelago peers commonly embed it in a peer 
request (you can see more about peer requests here ?). The peer request always 
holds the original request and optionally, several other fields that are of 
importance to the peer.

In our case, we keep in cached's peer requests a new structure called Cache-IO, 
on which we store the xcache handler of the request handler as well as the 
state of the request. Moreover, since the request may break internally in many 
others, we keep track of how many pending requests remain until the original 
one can be completed.

\subsection{Book-keeping}\label{sec:cached-states-design}

In order to know when an object is in flushing state, when a bucket has been 
allocated or when a bucket is being read, cached must employ some sort of 
book-keeping of their states. The entities that require to track their states 
are:

\begin{enumerate}
\item \textbf{buckets:} Each bucket has two different states that must be 
	tracked. The first is its allocation state:
	\begin{enumerate}[i)]
		\item free, meaning that the bucket is not allocated
		\item claimed, meaning that the bucket is allocated
	\end{enumerate}
	The second is its data state:
	\begin{enumerate}[i)]
		\item invalid, meaning that it currently holds no data
		\item valid, meaning that it has data that correspond with the 
			blockers data
		\item loading, meaning that a read request has been sent to the 
			blocker to be filled with data
		\item dirty, meaning that it currently holds newer data that 
			what the blocker has.
	\end{enumerate}
\item \textbf{Objects:} The bucket states of an object provide a good 
	indication of the object's status, yet not a complete one. The statuses 
	we keep for the objects are:
	\begin{enumerate}[i)]
		\item ready, meaning that it is ready to accept data
		\item invalidated, meaning that it is not ready to accept data
		\item flushing, meaning that it is currently flushing dirty 
			data to the blocker
		\item failed, meaning that a request has failed for this object 
			and we must stop using it
	\end{enumerate}
\item \textbf{Cache-ios:} Cache-ios also have states. They are the 
	following:
	\begin{enumerate}[i)]
		\item accpted, meaning that the request has just been accepted
		\item reading, meaning that the request wants to read data
		\item writing, meaning that the request wants to write data
		\item served, meaning that the request has been served the data 
			it needed
		\item failed, meaning that the request has failed
	\end{enumerate}
\end{enumerate}

Moreover, we keep global and per-object counters of every object's bucket 
states. The benefits from this approach is that we can know at any time if an 
object (or cached in general) has a bucket in a certain state.

\section{Cached Operation}\label{sec:op-design}

\subsection{Write}

This is the flow for the write path:

\fixme add diagram and explanation

\subsection{Read}

This is the flow for the read path:

\fixme add diagram and explanation

\subsection{Copy}

This is the flow for the read path:

\fixme add diagram and explanation

\subsection{Info}

This is the flow for the read path:

\fixme add diagram and explanation

