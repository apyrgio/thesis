\chapter{Necessary theoretical background}\label{ch:theory}

In this chapter, we will explain the main concepts and mechanisms that are used 
by Archipelago as well as our implementation. The concepts that will be 
discussed are basic as far as Operating Systems are concerned and should pose 
no comprehension difficulties to a reader with elementary background on the 
subject.

More specifically, Section \ref{sec:multi-theory} discusses what is 
multithreading and lists its advantages and disadvantages. Section 
\ref{sec:ipc-theory} introduces the Interprocess Communication mechanisms that 
are employed by Linux and concentrates on the ones that are used in Archipelago 
and our implementation. Finally, Section \ref{sec:conc-theory} explains the 
various concurrency control methods that exist to synchronize threads or 
processes.

\section{Multithreading}\label{sec:multi-theory}


Mutlithreading is a programming concept that has been the subject of research 
long before the emerge of SMP systems\footnote{
	Symmetric multiprocessing systems, commonly systems with multiple 
	processors}. More specifically, temporal multithreading has been introduced 
in the 1950s whereas simultaneous multithreading (SMT), which is the current 
invocation of multithreading programming, was first researched by IBM in 
1968\cite{mt}.

The difference between these two types is the number of threads that can run 
simultaneously on the system. For simultaneous multithreading, there are 
commonly more than one threads that can run in parallel, whereas for temporal 
multithreading, there is only one. This means that the running thread must be 
descheduled to let the other threads run. Temporal multithreading was an old 
concept that predates SMP systems and this limitation mirrors the limitations 
of the hardware of that era.

Before threads, programs could utilize the concurrency of SMP systems using 
forked processes that would communicate with each other. The introduction of 
threads did not render this practice obsolete, but instead provided an 
alternative technique to speed up applications.

Threads and process have some fundamental differences, which are shown in the 
following list:

\begin{itemize}
	\item Threads are always parts of a process, whereas processes are 
		independent from each other and may only have a parent-child connection 
		between them.
	\item Forked processes have their own address space and resources, which 
		are inherited by the parent process with CoW semantics.  Multiple 
		threads, on the other hand, usually share the same memory and resources 
		with the other threads in the same process.
\end{itemize}

From the above differences, we can see that there are no clear advantages of a 
multithreaded approach over a multiprocess one. To better demonstrate our 
point, we will present the advantages and disadvantages of multithreading 
programming in the following lists:

The advantages are:

\begin{itemize}
	\item Context switching is generally faster between threads, mostly due 
		to the fact that the TLB
		\footnote{Translation Lookaside Buffer, a hardware cache that speeds up 
			the translation of virtual addresses to physical RAÎœ pages.}
		cache does not need to be flushed. The TLB cache misses are 
		expensive and are avoided as much as possible\cite{tlb}.
	\item Sharing data between threads is easier, due to the fact that they 
		use the same memory by default.
\end{itemize}

Whereas the disadvantages are:

\begin{itemize}
	\item Processes are more isolated than threads, which means they are 
		guarded against two things:
		\begin{inparaenum}[i)]
		\item thread-unsafe functions and
		\item data corruptions, which if they happen to one thread they 
			bring the whole process to a halt.
		\end{inparaenum}
\end{itemize}

Regardless of the chosen method, at some point the programmer will have face 
two of the biggest challenges of multithreading/multiprocess programming;
interprocess communication, which is discussed in Section \ref{sec:ipc-theory}, 
and concurrency control, which is discussed in Section \ref{sec:conc-theory}.

\section{Interprocess Communication - IPC}\label{sec:ipc-theory}

Interprocess Communication is a concept that predates the SMP systems that we 
all use nowadays. It is a set of methods that an OS uses to allow processes and 
threads to communicate with each other. Archipelago for example, uses various 
IPC methods to synchronize its different components.

The full list of Linux's IPC methods is presented below:

\begin{itemize}
	\item \textit{Signals,} which are sent to a process to notify it that 
		an event has occurred.
	\item \textit{Pipes,} which are a one-way channel that transfers 
		information from one process to another.
	\item \textit{Sockets,} which are bidirectional channels that can 
		transfer information between two or more processes either 
		locally or remotely through the network.
	\item \textit{Message queues,} which is an asynchronous communication 
		protocol that is used to exchange data packets between 
		processes.
	\item \textit{Semaphores,} which are abstract data types that are used 
		mainly for controlling accesses on a same resource.
	\item \textit{Shared memory,} which is a memory space that can be 
		accessed and edited by more than one process.
\end{itemize}

We will concentrate on the following IPC methods:
\begin{inparaenum}[i)]
\item signals,
\item sockets and
\item shared memory,
\end{inparaenum}
since these are the methods that Archipelago and our implementation use.

\subsection{Signals}

Signals are notifications that are sent to processes and can be considered as 
software interrupts. The signal's purpose is to interrupt the execution of a
process and inform it that an event has occurred.

Given that there more than one events and exceptions that can occur in a 
system, there are also various signals that match to each one of these
events. For more information about the signals that Linux supports as well as 
the conditions on which they are raised, the reader is prompted to consult the 
man pages for signal(7) or read the POSIX.1-1990, SUSv2 and POSIX.1-2001 
standards.
  
Moreover, the above standards dictate the standard behavior of a process when a 
signal is received. The standard actions that a process can take, fall roughly 
in the following categories:

\begin{itemize}
	\item ignore the signal,
	\item pause its execution,
	\item resume its execution or
	\item stop its execution and/or dump its core
\end{itemize}

Finally, a process is not limited to this set of actions. It can instead do one 
of the following things for each signal, with the exception of SIGKILL and 
SIGSTOP signals:

\begin{itemize}
	\item ignore the signal
	\item block the signal, which is part of the Archipelago IPC and its 
		usage is described in Section ?
	\item install a custom signal handler function, which essentially 
		passes the signal handling task to the process.
\end{itemize}

\subsection{Sockets}

Sockets are a bidirectional means of sending data between processes.  The 
processes can be in the same host but most commonly, they are in remote hosts 
and the data are sent over the network. Furthermore, from all the IPC methods 
that we have described above, sockets are the only method that enables remote 
communication. 

There are many socket implementations for different purposes, which are divided 
in several communication domains, most of which are rather obscure. The three 
communication domains, however, that are supported by most UNIX and UNIX-like 
operating systems are:

\begin{itemize}
	\item \textit{IPv4} domain, which allows communication between 
		processes over the Internet Protocol version 4 network.
	\item \textit{IPv6} domain, which allows communication between 
		processes over the Internet Protocol version 6 network.
	\item \textit{UNIX} domain, which allows communication between 
		processes in the same host
\end{itemize}

The above three communication domains are further divided in two types, based 
on the transport layer protocol that they use.

\begin{itemize}
	\item \textit{Stream sockets}, which use the Transmission Control 
		Protocol (TCP) or Stream Control Transmission Protocol (SCTP),
	\item \textit{Datagram sockets}, which use the User Datagram Protocol 
		(UDP),
\end{itemize}

The TCP/UDP protocols are only one layer out of the four layers of the TCP/IP 
protocol stack that the RFC 1122\cite{1122} defines, and we will explain them 
in detail in the following sections.  Although a thorough explanation of the 
TCP/IP protocol stack is out of the scope of this thesis and is not needed to 
understand the following sections, we will provide a brief explanation of it 
for the sake of completeness.  

The TCP/IP protocol stack is the basis for the World Wide Web and the most used 
form of networking. It specifies all the stages of the data processing that 
need to happen in various levels and entities, such as operating systems, 
network cards, routers etc. in order to connect two machines over the network.  
For this reason, the data that are sent are encapsulated in layers, which can 
be seen in Figure \ref{fig:data-encapsulation.pdf}.

\diagram{Data encapsulation for a UDP packet}{data-encapsulation.pdf}

We now continue with an presentation of TCP and UDP protocols.

\subsubsection{TCP}

The Transmission Control Protocol is connection-oriented, i.e. it provides 
unique connection between two sockets, and has the following key features:

\begin{description}
	\item[Reliability] The data will arrive to the receiver as a whole, or 
		they will not arrive at all. In the latter case, the receiver 
		may receive sparious packets but it will not acknowledge them 
		until it has received all of them.
	\item[Ordered transfer] The data will arrive in the same order that 
		they were sent.
	\item[Error-checking] The data are checksummed to allow the receiving 
		end to check if there was any data corruption.
	\item[Rate-limiting] When the receiver accepts packets with slower rate 
		than the sender, the sender will adjust its rate to ensure 
		packet delivery and less congestion.
	\item[Byte-stream] The data that are sent do not have a boundary.
\end{description}

\subsubsection{UDP}

The User Datagram Protocol on the other hand is a much simpler protocol. It is 
used to send \textit{datagrams}, which are basically extended IP packets with 
some extra features. The UDP protocol has the following differences from 
TCP:

\begin{itemize}
	\item It is connectionless, meaning that the socket can receive 
		requests from anyone.
	\item It provides no guarantees about the delivery of the messages.  
	\item The messages can arrive in other order than the one they were 
		sent.
	\item There is no rate-limiting, meaning that the congestion control 
		must be handled in the application level.
	\item It cannot send streaming data since datagrams are bounded.
\end{itemize}

The UDP protocol is often preferred over TCP by applications that value speed 
over data loss (e.g. video streaming applications) due to its low overhead.

\subsection{Shared memory}

When two or more processes share the same memory segment, they can exchange 
data by placing it in a region of the segment. The data then becomes instantly 
visible to the other processes too, since their page-table entries for this 
segment point to the same physical RAM pages.

A popular way of mapping shared memory to a process's address space, which is 
also used in Archipelago, is with POSIX \texttt{mmap()}. There are two mapping 
types of mapping:

\begin{itemize}
	\item \textit{Private mapping}, in which case the mapping contents will 
		not be visible to other processes that have mapped the same 
		file and
	\item \textit{Shared mapping}, in which case the mapping contents will 
		be visible to all processes that map this file and changes to 
		the mapping will be propagated to the shared memory.
\end{itemize}

Finally, an issue with mappings is that the start of the shared memory is not 
always mapped in the same virtual address for all processes. For this reason, 
when processes want to share data, they should not pass direct pointers to 
them, but relative pointers (i.e. offsets) from the start of the segment, which 
are common for all processes and can be translated to the correct direct 
pointers.

\section{Concurrency control\label{sec:conc-theory}}

Concurrency control is the set of methods that a program uses to ensure that 
concurrent accesses to the same data will leave them in a consistent state.

There are several techniques that are used for concurrency control and are 
listed below:

\begin{itemize}
	\item \textit{Spinlocks}, which are locks that protect a critical segment.  
		Typically, a thread acquires a lock at the start of the critical 
		segment and releases it at the end of it. Threads that are waiting for 
		the lock essentially "spin", i.e. they busy-loop until the lock is 
		released.
	\item \textit{Mutexes}, which are locks that protect a critical segment in 
		the same fashion as spinlocks. Their difference from spinlocks, however 
		is that if a thread cannot get the lock, it will block instead of 
		busy-loop.
	\item \textit{Semaphores}, which are also an IPC method. In concurrency 
		control context, they are abstract data types that restrict the 
		number of simultaneous accesses to a resource or a critical 
		segment.  When the number of times is one (1), they essentially 
		degenerate to mutexes, with the main difference that they have 
		no concept of an owner.
	\item \textit{Atomic operations}, which are hardware-assisted 
		operations whose purpose is to atomically update a value as 
		fast as possible. The atomicity is usually achieved with 
		implicit hardware locks on the bus or cache-line.  Atomic 
		operations come in many flavors such as "add-and-fetch", 
		"compare-and-swap" etc.
\end{itemize}

Concurrency control - and locking in particular - have three important caveats 
that the programmer needs to know before he/she decides on the techniques that 
will be used:

\begin{description}
	\item[Lock overhead] \hfill \\
		Lock overhead is the overhead that the locking mechanism 
		introduces.  For example, semaphores are a mechanism with big 
		overhead, since they must be read and written to using system 
		calls. If the critical segment they protect is simply the 
		update of a variable, then the programmer is probably better 
		off using a spinlock or atomic operations.
	\item[Lock contention] \hfill \\
		Lock contention can be considered as the overhead of the 
		coarseness of the lock. There is contention for a lock when it 
		is requested by many threads, to the point that the waiting 
		time is longer than the execution time.
		This has a big performance impact to the implementation, since 
		threads may consume their scheduled time spinning until they 
		acquire a lock, or sleeping while they could do something more 
		useful.
		The solution to this problem is commonly to redesign the 
		locking scheme in order to break such locks into smaller ones.
	\item[Deadlocks] \hfill \\
		Deadlock is a situation in a multi-lock scenario where each 
		process in a group of processes needs to acquire a lock which 
		is held by another process in the same group. Since no process 
		can continue, the operation of the group is essentially 
		stalled.

		\diagramstrict{Deadlock example}{deadlock.pdf}

		As a rule of thumb, the circular dependency of the locks can 
		break if the locks are acquired in a predefined order. This 
		however is only possible in less complex scenarios and in 
		general, a more well-thought design is required.
\end{description}
