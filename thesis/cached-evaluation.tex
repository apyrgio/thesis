\chapter{Performance evaluation of cached}\label{ch:cached-evaluation}

\begin{flushright}
	\textit{"There are three kinds of lies:\\
		lies, damned lies, \\
		and \sout{statistics} benchmarks."}	\\

	Mark Twain (modernized)
\end{flushright}

It may seem as an ironic statement, considering that we are about to provide 
benchmark results for cached, but it's actually is a valid one.
\begin{comment}
What Mr.  Twain tries to say here
\footnote{
	and that's a phrase usually not heard in programming contexts...
}
is that the presentation of partials facts for something can be used to 
fabricate a plausible truth for it.
%This is shit, fix it
In science's case, it so often happens that promising results for an experiment 
can seem more important to the researcher's eye than negative ones due to 
positive reinforcement.
\end{comment}
In our case, we will try not to merely smear the next pages with diagrams but 
first explain the benchmarking methodology behind them and then provide a 
concrete depiction of cached's performance under various workloads.

The skeleton of this chapter is the following: Section \ref{sec:perf-meth} 
explains the methodology behind our measurements. Section \ref{sec:test-bed} 
provides details about the hardware on which we have conducted our benchmarks.  
Section \ref{sec:perf-plot} presents the results of the benchmarks that we have 
conducted and provides in-depth explanations about each of them.  Finally, 
Section ?  is reserved for cached on a VM (\fixme take these measurements).

\section{Benchmark methodology}\label{sec:perf-meth}

The benchmarks that have been executed and whose results are presented in this 
chapter, will be split in two categories, both of which have their own distinct 
goals:

The first category is the comparison between using cached \textbf{on top} of 
the sosd peer (sosd has been discussed here ? \fixme add section) and using 
solely the sosd peer.

\begin{comment}
The category's goal is to "defend" one of the core thesis arguments, that 
tiering is a key element that will improve the performance of Archipelago.  
\end{comment}

In order to effectively compare the performance of cached and sosd, we must 
consider the following: 

\begin{enumerate}
	\item The comparison of the two peers should try to focus on what is 
		the best performance that these peers can achieve for a series 
		of tough workloads.
	\item The circumstances under which both peers will be tested need not 
		be thorough but challenging. For example, it may be interesting 
		to test both peers against sequential requests, but
		\begin{inparaenum}[i)]
		\item such patterns are rarely a nuisance for production 
			environments
		\item they do not stress the peers enough to provide something 
			conclusive
		\item they are out of the scope of this section as there can be 
			many of these kinds of tests and adding them all here 
			will impede the document's readability.
		\end{inparaenum}
	\item Both peers must be tested under the same, reasonable workload, i.e a 
		workload that can be encountered in production environments.
	\item If the peer doesn't show a consistent behavior for a workload, it 
		must be depicted in the results.
\end{enumerate}

Having the above in mind, the next step is to choose a suitable workload.  This 
choice though is fairly straight-forward; in production environments, the most 
troublesome workload is the burst of small random reads/writes and is usually 
the most common one that is benchmarked.  

One may ponder however, how many requests can be considered as a "burst" or 
which block size is considered as "small". Of course, there is not only one 
answer to this question so, we will work with ranges. For our workload, we will 
use block sizes ranging from 4KB to 64KB and parallel requests ranging from 4 
to 16.

The second category deals solely with the inner-workings of cached and its 
behavior on different operation modes or states. Its aim is not to capture the 
performance against a tough workload, but to explain \textbf{why} this 
performance is observed and how each of the options affect it. In this 
category, we measure how threads impact the performance of cached or what 
impact (if any) does our index mechanism have.

Finally, in the following sections, for brevity reasons, we will talk about 
comparing cached and sosd. What the reader must keep in mind however is that 
cached is essentially the cache layer above sosd, which means that we actually 
test sosd vs cached over sosd.

\section{Specifications of test-bed}\label{sec:test-bed}

The specifications of the server on which we conducted our benchmarks is the 
following.

\begin{table}[H]
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Component & Description \\ \hline \hline
		CPU &  2 x Intel(R) Xeon(R) CPU E5645 @ 2.40GHz \cite{e5645} \\
		 & Each CPU has six cores with Hyper-Threading enabled, which equals to 
		 24 threads. \\ \hline
		RAÎœ & 2 banks x 6 DIMMs PC3-10600 \\
		& Peak transfer rate: 10660 MB/s \\ \hline
	\end{tabular}
	\caption{Test-bed hardware specs}
	\label{tab:hardware-specs}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Software & Version \\ \hline \hline
		OS &  Debian Squeeze \\ \hline
		Linux kernel & 3.2.0-0 (backported) \\ \hline
		GCC & Debian 4.4.5-8 \\ \hline
	\end{tabular}
	\caption{Test-bed software specs}
	\label{tab:software-specs}
\end{table}

\todo add configuration of RADOS cluster (journal, backing storage, network 
connection)

\fixme Mention that the peers were evaluated by sending requests directly at 
their ports.

\section{Performance comparison between cached and sosd}
\label{sec:perf-plot}

As mentioned above, for our first test, we will evaluate the read and write 
performance of cached and sosd for a random workload with parallel requests of 
small size. In order to measure accurately their performance, we will use two 
different metrics:

\begin{description}
	\item[Bandwidth:] \hfill \\
		Bandwidth measures the maximum throughput that the application 
		can sustain. This metric is usually used to indicate how much 
		I/O (from various inputs) can an application handle within a 
		second.
	\item[Latency:] \hfill \\
		Latency is the converse of bandwidth. It is a measurement from 
		the viewpoint of the issuer of requests and indicates the 
		responsiveness of the tested application. It is commonly 
		calculated as the average reply time for a series of requests.
\end{description}

On the following sections, we present the benchmarks that we conducted for the 
first category. The first benchmark, which is shown in Section 
\ref{sec:peak-plot} attempts to depict the behavior of cached during a peak of 
I/O requests. The second benchmark, which is shown in Section 
\ref{sec:sustained-plot}, illustrates the behavior of cached under continuous 
load.  On these sections, cached is always multi-threaded and uses 4 threads.

\subsection{Workload smaller than cache size - Peak behavior}
\label{sec:peak-plot}

We begin with the bandwidth performance of our peers. The write performance can 
be seen in Figure \ref{fig:bw-write-comp-lie.pdf} while the read performance 
can be seen in Figure \ref{fig:bw-read-comp-lie.pdf}.

Before we proceed with the interpretation of the diagram results, we will 
briefly comment on the diagram structure. Due to the fact that the performance 
of the two peers differs in at least two orders of magnitude, the results would 
look too flat in a conventional diagram that would scale from 0 to 11000. To 
amend this, we have broken the y-axis of our diagrams in two parts with 
different scales and starting values, in order to make the comparison easier to 
the eye.

\diagram{Comparison of bandwidth performance for write 
	peaks}{bw-write-comp-lie.pdf}
\diagram{Comparison of bandwidth performance for read 
	peaks}{bw-read-comp-lie.pdf}

The initial results look very promising. For write requests, the speedup for 
very small block sizes (4KB - 16KB) is approximately \textbf{100x} whereas for 
larger ones (32KB - 64KB) it ranges from \textbf{50x to 200x}. For read 
requests, the speedup for very small block sizes is approximately \textbf{50x}, 
whereas for larger block sizes, it ranges between \textbf{20x - 75x}.

These results not only illustrate the performance gap between RAM and
HDDs but also show that our implementation manages to keep its bandwidth 
consistently over 1 GB/s in stress scenarios. However, we must keep in mind 
that they only show a part of the full picture, since it does not show how 
cached behaves past its cache size.

Moreover, upon closer inspection the following questions arise:

\textbf{1) Where is the speedup difference between reads and writes attributed 
	to?}

We have mentioned above that the speedup for writes is between 50x and 200x 
whereas for reads is between 20x and 75x. Although it is a large speedup in 
both cases, it also shows some of the shortcomings of our implementation.

We attribute the speedup difference to three factors:

\begin{enumerate}
	\item The performance of cached is almost the same for writes and 
		reads.  This is expected behavior as the read and write paths 
		for cached have many common parts (\fixme show write/read path)
		\footnote{To be more strict, reading seems a bit faster than 
			writing, but that is probably attributed to CPU caches 
			and the fact that reading from RAM is a non-destructive 
			operation in contrast to writing.}.
	\item Cached doesn't scale much past the 16KB block size. This is an 
		interesting observation with an unexpected answer. It may seem 
		implausible at first, but what happens is that we are actually 
		hitting the bandwidth limit of the RAM modules. If we consult 
		the table \ref{tab:hardware-specs}, we can see that the 
		bandwidth limit of the RAM is about \mytilde10.7GB/s
		\footnote{A more careful look shows that we surpass this limit, 
			which is expected given the fact that we have a multi 
			channel RAM setup.}.
		This limit is approached asymptotically as the block size 
		increases and the index overhead decreases (more about the 
		index overhead later in the measurements of Section ? \fixme 
		add section).
	\item On reads, sosd is benefited from the existence of caches in 
		various levels e.g. on OSD level, on RAID controller level.
\end{enumerate}

To sum up, the cached's performance remains relatively the same in both reads 
and writes, it is merely the sosd that is getting faster in reads due to 
caches. 

\textbf{2) Why is cached's performance increased along with block size?}

We expected that sosd's performance would increase proportionally to the block 
size, due to the rotational nature of hard disk drives, but why does this 
affects cached too? It is certainly not attributed to any \texttt{memcpy()} 
performance tricks, since we always write in bucket granularity, which means 
that a 16KB write is translated to 4 x 4KB writes.

From this observation, we extrapolate that the CPU operations such as indexing,
job enqueuing, accepting and responding requests, that occur proportionally 
more times for smaller block sizes, dominate the cached's performance.

We can see this more clearly if we consult Table \ref{tab:2threads}.

\begin{table}[H]
	\centering
	\begin{tabular}{ | l | l | l | l | l |}
		\hline
		\input{tables/cached-2threads-write.tex}
	\end{tabular}
	\caption{Write performance results for 2-threaded cached}
	\label{tab:2threads}
\end{table}

\fixme elaborate on that

Merely using two threads instead of four lessens the performance gap 
significantly.

\textbf{3) Why cached's perforance does not improve proportionally to the 
	parallel requests?}

The reason why we see only a minor increase in the performance of cached, even 
though it is multi-threaded, is because our locking scheme is not fine-grained 
enough. We have a single lock for our request queue, a single lock for most of 
the hash table accesses and this inevitably causes a lot of threads to spin.  
This slight improvement we see is mainly due to the fact that requests are 
effectively being pipelined while waiting each other to release its lock.  

We now proceed to the latency results. The write performance can be seen in 
Figure \ref{fig:lat-write-comp-lie.pdf} while the read performance can be seen 
in Figure \ref{fig:lat-read-comp-lie.pdf}.

\diagram{Comparison of latency performance for write 
	peaks}{lat-write-comp-lie.pdf}
\diagram{Comparison of latency performance for read 
	peaks}{lat-read-comp-lie.pdf}

The latency results corroborate the observations that we made for the bandwidth 
results. As we can see, the latency of cached is proportional to the number of 
parallel requests, which reinforces the assumption that our locking scheme is 
not granular enough, whereas sosd has only a small variance.

\subsection{Workload larger than cache size - Sustained behavior}
\label{sec:sustained-plot}

We now proceed to the second part of the comparison between cached and sosd.  
On this part, we will once again evaluate their performance against a random 
workload with many parallel requests. Unlike the first part though, where the 
cache size was larger than the workload, on this part the cache size will
only be half of it. Having a smaller cache than the workload is after all the 
most common scenario for production environments and the results should 
complete the picture of cached's behavior in a production environment.

For this test, there are two main parameters we must take into account: the 
cache size and the maximum objects. These parameters have been decoupled in our 
implementation and we expect different results for each combination. More 
specifically, we have tested cached with half the workload size and:

\begin{enumerate}
	\item half the objects of the workload (referenced as 
		\textbf{cached-limited} from now on).
	\item more than the objects of the workload (referenced as 
		\textbf{cached-unlimited} from now on).
\end{enumerate}

Also, we have tested cached in write-through mode, to see its behavior in 
sustained writes. \fixme say it has unlimited objects 

Furthermore, for this benchmark, we tested only the write performance of cached 
and sosd.  Also, we chose to use 16 parallel requests and 4KB block sizes, 
since these seemed the most troublesome in the previous benchmark.

Before we proceed to the results, we must explain first how the diagrams below 
are structured. Since cached's performance is fairly unstable in this scenario, 
we have chosen to illustrate it as the benchmark progresses.  Thus, the x-axis 
shows what percent of the benchmark has been completed and for every 5\%, we 
show the performance of cached for that part.

The bandwidth results can be seen in Figure \ref{fig:bw-write-comp-truth.pdf}, 
while the latency results can be seen in Figure 
\ref{fig:lat-write-comp-truth.pdf}.  Due to the vast bandwidth drop of cached, 
the y-axis uses a logarithmic scale to show a more clear picture of what 
happens when the performance reaches the 10 MB/s mark.

\diagram{Comparison of bandwidth performance for sustained writes} 
{bw-write-comp-truth.pdf}

\diagram{Comparison of latency performance for sustained writes}
{lat-write-comp-truth.pdf}

This diagram has several interesting points which we will address below.

\textbf{1) Why cached-unlimited's performance drops in the middle of the 
	benchmark?}

Since cached-unlimited can index more objects than the workload, there is no 
need to evict entries from the hash table to insert new ones. Cached-limited on 
the other hand, has at any point a 50\% chance to receive a request for a 
target that does not index. Thus, when cached-unlimited starts to receive the 
first 50\% of requests, it only needs to store them and, as we have seen in the 
first part of this section, it does so very fast.

However, at around 50\% of the benchmark its space is depleted and thus, it has 
to manually evict entries. At this point, we see that its performance 
degenerates to the performance of cached-limited.

\textbf{2) Why cached-limited's performance is significantly less than the 
	sosd's performance?}

As we have explained above, cached-limited constantly evicts entries due to its 
object limitation. It may seem that the only cost of eviction is that the 
performance degenerates to the performance of sosd, but that is not entirely 
true.

When an entry is evicted, all of its contents must be flushed before the new 
entry can overwrite them. Moreover, the cost of concurrency control for hash 
table migrations and the cost of creating new requests and copying the data in 
them is not negligible. Finally, due to constant evictions, there are only a 
few buckets cached for any entry, which in turn does not leave any margin for 
coalescences.

\textbf{3) Why cached-unlimited's performance increases after the 50\% mark ?}

After the 50\% mark, cached-unlimited will have to resort to evictions in order 
to store new data. Unlike cached-limited though, cached-unlimited has already 
cached half of the workload's data, which allows for many coalescences.  
Moreover, cached evicts whole objects to get their buckets, which means that 
evictions will be less frequent that cached-limited, which evicts entries to 
index other entries.

\textbf{4) Why cached in write-through mode performs the same as sods?}

The performance of cached in write-through mode is the same as sosd's since 
write requests are essentially forwarded to sosd and are cached when sosd 
responds. The caching has no significant overhead, compared to the waiting time 
for sosd, since there are no dirty entries and thus no costly evictions.

What is more interesting however is the comparison of read performance between 
cached and sosd, which can be seen in Table \ref{tab:writethrough}.

\begin{table}[H]
	\centering
	\begin{tabular}{ | l | l | l | l | l | l |}
		\hline
		Peer& Bandwidth (MB/s) & IOPS	& Latency (usec) & Iodepth & Block size 
		(KB) \\
		\hline \hline
		\input{tables/sosd-read.tex}
		\input{tables/cached-writethrough-read.tex}
	\end{tabular}
	\caption{Read performance comparison of cached in write-through mode and 
		sosd}
	\label{tab:writethrough}
\end{table}

The read performance of cached has a noticeable increase of 25\%. This is 
expected, since cached serves read requests either from its cached data or from 
sosd. This means that it has the baseline performance of sosd plus the extra 
performance gain from cache hits.

\section{Performance evaluation of cached internals}

To some extent, we have already discussed about the behavior of cached when we 
tweak various of its parameters, such as the over-subscription of its objects, 
the cache size or its write policy. In this category, we will further solidify 
our previous assumptions about the inner workings of cached by measuring:

\begin{enumerate}
	\item how cached scales with the number of threads
	\item the overhead of the index mechanism
\end{enumerate}

Note that the tests will be run with the following parameters:

\begin{description}
	\item[Mode] Write-back
	\item[Block size] 4KB
	\item[Cache size] Always larger than benchmark size
	\item[Operation] Writes
\end{description}

The above options have been chosen to isolate cached of any other factors that 
may alter its performance, such as evictions and delays due to sosd.

\subsection{Cached performance per number of threads}

Measuring the performance impact of multi-threading is nonsensical, if the 
application is not tested against many parallel requests. Hence, we will test 
each n-threaded cached configuration against a range of parallel requests and 
measure its performance.

The bandwidth results can be seen in Figure \ref{fig:bw-write-threads.pdf} 
whereas the latency results can be seen in Figure 
\ref{fig:lat-write-threads.pdf}.

\diagram{Bandwidth performance for each number of 
	threads}{bw-write-threads.pdf}
\diagram{Latency performance for each number of threads}{lat-write-threads.pdf}

From these results, we derive the following conclusions:

\begin{enumerate}
	\item Cached is generally benefited from multi-threading, when the amount 
		of parallel requests is more than two. More specifically, we achieve a 
		major performance improvement of up to 75\% when using two threads, as 
		well as a lower, but significant, performance improvement for up to 
		four threads, as the number of parallel requests increases.
	\item Cached does not scale well past four parallel requests, regardless of 
		the number of threads.
	\item Adding more than two threads degrades significantly the performance 
		of cached, when the number of parallel requests is small.
\end{enumerate}

Finally, these results, along with the results of the first part, clearly show 
that we cannot fully utilize the multi-threading capabilities of cached unless 
we employ a more fine-grained locking scheme.

\subsubsection{Cold cache vs Hot cache}

This scenario will attempt to evaluate the overhead of cache misses in cached
against cache hits for \textbf{write} operations. Theoretically, this should 
account to the overhead of adding new entries to cached and consecutively, an 
indication of the complexity of our index mechanism.

For this reason, we have written 128K (where \textbf{K} is \textbf{*1024} and 
\textbf{M} is \textbf{*1024\^2}), 256K, 512K and 1M objects and have measured 
their latency performance. We expect that the experimental results will verify 
the claim that our implementation is O(1).

To get the most accurate results and since we want to test just the performance 
of our indexing mechanism, we have also used only 1 thread and only 1 IOdepth. 

On Figure \ref{fig:cold1.pdf} we can see the results we were talking about. The 
major point in these results is that we can see that write latency, either of 
cold or warm cache, remains practically the same as the number of objects 
increases.

\diagram{Latency performance of cold/warm cache for variable sizes}{cold1.pdf}

As a side note, we observe a constant decrease in latency as the number of 
objects increase this is not something that should be attributed to our 
implementation. (explain that we have used a hash table that holds 2million 
objects, so it is not mapped to our process's address space. When more objects 
are indexed, the hash table becomes fuller and the latency of mmap()s is 
equally distributed to the objects. Else, the hash table is more scarce but the 
same blocks are hit, albeit not fully written, and thus the mmap latency is the 
same but distributed to less objects.)
