\chapter{Design of cached}\label{ch:cached-design}

In the previous chapters, we have addressed the need for tiering in terms of
scalability as well as performance. % Which are these chapters? Link them.

We have also evaluated current caching solutions and described why they couldn't 
be used as a cache tier in Archipelago. % Provide link to a table of comparisons

With the results of chapter ? in mind, we can provide some more strict 
requirements that our solution must have:

% List the requirements for our solution
\begin{enumerate}
	\item \textbf{Nativity:} Our solution must be native to Archipelago i.e.  
		not need any translation layers to communicate with it.
		%explain why
	\item \textbf{Pluggability:} Our solution must be able to provide a 
		caching layer between peers that are already in operating mode 
		without restarting Archipelago. Also, it must be removed 
		without disturbing the service.
	\item \textbf{In-memory:} Our solution must cache requests in RAM, 
		since the next fastest tier, SSDs, are already being used in 
		RADOS as a journal.
\end{enumerate}

For the following chapters, we will drop the \textit{"solution"} moniker and we 
will use instead the proper name of our implementation, "cached", which simply 
means \textbf{cache d}aemon).

The following two chapters are the main bulk of this thesis and they present our 
own implementation that aims to fill the above requirements.

% Foretell what the following sections will contain
More specifically, this chapter provides an in-depth description of the design 
of cached. Section \ref{sec:rationale-design} provides the design rationale of 
cached and explains how its design meets the above requirements. Section 
\ref{sec:comp-design} presents the building blocks of cached while Sections 
\ref{sec:xcache-design}, \ref{sec:xworkq-design} and \ref{sec:xwaitq-design} 
provide a detailed explanation of their design. Section \ref{sec:cached-design}  
presents the interaction between cached and its building blocks as well as the 
unique components that cached consists of.  Finally, in Section ?  we 
illustrate the flow of requests for cached.

\section{Design rationale}\label{sec:rationale-design}

One of the first architectural decisions was to implement cached as an 
Archipelago user-space peer (see Section \ref{sec:arch-peer} about Archipelago 
peers). This choice was the most natural one since it provides the smallest 
possible communication overhead with the other Archipelago peers. Also, this 
design decision covers the nativity requirement we posed at the beginning of 
this chapter.

The above design choice has another advantage too; we can register on-line the 
cached peer between the vlmc and blocker and unregister it when we want to.  
This opens up numerous possibilities such as plugging cached for 
QoS\footnote{Quality of Service} reasons when there is a peak in I/O requests.  
This is possible because, as we have mentioned in Section \ref{sec:arch-ipc}, 
XSEG ports can be registered on-line. Thus, during normal operation, the 
administrator can add the cached port to the request path between vlmc and 
blocker, and all requests will seamlessly be intercepted by cached. This 
follows the same principle with bcache, which plugs its own request\_fn() 
function to the virtual device it creates.  Unlike bcache however, cached can 
be plugged on and off at any time.

This also means that the pluggability requirement is also being met.

The next important design decision was what will cached index. Given that it 
will reside between the vlmc and blocker, where the VM's requests have already 
been translated to object requests, the natural choice is to cache objects.  

The decision to cache objects not only is the most natural one, but also is 
closer to the way our storage (RADOS) handles data. To understand the 
importance of it, consider the following:

Like bcache, our implementation must not only cache object requests fast but 
also try to coalesce them so that, when needed, they will be flushed to the 
slower medium in a more sequential fashion. The fact however that the VMs' 
volumes are partitioned into different objects, means that sequential data (in 
volume context) which reside in different objects will probably not be 
sequential in the storage backend too.

Thus, unlike bcache which expects that the backing device is also the physical 
device and coalesces date accordingly, our implementation is limited only in 
coalescing data in the object range (commonly 4MBs). If our implementation was 
caching in block or volume level, it would be unaware of that fact. 

Having decided that cached will cache objects, the next step is to decide
\begin{inparaenum}[i)]
\item on the index mechanism and
\item on what \textbf{exactly} would we index.
\end{inparaenum}

As for what we would index, it would be an overkill to further partition the 
objects and index regions within them. Moreover, this would make sense only if 
the objects where large (e.g. like volumes). So, our index mechanism should 
index object names solely. As for the index mechanism, we have chosen to use a 
very fast in-memory hash-table for this job. This covers the in-memory 
requirement we have set above. Also, this choice is one of the main reasons 
that our implementation is O(1).

Finally, another important decision was whether cached would be a 
multi-threaded peer. We have decided that we will implement it this way and 
then evaluate the performance of the implementation to find out if we are 
benefited by multi-threading or not.

Thus, cached must be able to work with multiple threads which will accept 
requests from cached's request queue and serve them concurrently with the other 
threads. Of course, multi-threading can be very tricky, especially when we are 
dealing with I/O requests and simultaneous accesses to the same object blocks.  
So, in order to achieve a balance between safety and speed, we use a
fine-grained locking scheme in critical sections that can be seen is discussed 
in detail in Section \ref{sec:xworkq-design}.

\section{Cached components}\label{sec:comp-design}

At this point, we must do an intermission before we show the design of cached.  
Specifically, we will show first the design of the cached's components, since 
many cached operations rely on them and the reader needs prior knowledge of 
them to grasp the cached design.

\subsection{Overview}

In this section, we will list the main components that cached relies on. Per 
Archipelago policy, most of these components have been written in the xtypes 
fashion (see Section \ref{arch-xtypes} about xtypes).  

The components of cached can be seen below:
 
% TODO: Link List to the definitions
\begin{itemize}
	\item xcache, an xtype that provides indexing support, amongst many 
		other things
	\item xworkq, an xtype that guarantees atomicity for execution of jobs 
		on the same object
	\item xwaitq, an xtype that allows conditional execution of jobs
\end{itemize}

and their design will be discussed in-depth in the following sections.

Also, we must note that the above components predate our cached implementation 
and are not a contribution of this thesis\footnote{xcache is an exception since 
	we have extended its functionalities for our purposes}. They are 
presented however in this thesis for clarity reasons. 

\subsection{The xcache xtype}\label{sec:xcache-design}

xcache is the most important component of cached. It is responsible for several 
key aspects of caching such as:

\begin{itemize}
	\item entry indexing,
	\item entry eviction,
	\item concurrency control and
	\item event hooks
\end{itemize}

Below, we can see a design graph of xcache:

\fixme add Figure here

% While explaining, point to the diagram objects, a, b, c etc.
\fixme add better design explanation \\
As we can see above, xcache utilizes two hash tables. One hash table is 
responsible for indexing entries (or more generally speaking "cache entries") 
that are active in cache.  The other hash table is responsible for indexing 
evicted cache entries that have pending jobs.  Again, more generally speaking, 
evicted cache entries are entries whose refcount has not dropped to zero yet.

On the following subsections, we present the features of xcache as well as 
their design.

\subsubsection{Entry Preallocation}\label{sec:xcache-entry-design}

Since xcache indexes a bounded number of entries, there is no need to allocate 
them on-the fly using malloc/free. Considering that we are caching at RAM level 
and not at SSD level, the system call overhead will have a considerable impact 
on performance. Thus, in our case we pre-allocate the necessary space in 
advance.

\subsubsection{Entry indexing}\label{sec:xcache-index-design}

The index mechanism that xcache uses is a hash table named xhash, also an 
xtype. The reason why a hash table is used as an index mechanism is because:

\begin{enumerate}
	\item Given that we index only a certain number of entries, we expect 
		the that the insert, lookup and delete operations are in 
		constant time (see below for an explanation why)
	\item Hash tables can preallocate the space needed whereas 
		tries/b-trees/bst will allocate nodes as new entries are 
		inserted. Again, the fact that we index a certain number of 
		entries means that we expect that we will have many evictions 
		and insertions.
	\item We don't need to do substring matches (advantage of tries)
	\item We don't need to traverse the entries sequentially (advantage of 
		B-trees and BSTs)
\end{enumerate}

The hash table that is used is heavily based on dictobject\cite{dictobject},
the Python dictionary implementation in C. Distobject has been created to 
minimize the collisions and the hops (\fixme Explain that better). Its only 
drawback is that it needs to resize when the table's entry history has reached 
the 2/3 of its capacity.

\subsection{Entry eviction}

The decision to have xcache index a bounded number of entries means that when 
it reaches its maximum capacity and is requested to index a new entry, it has 
to resort to the eviction of a previously cached entry.

xcache handles evictions in an interesting way. More specifically, evictions 
occur implicitly and not explicitly, meaning that the user (peer) doesn't have 
to evict entries manually. For example, when a user tries to insert a new entry 
to an already full cache, the insertion will succeed and the user will not be 
prompted to evict an entry manually. Moreover, the user will be notified via 
specific event hook that is triggered upon eviction.

The scheme of implicit evictions and later on notification of the user has the 
advantage that lookups, inserts and evictions can occur atomically by xcache.  
This wouldn't be the case if the user was responsible for the evictions.

As for the eviction strategy, we have utilized an LRU queue. Not only it's 
optimal (\fixme verify it) for our purposes, but we have also mitigated the 
cost of keeping the last references for each entry by creating a simple LRU 
algorithm, which has O(1) complexity for all update actions. More about the 
implementation of the LRU algorithm can be found in Section 
\ref{xcache-evict-imp}.

\subsection{Concurrency control}

The concept of concurrency control has been discussed in chapter ?. The goal of 
xcache is to handle safely - and preferably fast - simultaneous accesses to 
shared memory.

In order to do so, we must first identify which are the critical sections of 
xcache, that is the sections where a thread can modify a shared structure. These 
sections are the following

\begin{itemize}
	\item
		All xhash operations: Two of the three xhash operations (inserts 
		and removals) can modify the hash table (e.g. they can resize 
		it, add more entries or delete existing ones).  This means that 
		the third one (lookups) must not run concurrently with the 
		other.
	\item
		Cache node claiming: Before an entry is inserted, it must	
		acquire one of the pre-allocated nodes and we must ensure that 
		this can happen from all threads.
	\item
		Entry migration: An entry can migrate from one hash table to 
		the other e.g. on cache eviction. This migration involves a 
		series of xhash operations; removal from one hash table and 
		subsequent insertion to the other. This a scenario that must be 
		handled properly.
	\item
		Reference counting: Every entry must have a reference counter.  
		Reference counters provide a simple way to determine when an 
		entry can be safely removed. You can see more about reference 
		counting in chapter ?
	\item
		LRU updates: Most actions that involve cache entries must 
		subsequently update the LRU queue. Being a doubly linked list, 
		if two threads update the LRU simultaneously, we can lead to 
		segfaults.
\end{itemize}

Let's see what guarantees we provide for each of the above scenarios:

\begin{itemize}
	\item
		xhash operations: We provide a lock for each hash table
	\item
		Cache node claiming: The free node queue is protected by a fast 
		lock
	\item
		Entry migration: We always take fist the lock for entries and 
		then for rm\_entries
	\item
		Reference counting: Another important guarantee is the 
		reference counting of entrys. xcache uses atomic gets and puts 
		to update the reference count of an entry.
	\item
		LRU updates: Since all LRU operations take place for entries in 
		"entries" hash table and LRU updates are blazing fast we can 
		secure our LRU with the cache->lock.
\end{itemize}

% TODO: Yada yada about refcount

\subsection{Re-insertion}

We have previously mentioned that in xcache, there can be data migration 
between hash tables. This is easy to see why in case of evictions: an entry 
that previously was in "entries" must now be migrated to "rm\_entries" until 
its reference count falls to zero and can be freed.

However, what happens when xcache receives a request for an evicted entry? 

%Explain, explain, explain the re-insertion
there is a concept called "re-insertion".  In order for an entry to
be re-inserted to the primary hash table (which will be called "entries" from
now on) it must first reside in the hash table that indexes the evicted cache
entries (which will be called "rm\_entries" from now on). As mentioned above,
an entry that is in rm\_entries has probably pending jobs that delay its
removal.

So, what happens if a lookup arrives for that entry while on this stage? In
this case, we re-insert it to entries and increase its refcount by 2, since
there is one reference by the hash table and one reference by the one who
requested the lookup.

\subsection{Event hooks}

We have mentioned that xcache is not a standalone program but aims to provide 
core caching functionalities for other peers to use. Moreover, an important 
design feature is to not bother the top peer with the xcache internals, such as 
refcounting or which hash table its resides.

There is an issue that arise with this bla.

Different peers have different things to cache. For example, cached needs to 
cache objects but xcache was originally created for another peer that needed to 
cache file descriptors. So, xcache may pose no restriction to what is cached but 
that also means that it has no control over what happens to the peer's contents 
on insertion, eviction, removal etc. This is fair, but shouldn't the peer be 
alarmed for events such as eviction of an entry?

To this end, we have crafted event hooks in xcache onto which the peer can plug 
its own function and take brief control of the event. For example, when cached 
is on writeback mode, it plugs a special function on the entry eviction hook to 
check if this entry is dirty.

\subsection{xcache flow}

Below we will see three important scenarios

\textbf{Insertion}

Figure

\textbf{Lookup}

Figure

\textbf{Put}

Figure

\section{The xworkq xtype}\label{sec:xworkq-design}

The xworkq xtype is a useful abstraction for concurrency control. It's purpose 
is to enqueue "jobs" and ensure that only one thread will execute them at a 
time. There is no distinction as to which thread this will be, as well as no 
execution condition. The winner is simply the one that acquires first the lock.

This means that xworkq is generally used when multiple threads want atomic 
access to the same memory segment. xworkq is also generic by nature, since the 
"jobs" are simply a set of functions and their input data.

In cached context, every object has a workq. Whenever a new request is 
accepted/received for an object, it is enqueued in the workq and we are sure 
that only one thread at a time can have access to the object's data and 
metadata.

\begin{comment}
	This is wrong, xworkq understands only jobs.
on object level.  It is important to distinguish between cache level operations 
and object level operations. Cache level operations include insertions, lookups, 
removals, allocations and refcount handling. On object level, there is a 
different set of operations that must be synchronized across threads. Namely, we 
have bucket claiming, read/write operations and object flushes.

The above distinction makes it easy to see that provided that operations on 
object level need not worry about interactions with other objects. Each object 
is "sandboxed", so to speak.
\end{comment}

Let's see the design of the xworkq xtype. 

% Design of xworkq

It consists of a queue where jobs (e.g.  read from block, write to block) are 
enqueued. The thread that enqueues a job can attempt to execute it to, by 
acquiring a lock for the workq. If the lock is free, the thread will be able to 
execute the enqueued job. Also, other threads can enqueue their jobs, so the 
thread that has the lock can do those too. There is an xworkq for every object.


\section{The xwaitq xtype}\label{sec:xwaitq-design}

The xwaitq xtype bears some similarities to the xworkq xtype. Like xworkq, it is 
also an abstraction where "jobs" are enqueued and dequeued later on to be 
executed. Unlike xworkq though, in xwaitq the "jobs" that are being enqueued are 
considered to be thread-safe and can be executed concurrently by any thread.  
Moreover, the execution scheme is different. Specifically, jobs are executed 
only when a specific condition is true. This way, xwaitq provides an abstraction 
to...

In cached context...

Let's see the design of the xwaitq xtype. 

% Design of xwaitq

\section{Cached internals}\label{sec:cached-design}

The components that have been discussed in the previous sections provide core 
functionalities for cached. Also, besides bridging these components, cached also 
has some internal bla bla that are crucial for cached's operation.

Namely, these are:

\begin{enumerate}
	\item Bucket pool
	\item Per-object request queues
	\item Bucket/Object/cio states
	\item Write policy
\end{enumerate}

\subsection{Buckets}

% Explain what buckets are:
% a) That they are bounded
% b) That they have states
% c) That they are preallocated and every thread enqueues and dequeues an index 
% to them

\subsection{Object states}


\section{Bucket pool}

There is however a problem when operating solely on object level. Objects have 
typically 4MB of size. What would happen if a user requested e.g. a 16KB chunk 
of an object?

n this case, we would need to read and cache the whole object just to reply to 
the user's request. If the user then requests a chunk from another object, we 
would have to cache that object too and in the end, we would thrash our 
cache\footnote{cache thrashing occurs when we aggressively cache data that is 
	only used once and effectively leads to a snowball of evictions}.

The solution to this is to further divide objects to the next and final logical 
entity, \emph{buckets} (typically 4KB of size). Each bucket consists of its data 
and metadata and cannot be half-empty, or half-allocated. This way, we can also 
know which parts of the cached object are actually written, or are in the 
process of being read etc.

The buckets are pre-allocated, which means two things:

\begin{enumerate}
	\item We don't need to care about memory fragmentation and system call 
		overhead
	\item We cannot index single buckets. <FILLME>
\end{enumerate}

\subsection{Per-object peer requests}

Reads and writes to objects are practically read/write request from other
peers, for which a peer request has been allocated. There are cases though
when an object has to allocate its own peer request e.g. due to a flushing of
its dirty buckets. Since this must be fast, there are pre-allocated requests
hard-coded in the struct of each object which can be used in such cases.

\subsection{Bucket/Object states}

In order to know on how to operate on an object/bucket/cio, we must have some 
sort of book-keeping. The book-keeping we use is to check the state of the 
above. (arggh, silly)

\subsection{Write policy}\label{sec:wcp-design}

The user must define beforehand what is the write policy of cache. There are
two options: write-through and write-back. These policies aren't new and have 
been discussed extensively in chapter ?, but let's see what these policies 
translate to in cached context.

\begin{itemize}
	\item
		In \textbf{write-back} mode, cached caches writes, immediately 
		serves the request back and marks the data as dirty. When a read 
		arrives, it either serves the request with the dirty data 
		(read-hit) or forwards the request to the storage peer and 
		caches the answer (read-miss).

		This policy is used when we want to improve read and write speed 
		and can sacrifice data safety.
	\item
		In \textbf{write-through} mode, cached forwards writes to 
		blocker, servers the request when blocker replies, caches the 
		data and marks them as valid.  When a read arrives, it either 
		serves the request with the valid data (read-hit) or forwards 
		the request to the storage peer and caches the answer 
		(read-miss).

		This policy is used when we want to improve read speed and want 
		to make sure that no data will be lost.
\end{itemize}	

These policies are specified once during cached's deployment and cannot be 
switched on/off later on.

\section{Cached Operation}\label{sec:op-design}

\subsection{Polling for new requests}

We have explained in Section ? that a peer can send an I/O request to another 
peer by submitting to its port. However, what is are


\begin{comment}
Let's attempt to make the above a bit clearer. When cached receives a request, 
it first checks the request target (i.e. the object name and then calculates 
which bucket objects are within the request's range. It is easy to see that this 
is a 1:1 mapping to the object's data.
\end{comment}

\subsection{Write-through mode}

Here we will see how cached operates in write-through mode.

\subsubsection{Write}

This is the flow for the write path:

\subsubsection{Read}

This is the flow for the read path:

\subsection{Write-back mode}

Here we will see how cached operates in write-back mode.

\subsubsection{Write}

This is the flow for the write path:

\subsubsection{Read}

This is the flow for the read path:

