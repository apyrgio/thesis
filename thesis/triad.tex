\chapter{Scalability, Tiering and Caching}\label{ch:triad}

In this chapter, we will discuss the challenges of today's data storage and 
will attempt to explain the role of scalability, tiering and caching in 
mitigating costs and increasing performance.  Moreover, we present the current 
solutions for boosting performance and we evaluate if they can be used in 
conjunction with Archipelago.

The structure of this chapter is the following. Sections 
\ref{sec:scalability-triad}, \ref{sec:tiering-triad} and 
\ref{sec:caching-triad} explain what scalability, tiering and caching mean 
respectively. Section \ref{sec:real-life-triad} attempts to exhibit the need 
for these techniques by providing a typical real-life scenario in which they 
can be used.  Finally, Section \ref{sec:solutions-triad} lists and evaluates 
some of the 3rd party, open-source solutions that employ the aforementioned 
techniques.

\section{What is scalability?}\label{sec:scalability-triad}

Scalability, in storage service context, is the ability of the service to 
achieve two specific things:
\begin{enumerate}
	\item accommodate the growth of load in a manner that does not impact 
		the quality of the service and
	\item utilize the addition of new resources to their full extend, in 
		order to improve its performance.
\end{enumerate}

There are two methods of scaling, horizontal (scaling out) and vertical 
(scaling up), which are explained below:

\begin{itemize}
	\item \textit{Horizontal scaling} applies to distributed services. It 
		relies on the principle that adding more nodes to a system will 
		mitigate the high load of the other nodes.
	\item \textit{Vertical scaling} applies to all types of systems and 
		refers to the addition of more resources such as better 
		hardware, more RAM etc. to a node of the system.
\end{itemize}

The rule of thumb about these methods is that scaling up is the simpler 
solution for a service, albeit its performance cannot be increased much due to 
hardware limitations.  On the other hand, scaling out is far more complex and 
requires a robust method of managing many nodes as well as their failures, but 
it may have lower costs (if nodes are made of commodity hardware), and has 
theoretically no limitations in performance gain (especially in share-nothing 
architectures).

\section{What is tiering?}\label{sec:tiering-triad}

Tiering is the organization of different storage types in levels (or tiers) 
depending on their performance.  These storage types usually differ in one of 
the following attributes: capacity, price or performance.  Tiers such as SSD 
arrays or caches are necessary in most medium or larger deployments, in order 
to bridge the performance gap between RAM and magnetic disks, which can be seen 
in Table \ref{tab:gap}. To understand the need for tiering, consider the fact 
that when data do not reside in RAM and SSDs are not used, the performance 
penalty is x10,000 times the access time of RAM.

\begin{table}
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Medium & Access time (ns) \\ \hline \hline
		CPU registers and cache & < 10 \\ \hline
		RAM & < 10\textsuperscript{2}  \\ \hline
		SSD & < 10\textsuperscript{5} \\ \hline
		Hard Disk & < 10\textsuperscript{7} \\ \hline
	\end{tabular}
	\caption{Access times of storage media}
	\label{tab:gap}
\end{table}

Tiered storage is analogous to the computer architecture model of memory 
hierarchy, which can be seen in Figure \ref{fig:mem-hier.pdf}. Tiered storage 
is based on the same principles as memory hierarchy, in the sense that its 
objective is to keep "hot" data, i.e. data that are requested frequently, in 
the higher tiers.

\diagram{Computer Memory Hierarchy}{mem-hier.pdf}

\section{What is caching?}\label{sec:caching-triad}

In the context of I/O requests, caching is the addition of a fast medium in a 
data path, whose purpose is to transparently store the data that are intended 
for the slower medium of this data path. The benefits from caching is that 
later accesses to the same data will be faster than fetching them from the 
slower medium.

Caching is extensively used in computer architecture, as is evident from Figure 
\ref{fig:mem-hier.pdf}. Besides its application in a single computer, it is a 
widely employed concept in multi-node storage services where fast media are 
used to cache slower ones or where dedicated servers are used to cache data, 
like in memcached's case (see more in Section \ref{sec:memcached-triad}).

\subsection{Write policies}\label{sec:wp-triad}

Cache write policies dictate the behavior of the cache when it receives a write 
request. There are two main write policies:

\begin{description}
	\item[Write-through] \hfill \\
		The write is acknowledged only when the data are written both 
		in the cache and on the slower medium.
	\item[Write-back] \hfill \\
		The write is acknowledged when data are written in cache. The 
		slower medium is later updated with the correct data, when they 
		need to be flushed or replaced by new ones.
\end{description}

Moreover, there are also policies that affect the behavior of a cache in write 
miss scenarios:

\begin{description}
	\item[Write allocate] \hfill \\
		The cache loads the corresponding data block from the slower 
		medium and the data are written to it.
	\item[No-write allocate] \hfill \\
		The write request bypasses the cache and writes directly to the 
		slower medium.
\end{description}

Although the above policies can be combined as we wish, there are two main 
combinations that make more sense:
\begin{inparaenum}[i)]
\item write-back with write-allocate, so that writes are cached to benefit from 
	subsequent reads and
\item write-through with no-write allocate, because the extra load of write 
	allocate will not benefit us, since we write directly to the slower 
	medium.
\end{inparaenum}

\subsection{Caching limitations}

In our introduction we have explained that fast media like RAM and SSD drives 
cost more dollars/GB than slower media, such as hard disks.  For this reason, 
caches always have smaller capacity than the media they cache.  So, when a 
cache reaches its maximum capacity, it must evict one of its entries.  However, 
which entry is the one that must be evicted?

This is a very old and well documented problem that still troubles the research 
community. It was first faced when creating hardware caches (the L1, L2 CPU 
caches we are familiar with). In 1966, Lazlo Belady proved that the best 
strategy is to evict the entry that is going to be used more later on in the 
future\cite{Belady}.  However, the clairvoyance needed for this strategy is a 
little difficult to implement, so we resort to one of the following, well-known 
strategies:

% Mention ehcache approach: 
% http://ehcache.org/documentation/apis/cache-eviction-algorithms
%
% Also look at the following papers
%L. Belady, “A Study of Replacement Algorithms for a
%Virtual-Storage Computer,” IBM Systems Journal, vol.5,
%no.2, pp.78-101, 1966.
\begin{itemize}
	\item \textbf{Random:} Evict a randomly chosen entry. This strategy, 
		although it seems simplistic at first, is sometimes chosen due 
		to the ease and speed of its implementation. It is preferred in 
		random workloads where freeing quickly space for an entry is 
		more important than the entry that will be evicted.
	\item \textbf{FIFO (First-In-First-Out):} Evict the entry that was 
		inserted first. This is also a very simplistic approach as well 
		as easy and fast.  Interestingly, although it would seem to 
		produce better results than Random eviction, it is rarely used 
		since it assumes that cache entries are used only once, which 
		is not common in real-life situations.
	\item \textbf{LRU (Least-Recently-Used)}
		Evict the entry that has been less recently used. This is one 
		of the most common eviction strategies, however, it is not 
		simple to implement since the application needs a way to track 
		and index fast the last references to all entries.
	\item \textbf{LFU (Least-Frequently-Used)}
		Evict the entry that has been less frequently used. There have 
		been many derivatives of this algorithm that also use parts of 
		the LRU algorithm which have promising results, but this 
		algorithm itself is not commonly used. The reason is because it 
		overestimates the frequency of references to an item and it 
		performs poorly in cases when an item is frequently accessed 
		and then is not used at all.
\end{itemize}

%Also, check out this paper:
%An optimality proof of the LRU-K page replacement algorithm
%that proves that no algorithm that keeps track of the K most recent references 
%for a page can be more optimal than LRU.

The fact that write requests that have spawned the evictions cannot continue 
until the dirty data have been safely written to the storage backend, means 
that when the cache has no space left, its speed deteriorates to the speed of 
the slower medium.

The above observation indicates that the challenge for caching algorithms is 
how friendly their flushes are to the underlying storage. To elaborate on that 
a bit, hard disks excel on sequential payloads, so if a cache could flush in a 
more sequential way to the disk, it would boost its performance in these 
scenarios.

\section{Real-life scenario}\label{sec:real-life-triad}

Usually, when a small deployment makes its first steps, it doesn't use SSDs due 
to management/hardware costs and since it is an investment that is actually 
needed when the deployment has proved that it will attract traffic. Instead, 
the most common setup is an array of RAID-protected commodity hard disks or 
fast SAS drives.

When the storage demands start to increase and more users use the service, the 
OS caching system of the storage nodes will soon prove ineffective and the 
randomness in the requested data will skyrocket the access times.

At this point, the administrators must take one (or more, if the budget allows 
it) of the following decisions:

\begin{enumerate}
	\item Add more storage nodes in order to lower the load on the existing 
		ones (horizontal scaling).
	\item Buy battery-backed array controllers with volatile memory on-board, 
		to improve access times (vertical scaling).
	\item Put time-critical storage operations, such as journaling, in higher 
		tiers (tiering)
	\item Add RAM or SSD caches in write-back mode that will ACK the 
		requests before they reach the slower media (caching).
\end{enumerate}

The employment of one of the aforementioned techniques (scaling, tiering, 
caching) is of paramount importance for the future of the service.

\section{Current Solutions}\label{sec:solutions-triad}

For the thesis purpose, we have evaluated a numerous of caching solutions. The 
results of our evaluations are presented below:

\subsection{Bcache}

\subsubsection{Overview}

Bcache has been designed by Kent Overstreet since 2011 and has been included in 
the Linux kernel (3.10) since the May of 2013.

Bcache allows one to use one or more fast media as a cache for slower ones.  
Typically, the slow medium is a RAID array of hard disks and the fast medium 
are SSD drives. Bcache has been specifically built for SSDs and has the 
following characteristics:

\begin{enumerate}
	\item The data are written sequentially and in erase block size 
		granularity, in order to avoid the costly 
		read-erase-modify-write cycle.
	\item It takes special care to mitigate wear-leveling by touching 
		equally all SSD cells
	\item It honors TRIM requests and uses them as hints for its garbage 
		collection.
\end{enumerate}

\subsubsection{Installation and usage}

Bcache is a kernel driver that needs a patched kernel and intrusive changes to 
the backing device.

On a nutshell, bcache edits the superblock of both the cache and backing 
devices in order to use them, rendering existing data unreadable.  Then, it 
exposes to the user a virtual block device, which can be formatted to any 
file-system. This virtual block device is the entry point to the bcache code.  
Then, the caching device is attached to the backing device and at this point 
the virtual block device is ready to accept requests.

At any point, the bcache parameters can be further tuned via the sysfs 
interface.  

\subsubsection{Features and limitations}

The most striking bcache feature is that it uses a custom built B+tree as an 
index, which has the added benefit that dirty data can be coalesced and flushed 
sequentially to the slower spinning medium. This provides a considerable 
performance speed-up for hard disks.

Some other noteworthy features of bcache are the following:

\begin{enumerate}
	\item It can be used to cache more than one devices.
	\item It can operate in three modes, write-through, write-back and 
		write-around, which can be switched on/off arbitrarily during normal 
		usage or when the fast medium is congested.
	\item It utilizes a journal log of outstanding writes so that the data are 
		safe, even when an unclean shutdown occurs.
	\item It can bypass sequential IO and send it directly to the backing 
		device, since this workload is tailored for spinning disks.
\end{enumerate}

\subsection{Flashcache}

\subsubsection{Overview}

Flashcache has been designed by Facebook and has been open-sourced in the April 
of 2010. It is a kernel module that is officially supported for kernels between  
2.6.18 and 2.6.38 and is based on the Linux Device Mapper, which is used to map 
a block device onto another.

\subsubsection{Installation and Usage}

Flashcache's installation is not system-intrusive, in the sense that it needs 
only to compile the module against the kernel's source, modprobe it and then 
map the cache device upon the backing device, without making any changes to the 
latter.

\subsubsection{Features and limitations}

Flashcache uses a set-associative hash table for indexing. It has three modes 
of operation, write-through, write-back and write-around, and some basic 
performance tuning options such eviction strategies and dirty data threshold.  
Also, it has the following limitations:

\begin{enumerate}
	\item It does not provide atomic write operations, which can lead to 
		page-tearing.
	\item It does not support the TRΙΜ command.
\end{enumerate}

\subsection{EnhanceIO}

\subsubsection{Overview}

EnhanceIO has been developed by STEC Corp. and has been open-sourced in the 
December of 2012. It is a fork of Flashcache which does not use the Linux 
Device Mapper and has some major re-writes in parts of the code such as the 
write-back caching policy.

\subsubsection{Installation and Usage}

The installation method is similar to the Flashcache's method. The source code 
is compiled again the kernel's source, which produces a module that can be 
modprobed. After that, the utilities provided can be used to map the cache 
device on the backing device.

\subsubsection{Features and Limitations}

Similarly to Flashcache, EnhanceIO uses a set-associative hash table for 
indexing. It also has improvements upon the original Flashcache implementation 
in the following areas:

\begin{enumerate}
	\item The page-tearing problems have been solved.
	\item Dirty data flushing using background threads.
\end{enumerate}

\subsection{Memcached}\label{sec:memcached-triad}

\subsubsection{Overview}

Memcached is a distributed memory caching system that is being widely employed 
by large sites such as Youtube, Facebook, Twitter, Wikipedia. It has been 
created in 2003 by Brad Fitzpatrick while working in LiveJournal and to date 
there have been numerous forks of the code, most notably including Twitter's 
twemcache and fatcache, Facebook's implementation etc.

When memcached came into existence, many social sites like LiveJournal were 
experiencing the following problem:

User pages would often have queries that would be executed hundreds of times 
per second or would span across the database due to a big SELECT, but whose 
nature would be less critical or would not change rapidly. Queries such as "Who 
are my friends and who of them are online?", "What are the latest news in my 
feed?" etc. which could be easily cached, would instead cripple the database by 
adding a lot of load to it.

To tackle this problem, memcached can be used to utilize the unused RAM of the 
site's servers to cache these kinds of queries. Ten years later, memcached has 
become the defacto scale-out solution, and has use cases such as Facebook's, 
whose 800 dedicated memcached servers can serve billions of request per second 
for trillions of stored items\cite{facebook-memcached}.

\subsubsection{Installation and usage}

Memcached adheres to the client server model, with N clients connecting to M 
servers. Memcached, which is a user space daemon, runs on every server and
listens for requests typically on port 11211. The installation is very easy 
since there are packages for most known distros. Once memcached has been 
installed, the administration needs to specify only the port and several 
performance options such as cache size and number of threads.

The clients on the other hand communicate with the memcached servers using 
native libraries. There are libraries that are written for most programming 
languages such as C, PHP, Python, Haskell etc. The clients can then specify 
which queries - or keys in general - want to be cached and the actual caching 
is done in runtime.

\subsubsection{Features and limitations}

Architecturally, memcached tries to do everything in O(1) time. Each memcached 
server consists of a hash table that indexes the keys and their data. Since the 
data size can vary from 1 byte to 1MB, memcached uses SLAB allocation in order 
to prevent memory fragmentation. In SLAB allocation, memory is reserved in 
fixed-sized pages, e.g. 1MB for memcached, which are divided in blocks of equal 
size. Then, items are stored to the SLAB whose block size is closer to their 
size.

Moreover, each memcached must be able to handle tens of thousands connections 
from clients, so it relies in libevent to do the asynchronous polling.

What's more interesting about memcached is that its main strength is actually 
its biggest limitation. Memcached has no persistence and in fact, data can be 
evicted in numerous ways:

\begin{enumerate}
	\item Cached data have an expiration time after which they are 
		garbage-collected.
	\item Data can be evicted before their expiration time, if the cache 
		has become full.
	\item When memcached is out of SLAB pages, it must evict one in order 
		to regain space. This leads to the eviction of more than one 
		keys.
	\item When adding or removing memcached servers, the Ketama algorithm 
		that maps keys to servers will assign a portion of the existing 
		keys to other servers. This change in mapping, however, will 
		not actually move the existing keys to these servers and the 
		data are essentially invalidated.
\end{enumerate}

To sum up, the lack of persistence means that memcached will never hit the disk 
bottleneck due to flushes and will always be very fast, as long as the cache 
hit rate is high. On the other hand, its unreliable nature means that it is not 
a general purpose software and only specific workloads will be benefited from 
it.

\subsection{Couchbase Server}

\subsubsection{Overview}

Couchbase server, a NoSQL database which has been under active development by 
Couchbase Inc.  since the January of 2012, is actually the product of the merge 
of two independent projects, CouchDB and Memebase, with CouchDB continuing as 
an Apache funded program.  Couchbase aims to combine the scalability of 
memcached with the persistence of a database such as CouchDB. 

\subsubsection{Installation and usage}

Couchbase provides two versions, a community edition, that lacks the latest bug 
fixes, and an enterprise edition. The community edition has an open-source 
license and can be installed easily in all major distributions from the 
official packages.

Once Couchbase Server has been installed, it can be configured through a 
dedicated web console or the command-line or the REST API. Its configuration 
has to do with the amount of RAM it will use and most importantly the cluster 
that it will join. Clusters are a deviation from the classic memcached 
architecture. They are logical groups of servers (or nodes) that are used for 
replication and failover reasons.

Like memcached, the communication with the servers is done through client 
libraries. These libraries are written for many different programming languages 
such as C, Python, Java, PHP, Ruby etc.

\subsubsection{Features and Limitations}

Couchbase Server adds the following important features to memcached feature 
list:

\begin{enumerate}
	\item It can provide persistence for the data.
	\item It uses data replication, which is one of the persistence guarantees.  
	\item It re-balances the data on resizes, so that they are evenly 
		distributed across the database.
\end{enumerate}

\subsection{Honorary mentions}

\subsubsection{Repcached}

Repcached is a memcached 1.2 fork that aims to provide asynchronous data 
replication. It didn't catch up however for the following reasons:

\begin{enumerate}
	\item The added data replica merely slims the margins of losing the data 
		but not erases them.
	\item It is based in memcached 1.2, which has been released four years ago.  
		Since then, there have been numerous performance improvements.
	\item The synchronization cost of replication was high.
\end{enumerate}

\subsubsection{RAMCloud}

RAMCloud\cite{ramcloud} is a project that is being directed by John Ousterhout 
at Stanford University. RAMCloud is not a caching system, but a distributed 
storage system that uses primarily DRAM, whereas hard disks are used only for 
crash recovery scenarios\cite{recovery-ramcloud}.

RAMCloud aims to be persistent, meaning that writes to a server are 
acknowledged only when the data are replicated to at least three other servers 
and are in the process to be written asynchronously to disk. Moreover, since 
DRAM has faster access times than network, RAMCloud requires fast Infiniband 
connections between nodes for lower access times as well as for quick data 
migration in recovery scenarios. Finally, in order to retain data even after a 
power loss, RAMCloud needs either battery-backed servers or DIMM modules with 
integrated super-capacitors.

Although RAMCloud is an aspiring project, we have chosen not to investigate it 
further for two reasons:

\begin{enumerate}
	\item it requires non-commodity hardware, which is the opposite of our 
		goal.
	\item it is not production-ready yet.
\end{enumerate}

\subsubsection{Page-cache}

Theoretically, our search for a production-ready, fast cache that scales and 
can be used in conjunction with Archipelago can stop, if we allow the VM's 
hypervisor to use Linux's page cache. In fact, we have measured its performance 
(see Figures \ref{fig:bw-vm.pdf} and \ref{fig:lat-vm.pdf}) and we can expect 
approximately a 13x performance increase for writes and a 6x performance 
increase for reads.

However, there are several reasons why we have not favored this approach. The 
main reason is that this type of caching, as we will explain in the following 
section, is orthogonal to the way Archipelago handles data. It is agnostic to 
the CoW policy of Archipelago and as a result, it caches duplicated blocks 
wasting valuable memory.

Moreover, we have little or no control over the Linux page cache. For example, 
we cannot enforce a limit to the cache size or apply a replication policy.

Finally, the page-cache is dedicated to a block device and is therefore not a
suitable solution, if Archipelago ever manages to operate completely in user 
space.

\subsection{Evaluation}

The above solutions fall into two broad categories; block store and key-value 
store. Both of these categories can be used in Archipelago, since there are 
peers that use block store semantics, e.g.  when xsegbd receives a request, and 
peers that use key-value store semantics, e.g.  when vlmc has translated a 
block request to object request.

We will start our evaluation with the block store techniques first. These 
methods have in common that they are kernel modules which cache requests that 
are targeted to a single (or more) slow block device. So, we can use them in 
this way:

\begin{enumerate}
	\item Add SSDs to the host machine where the VMs are running.
	\item Partition the SSDs so that there is one partition for each volume 
		that is running in this host.
	\item Install the kernel module and when a VM is created, run the necessary 
		commands to map a partition of the SSD to the virtual block device of 
		the VM.
	\item Use the block device that the kernel module exposes and pass it to 
		the hypervisor.
\end{enumerate}

The main issue with this approach (and host caching in general) is this: If the 
cloud software is distributed then, when a host crashes, the VMs can be 
restarted in another host. This is possible in deployments where the instances' 
attributes are known by the respective cloud management software and their data 
are stored in a distributed storage system. 

The issue arises when caching in write-back mode, where the VM's most recent 
data will be down with the host. In this case, the VM will not be able to start 
in another host or worse, it will be in inconsistent state with whatever 
implications this may have.

Even if we ignored the above, there are other issues too, such as:

\begin{enumerate}
	\item If a user process segfaults, it can be restarted promptly, 
		without interrupting the rest of the VMs. If however the kernel 
		panics
		\footnote{Kernel panic is raised by the Linux Kernel when it 
			detects that it has encountered a fatal error, e.g.  
			when it dereferences a wrong memory address},
		the host will typically hang.
	\item Caching at xsegbd level does not take advantage of the fact that 
		large parts of a VM's volume are shared between other volumes 
		due to Copy-on-Write. This means there will be lost space in 
		the SSD for data that are actually duplicate.
	\item Flashcache has atomicity issues, which we want to avoid.
	\item Bcache is for newer kernels and to date it still has some bugs.
	\item Having a fixed partition for each volume does not scale, since 
		for each VM with high activity, there can be 10 other stale VMs 
		that practically eat up cache space.
\end{enumerate}

We will continue with the second category, the key-value store solutions. The 
programs that fall in this category have two important advantages:

\begin{enumerate}
	\item They are distributed by nature and try to eliminate any SPOF
		\footnote{Single Point of Failure}, in the same way that RADOS does.
	\item They can utilize the extra RAM of a node, which is plenty in the 
		RADOS nodes.
\end{enumerate}

However, there are also some fundamental problems with them:

\begin{enumerate}
	\item Memcached has no concept of persistence. Not only that, it basically 
		relies on the fact that has no persistence that hacking our way through 
		that issue would create a different software.
	\item Couchbase Server has no way to use RADOS as its backing device and 
		has its own concept of replication.
\end{enumerate}

For the above reasons, we have decided to roll out our own implementation, 
which is presented in the following chapter.
