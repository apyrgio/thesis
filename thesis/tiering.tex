\chapter{The Holy Triad: Scalability, Tiering and Caching}\label{ch:tiering}

In this chapter we will discuss the challenges of today's data storage and will 
attempt to explain the role of scalability - a word that is being repeated like 
mantra in cloud circles - tiering and caching in mitigating costs and 
increasing performance.  Moreover, we present the current solutions for tiered 
storage and we evaluate if they can be used in conjunction with Archipelago.

\section{What is scalability?}

Scalability is a term that has different meaning for different contexts. For 
example, scalability in multi-threading context can mean that the program's 
performance increases linearly with the number of threads.

We approach this subject from the storage service's perspective, which is an 
integral part of a cloud service. In this context, scalability means the 
ability of the storage service to handle the increase of I/O requests in a way 
that each independent user does not experience any latency.

There are two methods of scaling, horizontal (scaling out) and vertical 
(scaling up).

The horizontal scaling method applies to distributed services.  It relies on 
the principle that adding more nodes to a system will mitigate the high load of 
the other nodes.

The vertical scaling method applies to all types of systems and refers to the 
addition of more resources such as better hardware, more RAM etc. to a node of 
the system.

The rule of thumb about these methods is that scaling up is the simpler way to 
go, albeit its performance cannot be increased much due to hardware 
limitations.  On the other hand, scaling out is far more complex and requires a 
robust method of managing many nodes as well as their failures, but it may have 
lower costs (if nodes are made of commodity hardware), and has theoretically no 
limitations in performance gain (especially in share-nothing architectures).

\section{What is tiering?}

Tiering is the organization of different storage types in levels (or tiers) 
depending on their performance.  These storage types usually differ in one of 
the following attributes: capacity, price or performance.  Tiers such as SSD 
arrays or caches are necessary in most medium or larger deployments, in order 
to bridge the performance gap between RAM and magnetic disks, which can be seen 
in Table \ref{tab:gap}. To understand the need for tiering, consider the fact 
that when data do not reside in RAM and SSDs are not used, the performance 
penalty is x10,000 times the access time of RAM.

\begin{table}
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Medium & Access time (ns) \\ \hline \hline
		CPU registers and cache & < 10 \\ \hline
		RAM & < 10\textsuperscript{2}  \\ \hline
		SSD & < 10\textsuperscript{5} \\ \hline
		Hard Disk & < 10\textsuperscript{7} \\ \hline
	\end{tabular}
	\caption{Access times of storage mediums}
	\label{tab:gap}
\end{table}

Tiered storage is analogous to the computer architecture model of memory 
hierarchy, which can be seen in Figure \ref{fig:mem-hier.pdf}. Tiered storage 
is based on the same principles as memory hierarchy, in the sense that its 
objective is to keep "hot" data, i.e. data that are requested frequently, in 
the higher tiers.

\diagram{Computer Memory Hierarchy}{mem-hier.pdf}

\section{What is caching?}

In the context of I/O requests, caching is the addition of a fast medium in a 
data path, whose purpose is to transparently store the data that are intended 
for the slower medium of this data path. The benefits from caching is that 
later accesses to the same data will be faster than fetching them from the 
slower medium.

Caching is extensively used in computer architecture, as is evident from Figure 
\ref{fig:mem-hier.pdf}. Besides the memory hierarchy model, it is a widely 
employed concept in storage services where fast mediums are used as journals 
for slower ones, essentially caching the data, or where dedicated servers are 
used to cache data, like in memcached's case (see more in Section 
\ref{sec:memcached-triad}).

\subsection{Caching policies}

Do we even care?

\begin{itemize}
	\item Write-through:
		This policy bla bla bla
	\item Write-back:
		This policy blu blu blu
\end{itemize}

\subsection{Caching limitations}

Fast mediums like RAM and SSD drives are always more expensive than slower 
mediums, such as hard disks. Likewise, caches always have smaller capacity than 
the mediums they cache. So, when a cache reaches its maximum capacity, it must 
evict one of its entries. However, which entry is the one that must be evicted?

This is a very old and well documented problem that still troubles the research 
community. It was first faced when creating hardware caches (the L1, L2 CPU 
caches we are familiar with). In 1966, Lazlo Belady proved that the best 
strategy is to evict the entry that is going to be used more later on in the 
future\cite{Belady}.  However, the clairvoyance needed for this strategy is a 
little difficult to implement, so we resort to one of the following, well-known 
strategies:

% Mention ehcache approach: 
% http://ehcache.org/documentation/apis/cache-eviction-algorithms
%
% Also look at the following papers
%L. Belady, “A Study of Replacement Algorithms for a
%Virtual-Storage Computer,” IBM Systems Journal, vol.5,
%no.2, pp.78-101, 1966.
\begin{itemize}
	\item \textbf{Random:} Evict a randomly chosen entry. This strategy, 
		although it seems simplistic at first, is sometimes chosen due to the 
		ease and speed of its implementation. It is preferred in random 
		workloads where freeing quickly space for an entry is more important 
		than the entry that will be evicted.
	\item \textbf{FIFO (First-In-First-Out):} Evict the entry that was inserted 
		first. This is also a very simplistic approach as well as easy and 
		fast.  Interestingly, although it would seem to produce better results 
		than Random eviction, it is rarely used since it assumes that cache 
		entries are used only once, which is not common in real-life 
		situations.
	\item \textbf{LRU (Least-Recently-Used)}
		Evict the entry that has been less recently used. This is the most 
		common eviction strategy and there has been a paper that uses a 
		Bayesian statistic model to prove its optimality (\fixme add paper). It 
		is not however simple to implement since there needs to be some sort of 
		tracking of the time of reference for each entry.
	\item \textbf{LFU (Least-Frequently-Used)}
		Evict that entry that has been less frequently used. There have been 
		many derivatives of this algorithm that also use parts of the LRU 
		algorithm which have promising results, but this algorithm itself is
		not commonly used. The reason is because it overestimates the frequency 
		of references to an item and it performs poorly in cases when an item 
		is frequently accessed and then is not used at all.
\end{itemize}

%Also, check out this paper:
%An optimality proof of the LRU-K page replacement algorithm
%that proves that no algorithm that keeps track of the K most recent references 
%for a page can be more optimal than LRU.

\section{Common use-case}

Usually, when a small deployment makes its first steps, it doesn't use SSDs due 
to management/hardware costs and since its an investment that is actually 
needed when the deployment has proved that it will attract traffic. Instead, 
the most common setup is an array of RAID-protected commodity hard disks or 
fast SAS drives.

When the storage needs start to increase and more users use the service, the OS 
caching system of the storage nodes will soon prove ineffective and the 
randomness in the requested data will skyrocket the access times.

At this point, the administrators must take one (or more, if the budget allows 
it) of the following decisions:

\begin{enumerate}
	\item Add more storage nodes in order to lower the load on the existing 
		ones (horizontal scaling).
	\item Buy battery-backed array controllers with volatile memory on-board, 
		to improve access times (vertical scaling).
	\item Put time-critical storage operations, such as journaling, in higher 
		tiers (tiering)
	\item Add RAM or SSD caches in write-back mode that will ACK the requests 
		before they reach the slower mediums (caching).
\end{enumerate}

The employment of one of the aforementioned techniques (scaling, tiering, 
caching) is of paramount importance for the future of the service.

\section{Caching Solutions}

For the thesis purpose, we have evaluated a numerous of caching solutions. The 
results of our evaluations are presented below:

\subsection{Bcache}

\subsubsection{Overview}

Bcache has been designed by Kent Overstreet since 2011 and has been included in 
the Linux kernel (3.10) since the May of 2013.

Bcache allows one to use one or more fast mediums as a cache for slower ones.  
Typically, the slow medium is a RAID array of hard disks and the fast medium 
are SSD drives. Bcache has been specifically built for SSDs and has the 
following characteristics:

\begin{enumerate}
	\item The data are written sequentially and in erase block size 
		granularity, in order to avoid the costly read-erase-modify-write 
		cycle.
	\item It takes special care to mitigate wear-leveling by touching equally 
		all SSD cells
	\item It honors TRIM requests and uses them as hints for its garbage 
		collection.
\end{enumerate}

\subsubsection{Installation and usage}

Bcache is a kernel driver that needs a patched kernel and intrusive changes to 
the backing device, rendering previous data unreadable. On a nutshell, bcache 
edits the superblock of both the cache and backing devices in order to use 
them.  Then, it exposes to the users a virtual block device, which can be 
formatted to any filesystem.  This virtual device's request function actually 
points to the cache device and, when the cache device has reached its full 
utilization, it will flush dirty data to the backing device.

\subsubsection{Features and limitations}

The most striking bcache feature is that it uses a custom built B+tree as an 
index, which has the added benefit that dirty data can be coalesced and flushed 
sequentially to the slower spinning medium. This provides a considerable 
performance speed-up for hard disks since. Some of other noteworthy features of 
bcache are the following:

\begin{enumerate}
	\item It can be used to cache more than one devices
	\item It can operate in three modes, write-through, write-back and 
		write-around, which can be switched on/off arbitrarily during normal 
		usage or when the fast medium is congested.
	\item It utilizes a journal log of outstanding writes so that the data are 
		safe, even when an unclean shutdown occurs.
	\item It can bypass sequential IO and send it directly to the backing 
		device, since this workload is tailored for spinning disks.
\end{enumerate}

\subsection{Flashcache}

\subsubsection{Overview}

Flashcache has been designed by Facebook and has been open-sourced in the April 
of 2010. It is a kernel module that is officially supported for kernels between  
2.6.18 and 2.6.38 and is based on the Linux Device Mapper, which is used to map 
a block device onto another.

\subsubsection{Installation and Usage}

Flashcache's installation is not system-intrusive, in the sense that it needs 
only to compile the module against the kernel's source, modprobe it and then 
map the cache device upon the backing device, without making any changes to the 
latter.

\subsubsection{Features and limitations}

Flashcache uses a set-associative hash table for indexing. It has three modes 
of operation, write-through, write-back and write-around, and some basic 
performance tuning options such eviction strategies and dirty data threshold.  
Also, it has the following limitations:

\begin{enumerate}
	\item It does not provide atomic write operations, which can lead to 
		page-tearing.
	\item It does not support the TRΙΜ command.
\end{enumerate}

\subsection{EnhanceIO}

\subsubsection{Overview}

EnhanceIO has been developed by STEC Corp. and has been open-sourced in the 
December of 2012. It is a fork of Flashcache which does not use the Linux 
Device Mapper and has some major re-writes in parts of the code such as the 
write-back caching policy.

\subsubsection{Installation and Usage}

The installation method is similar to the Flashcache's method. The source code 
is compiled again the kernel's source, which produces a module that can be 
modprobed. After that, the utilities provided can be used to map the cache 
device on the backing device.

\subsubsection{Features and Limitations}

Similarly to Flashcache, EnhanceIO uses a set-associative hash table for 
indexing. It also has improvements upon the original Flashcache implementation 
in the following areas:

\begin{enumerate}
	\item The page-tearing problems have been solved.
	\item Dirty data flushing using background threads.
\end{enumerate}

\subsection{Memcached}\label{sec:memcached-triad}

\subsubsection{Overview}

Memcached is a distributed memory caching system that is being widely employed 
by large sites such as Youtube, Facebook, Twitter, Wikipedia. It has been 
created in 2003 by Brad Fitzpatrick while working in LiveJournal and to date 
there have been numerous forks of the code, most notably including Twitter's 
twemcache and fatcache, Facebook's implementation etc.

When memcached came into existence, many social sites like LiveJournal were 
experiencing the following problem:

User pages would often have queries that would be executed hundreds of times 
per second or would span across the database due to a big SELECT, but whose 
nature would be less critical or would not change rapidly. Queries such as "Who 
are my friends and who of them are online?", "What are the latest news in my 
feed?" etc. which could be easily cached, crippled instead the database by 
adding a lot of load to it.

To tackle this problem, administrator can instruct memcached to utilize the 
unused RAM of the site's servers to cache these kinds of queries. Ten years 
later, memcached has become the defacto scale-out solution, and has use cases 
such as Facebook's whose dedicated memcached servers serve the 95\% of their 
queries (\fixme add presentation)

\subsubsection{Installation and usage}

Memcached adheres to the client server model, with N clients connecting to M 
servers. Memcached, which is a user space daemon, runs on every server and
listens for requests typically on port 11211. The installation is very easy 
since there are packages for most known distros. Once memcached has been 
installed, the administration needs to specify only the port and several 
performance options such as cache size and number of threads.

The clients on the other hand communicate with the memcached servers using 
native libraries. There are libraries that are written for most programming 
languages such as C, PHP, Python, Haskell etc. The clients can then specify 
which queries - or keys in general - want to be cached and the actual caching 
is done in runtime.

\subsubsection{Features and limitations}

Architecturally, memcached tries to do everything in O(1) time. Each memcached 
server consists of a hash table that indexes the keys and their data. Since the 
data size can vary from 1 byte to 1MB, they are organized in SLABs in order to 
prevent memory fragmentation.  Moreover, each memcached must be able to handle 
tens of thousands connections from clients, so it relies in libevent to do the 
asynchronous polling.

What's more interesting about memcached is that its main strength is actually 
its biggest limitation. Memcached has no persistence and in fact, data can be 
evicted in numerous ways:

\begin{enumerate}
	\item Cached data have an expiration time after which they are 
		garbage-collected.
	\item Data can be evicted before their expiration time, if the cache has 
		become full.
	\item When memcached is out of SLAB pages, it must evict one in order to 
		regain space. This leads to the eviction of more than one keys.
	\item When adding or removing memcached servers, the Ketama algorithm that 
		maps keys to servers will assign a portion of the existing keys to 
		other servers. This change in mapping, however, will not actually move 
		the existing keys to these servers and the data are essentially 
		invalidated.
\end{enumerate}

To sum up, the lack of persistence means that memcached will never hit the disk 
bottleneck due to flushes and will always be very fast, as long as the cache 
hit rate is high. On the other hand, its unreliable nature means that it is not 
a general purpose software and only specific workloads will be benefited from 
it.

\subsection{Couchbase Server}

\subsubsection{Overview}

Couchbase server, a NoSQL database which has been under active development by 
Couchbase Inc.  since the January of 2012, is actually the product of the merge 
of two independent projects, CouchDB and Memebase, with CouchDB continuing as 
an Apache funded program.  Couchbase aims to combine the scalability of 
memcached with the persistence of a database such as CouchDB. 

\subsubsection{Installation and usage}

Couchbase provides two versions, a community edition, that lacks the latest bug 
fixes, and an enterprise edition. The community edition has an open-source 
license and can be installed easily in all major distributions from the 
official packages.

Once Couchbase Server has been installed, it can be configured through a 
dedicated web console or the command-line or the REST API. Its configuration 
has to do with the amount of RAM it will use and most importantly the cluster 
that it will join. Clusters are a deviation from the classic memcached 
architecture. They are logical groups of servers (or nodes) that are used for 
replication and failover reasons.

Like memcached, the communication with the servers is done through client 
libraries. These libraries are written for many different programming languages 
such as C, Python, Java, PHP, Ruby etc.

\subsubsection{Features and Limitations}

Couchbase Server adds the following important features to memcached feature 
list:

\begin{enumerate}
	\item It can provide persistence for the data.
	\item It uses data replication, which is one of the persistence guarantees.  
	\item It re-balances the data on resizes, so that they are evenly 
		distributed across the database.
\end{enumerate}

\subsection{Honorary mentions}

\subsubsection{Repcached}

Repcached is a memcached 1.2 fork that aims to provide asynchronous data 
replication. It didn't catch up however for the following reasons:

\begin{enumerate}
	\item The added data replica merely slims the margins of losing the data 
		but not erases them.
	\item It is based in memcached 1.2, which has been released four years ago.  
		Since then, there have been numerous performance improvements.
	\item The synchronization cost of replication was high.
\end{enumerate}

\subsubsection{Ramcloud}

RAMCloud (\fixme add paper) is a project that is being directed by John 
Ousterhout at Stanford University. RAMCloud is not a caching system per se, but 
a storage system that uses RAM as its primary storage and hard disks for 
failover scenarions. To be able to stay persistent even after a node crashes, 
it also maintains data replicas and requires Infiband connection between nodes 
to migrate the data quickly to other nodes.

RAMCloud states that it, or a system with similar design, will be the primary 
storage system of services such as AWS or Microsoft's Azure. For the time 
being, RAMCloud is under heavy development and its requirements are way of the 
budget of most deployment.


\subsection{Evaluation}

The above solutions fall into two broad categories; block store and key-value 
store. Both of these categories can be used in Archipelago, since there are 
peers that use block store semantics, e.g.  when xsegbd receives a request, and 
peers that use key-value store semantics, e.g.  when vlmc has translated a 
block request to object request.

We will start our evaluation with the block store methodologies first. These 
methods have in common that they are kernel modules which cache requests that 
are targeted to a single (or more) slow block device. So, we can use them in 
this way:

\begin{enumerate}
	\item Add SSDs to the host machine where the VMs are running.
	\item Partition the SSDs so that there is one partition for each volume 
		that is running in this host.
	\item Install the kernel module and when a VM is created, run the necessary 
		commands to map a partition of the SSD to the virtual block device of 
		the VM.
	\item Use the block device that the kernel module exposes and pass it to 
		the hypervisor.
\end{enumerate}

The main issue with this approach (and host caching in general) is this. When 
the host crashes, Ganeti attempts to restart the VM in another host. This is 
possible since the instance's attributes are known by Ganeti and its storage is 
either in DRBD slave or in RADOS. However, if we cache in writeback mode, the 
VM will not be able to start or worse, it will be in inconsistent state with 
whatever implications this may have.

Even if we ignored the above, there other issues too, such as:

\begin{enumerate}
	\item If a user process segfaults, it can be restarted promptly, without 
		interrupting the rest of the VMs. If however the kernel segfaults, the 
		host will go down.
	\item Caching at xsegbd level does not take advantage of the fact that 
		large parts of a VM's volume are shared between other volumes due to 
		Copy-on-Write. This means there will be lost space in the SSD for data 
		that are actually duplicate.
	\item Flashcache has page tearing issues, which we want to avoid.
	\item Bcache is for newer kernels and to date it still has some bugs.
	\item Having a fixed partition for each volume does not scale, since for 
		each VM with high activity, there can be 10 other stale VMs that 
		practically eat up cache space.
\end{enumerate}

We will continue with the second category, the key-value store solutions. The 
programs that fall in this category have two important advantages:

\begin{enumerate}
	\item They are distributed by nature and try to eliminate any SPOF
		\footnote{Single Point of Failure}, in the same way that RADOS does.
	\item They can utilize the extra RAM of a node, which is plenty in the 
		RADOS nodes.
\end{enumerate}

However, there are also some fundamental problems with them:

\begin{enumerate}
	\item Memcached has no concept of persistence. Not only that, it basically 
		relies on the fact that has no persistence that hacking our way through 
		that issue would create a different software.
	\item Couchbase Server has no way to use RADOS as its backing device and 
		has its own concept of replication.
\end{enumerate}

For this reason, we have decided to roll out our own implementation, which is 
presented in the following chapter.
