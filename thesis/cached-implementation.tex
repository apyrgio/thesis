\chapter{Implementation of cached}\label{ch:cached-implementation}

In the previous chapter, we have discussed in length the design of cached and 
its components. In this chapter, we will present how the above design has been
implemented. To aid us in this task, we will use code snippets from cached, and 
xcache and we will comment where necessary.

More specifically, Section \ref{sec:xcache-imp} presents the implementation 
information of xcache, the main cached component. Next, section 
\ref{sec:cached-imp} presents the implementation of cached, showcasing the 
structures and functions used.

\section{Implementation of xcache}\label{sec:xcache-imp}

For this section, we will attempt to provide a top-down view of the xcache 
implementation, starting from the functions that xcache exposes to peers and 
moving on to the more intrinsic details, such as the concurrency control.

\subsection{xcache initialization}

In order to use xcache, the peer must first initialize an xcache structure 
using xcache\_init, which can be seen in Listing \ref{lst:xcache-init.h}.

\ccode{\texttt{xcache\_init definition}}{xcache-init.h}

xcache\_init requests the following information from the peer:

\begin{description}
	\item[cache:] Simply, an allocated xcache struct
	\item[xcache\_size:] The number of objects xcache will index
	\item[ops:] The trigger functions for xcache's event hooks
	\item[flags:] Optional flags that tune the following two things:
		\begin{enumerate}
			\item The LRU algorithm. For the cached implementation, 
				we use the O(1) LRU, but xcache also allows to 
				use two more LRU algorithms, a binary heap 
				(O(log(N))) or an LRU array (O(N)).
			\item The usage of the hash table for evicted entries.  
				Although our cached implementation relies 
				heavily on it, this does not account for all 
				the other peers that use xcache and by default 
				is not used.
		\end{enumerate}
	\item[priv:] A pointer (void *) to a structure that will be returned 
		when an event hook is triggered. As most priv fields, it is 
		irrelevant to the xcache struct and relevant only to the top 
		caller. We initialize it with the peer struct.
\end{description}

The purpose of xcache\_init is to process the above data, populate the xcache 
struct and create the necessary entities, such as the hash table, the cache 
entries etc. On Listing \ref{lst:xcache-struct.h}, we can view the xcache 
struct and its respective fields.

\ccode{Main \xcache struct}{xcache-struct.h}

Each of the above \xcache struct fields is used to implement a design feature 
that has already been discussed in Section \ref{sec:xcache-design}. In the 
following sections, we will revisit these design features and present their 
implementation.

\subsection{Cache entry preallocation}

When xcache is initialized, it preallocates the necessary cache entries. The 
relevant fields of the xcache structure for this purpose can be seen in Listing 
\ref{lst:xcache-prealloc.h}.

\ccode{\xcache struct fields for preallocated entries}{xcache-prealloc.h}

The \textbf{size} field is the number of entries. The \textbf{free\_nodes} is a 
stack where all entry indexes are initially pushed and subsequently popped when 
a new entry is inserted. Finally, \textbf{nodes} is the space allocated for the 
cache entries and where the entry indexes point to.

Moreover, the definition of the \texttt{xcache entry} struct is shown in 
Listing \ref{lst:xcache-entry.h}.

\ccode{\texttt{xcache entry} struct}{xcache-entry.h}

We will comment briefly on the relevant cache entry fields for this section, 
which can be seen in Listing \ref{lst:xcache-entry-prealloc.h}. The rest of the 
fields will be discussed in the following sections.

\ccode{\texttt{xcache entry} fields, relevant for 
	preallocation}{xcache-entry-prealloc.h}

The description of the fields follows:

\begin{description}
	\item[ref] The reference count of the entry, initially set to zero.
	\item[state] The state of the entry. It can either be ACTIVE or EVICTED and 
		is initially set to the first.
	\item[name] The name of the entry. Since we cannot know its length 
		beforehand, we allocate as much space as possible by our segment, 
		typically 256 characters. During initialization, the entry name is 
		cleared out of junk values.
	\item[h] The entry's index.
	\item[priv] The private contents of the cache entry. On initialization, the 
		cache node creation hook is triggered and cached initializes the 
		private contents of cache entry with its data (more on the Section ?)
\end{description}

\begin{comment}
	The rest willwhaLet's start by listing what \texttt{xcache entry} 
	consists of.  First of all, it must have a name.  Since we preallocate 
	the entries and cannot know in runtime their length, we must allocate 
	as much space as possible. The \texttt{char name[XSEG\_MAX\_TARGETLEN + 
		1]} field, which is 256 characters long, is long enough to hold the 
	target's name.  Also, as we have mentioned in Section 
	\ref{sec:entry-prealloc-design}, xcache must be agnostic of the cache 
	contents.  To this end, we use the generic \texttt{void *priv} field as 
	a pointer to the actual entry content.  The rest of the fields will be 
	explained in the following chapters.

	Let's continue now with the fields of Listing \ref{lst:xcache-prealloc.h}.  
	Since we preallocate the entries using \texttt{malloc}, they take up a 
	contiguous space in memory.  The start of this space is the where the 
	\texttt{*nodes} field points to. The \texttt{free\_nodes} field works similarly 
	to the free\_entries field in Section \ref{sec:get-req-archip} i.e. it is a 
	stack where indexes to unused nodes are pushed. These indexes will be seen in 
	various code excerpts in this chapter and have a specific name, 
	\texttt{xcache\_handler}\footnote{\#define xcache\_handler uint64\_t}.
\end{comment}

\subsection{Cache entry initialization}

Before a peer can index a new entry, it must first allocate it from the cache 
entry pool and then initialize it. xcache has a special function for this 
purpose which can be seen in Listing \ref{lst:xcache-alloc-init.c}

\ccode{Cache entry allocation/initialization function}{xcache-alloc-init.c}

This function attempts to claim a cache entry from \texttt{free\_nodes}. Then 
it initializes it with the name given by the peer. Moreover, it triggers the 
cache entry initialization hook which cached uses to further initialize the 
entry.

An added benefit of this function is that it doesn't need to acquire the cache 
lock, so it does not slow down the indexing functions that rely on that look.

\subsection{Cache entry indexing}

This is the core feature of xcache. In Listing \ref{lst:xcache-index.h}, we 
present the fields of xcache struct that are relevant to the indexing task.

\ccode{xcache struct fields for entry indexing}{xcache-index.h}

As we have mentioned in Section \ref{sec:xcache-index-design}, we utilize two 
hash tables, one for the cached entries and one for the evicted entries. These 
hash tables can be accessed from the \texttt{xcache struct} and are the 
\texttt{*entries} and \texttt{*rm\_entries} respectively.

More importantly, in Listing \ref{lst:xcache-index.c} we can see the functions 
that are related to indexing and \xcache exposes to the peer:

\ccode{Indexing functions}{xcache-index.c}

All of these functions need a pointer to the \xcache struct. Here's a brief 
description of them:

\begin{description}
	\item[xcache\_lookup:]
		Takes the target's name as an argument and searches for it in 
		cache.\\
		Returns on failure: NoEntry\footnote{\#define NoEntry 
			(xcache\_handler)-1}\\
		Returns on success: the requested handler.\\
		\textbf{Note:} Looks only in \texttt{entries}.
	\item[xcache\_insert:]
		Takes the handler of an allocated entry as an argument and uses 
		it to index that entry.\\
		Returns on failure: NoEntry.\\
		Returns on success:
		\begin{inparaenum}[i)]
		\item the same handler or,
		\item if the same entry already exists in cache, the handler of that 
			entry.
		\end{inparaenum}\\
		\textbf{Note:} It looks up first if the entry exist in 
		\texttt{entries} or \texttt{rm\_entries}. The later case can 
		lead to re-insertions.
		\begin{comment}
			Probably not needed
		\item[xcache\_remove:]
			Takes the handler of an allocated entry as an argument and uses 
			it to remove that entry.\\
			Returns on failure: -1.
			Returns on success: 0.
			\textbf{Note:} Removes entries only from \texttt{entries} hash 
			table.
		\end{comment}
\end{description}

Moreover, we show in Listing \ref{lst:xcache-entry-index.h} the cache entry 
struct fields related to indexing and comment on how they are used by each 
function.

\ccode{xcache entry struct relevant indexing}{xcache-entry-index.h}

The commentary on the above fields follows:

\begin{description}
	\item[ref:] The reference counter of an object is increased on lookups 
		and inserts, since it is essentially referenced in these 
		operations.
	\item[state:] The state of an object is already ACTIVE for lookup 
		operations (or else lookup will fail). For insertions and 
		reinsertions, it is manually set to ACTIVE.
	\item[older/younger]: These fields show the neighbors of the entry in 
		the LRU queue. The LRU queue is sorted by reference time order, 
		so the neighbors are essentially the entries that have been 
		referenced right before and right after our entry.
\end{description}

\subsection{Entry eviction}\label{xcache-evict-imp}

The relevant fields for this purpose can be seen in code listing 
\ref{lst:xcache-evict.h}

\ccode{\xcache struct fields for eviction}{xcache-evict.h}

As we have mentioned in Section \ref{sec:xcache-evict-design}, we resort to 
eviction when the cache is full and new entries can't be inserted. By xcache 
policy, we evict the least recently used entry. The necessary fields for the 
doubly-linked list that we maintain for this purpose can be seen below:

\ccode{Doubly-linked LRU list}{xcache-dlist.h}

The last entry of the list (oldest) is usually the LRU. When an object is 
referenced, it can be instantly transferred to the head of the list (MRU), 
since we know its position via the hash table (alternatively, we would need to 
search all entries, which would require O(N) time).

Another feature of this LRU queue is that it doesn't require timestamps, so we 
can avoid the unnecessary system call.

Finally, when a cache entry is evicted from the hash table, it triggers the 
cache entry eviction hook.

\begin{comment}
\begin{itemize}
	\item Insert a new entry to the LRU list
	\item Evict the LRU entry
	\item Update an entry's access time (i.e. mark it as MRU)
	\item Remove an arbitrary entry
\end{itemize}

Lets explain these fields a bit:

\begin{description}
	\item[lru:] Obviously, it's the least recently used entry. It can be 
		considered as the one end of the doubly linked list.
	\item[mru:] The entry that has just been used. It can be considered as 
		the other end of the doubly-linked list
	\item[younger:] This entry-specific field points to an entry used right 
		after our entry was used.
	\item[older:] Same as "younger", it points to the entry that has been 
		used right before our entry was used.
\end{description}

Finally, as we have explained in Section \ref{sec:xcache-evict-design}, the 
eviction internals should normally not bother the user. However, if the user 
wants to, \xcache exposes the following functions:

\begin{description}
	\item[xcache\_evict\_lru:] The name says it all, it evicts the recently 
		used item.
	\item[xcache\_peek\_and\_get\_lru:] This function allows the user to 
		atomically take a peek on the Least Recently Used entry and also 
		update its refcount.
\end{description}
\end{comment}

\subsection{Concurrency control}\label{sec:xcache-conc-imp}

Concurrency control is an extremely important aspect of xcache, if we want to 
utilize an SMP system to its full potential. Although parts of the concurrency 
control have already been discussed in previous sections, in this section, we 
will provide an in-depth explanation of how xcache implements them.

The relevant fields for concurrency control can be seen in Listing 
\ref{lst:xcache-conc.h}, both for the xcache and xcache\_entry structs.

\ccode{Concurrency control fields}{xcache-conc.h}

There are three main techniques xcache uses for concurrentcy control. The first 
one is the usage of locks, which is presented in Section 
\ref{sec:xcache-lock-imp}.  The second one is reference counting, which is 
presented in Section \ref{sec:xcache-refcount-imp}. Finally, the third one is a 
more esoteric method that counters the ABA problem and is the tracking of 
parallel puts to an entry, which is presented in Section 
\ref{sec:xcache-parput-imp}.

\subsubsection{Locking}\label{sec:xcache-lock-imp}

With xcache, we have tried not to use a BKL
\footnote{BKL stands for Big Kernel Lock and was a giant lock in kernel space 
	that inhibited the performance of SMP systems and remained until the 
	late stages of the 2.6 Linux kernel}
type of lock, but instead use many smaller ones.

Specifically, we have used:
\begin{enumerate}
	\item a lock that protects the cache entry pool from concurrent 
		accesses. Since the only operation this lock protects is the 
		push and pop of cache entry indexes, we expect that there will 
		be no contention on it.
	\item the \texttt{lock} lock, as seen in the xcache struct, which is 
		our main lock as it protects the hash table of active entries 
		(\texttt{entries}) from concurrent accesses. This lock is used 
		during lookups, inserts and evictions, so it is the lock with 
		the most contention.
	\item the \texttt{rm\_lock}, which protects the hash table of evicted 
		entries (\texttt{rm\_entries}) from concurrent accesses and is 
		used during insertions, evictions and puts.
	\item a lock in every entry, which is specifically used when an entry 
		is put.
\end{enumerate}

Of major importance is also the issue of deadlocking. More specifically, during 
inserts or evictions, we need to have access on both hash tables. If a thread 
acquired the lock of one hash table and another thread acquired simultaneously 
the lock of the other, we would have a deadlock since both would need a lock 
that the other thread has.

To this end, xcache strictly acquires the locks in the following order: 
\texttt{lock} --> \texttt{rm\_lock} --> entry lock. With this policy we are 
sure that there will be no deadlocks.

\subsubsection{Reference counting}\label{sec:xcache-refcount-imp}

Each cache entry has a volatile uint64\_t field which is atomically get and 
put. The type is volatile to inform the compiler that it might be changed by an 
external process and therefore not cache it.

Furthermore, the atomic gets and puts are executed using the GCC builtins which 
are shown in Listing \ref{lst:xcache-atomic.c}.

\ccode{Atomic operations of GCC}{xcache-atomic.c}

The refcount model in xcache should be familiar to most people:

% Turn this to figure
\begin{itemize}
	\item When an entry is inserted in cache, the cache holds a reference 
		for it (ref = 1).
	\item Whenever a new lookup for this cache entry succeeds, the reference 
		is increased by 1 (ref++)
	\item When the request that has issued the lookup has finished with an 
		which entry, the reference is decreased by 1. (ref--)
	\item When a cache entry is evicted by cache, the its ref is decreased 
		by 1. (ref--)
\end{itemize}

Moreover, some common refcount cases are:

\begin{itemize}
	\item active entry with pending jobs (ref > 1)
	\item active entry with no pending jobs (ref = 1)
	\item evicted entry with pending jobs (ref > 0)
	\item evicted entry with no pending jobs (ref = 0)
\end{itemize}

\begin{comment}
\begin{table}
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Case & Refcount \\ \hline \hline
		active entry with pending jobs & ref > 1 \\ \hline
		active entry with no pending jobs & ref = 1 \\ \hline
		evicted entry with pending jobs & ref > 0 \\ \hline
		evicted entry with no pending jobs & ref = 0 \\ \hline
	\end{tabular}
	\caption{Reference counting of xcache}
	\label{tab:refcount}
\end{table}
\end{comment}

Unlike most refcount cases, however, the entry is not put when its refcount 
drops to zero. The reason is that the entry can be reinserted at any time. In 
the following section, we explain how we have handled that case

\subsubsection{Entry put}

The scenario of putting the entry has proved the most tricky one and deserves 
its own section in the concurrency control implementation.

For this scenario, we aimed to void the usage of our two biggest lock: 
\texttt{lock} and \texttt{rm\_lock}. Avoiding the first one was easy since the 
hash table of active entries was not used. However, the same did not hold true 
for the \texttt{rm\_lock}.

\fixme How much are we going to explain the ABA problem?

\subsection{Event hooks}

The hooks that xcache provides to users are stored in an xcache\_ops struct 
that can be seen in Listing \ref{lst:xcache-hooks.h}.

\ccode{xcache\_ops struct}{xcache-hooks.h}

The design of these hooks has been presented on Section 
\ref{sec:xcache-hooks-design}. The functions that are attached to each hook 
return two values:
\begin{inparaenum}
\item the private field of xcache (the peer structure in our case) and
\item the cache entry's private data (the object in our case) for which the 
	hook was triggered.
\end{inparaenum}

\section{Implementation of cached}\label{sec:cached-imp}

In this section, we will present the implementation of cached. Information 
about the design of cached is provided in Section \ref{sec:cached-design}.  
Similarly to xcache, we will begin with the initialization process, we will 
continuee with the request handling of cached and finish with presenting the 
challenges we faced and the solutions we implemented.

\subsection{Cached initialization}\label{sec:cached-init-imp}

We have mentioned in the previous chapters that cached can be multi-threaded, 
have different write policies, maximum number of objects, cache size etc. All 
these variables are given from command-line and used during cached 
initialization. The command-line arguments can be seen in Table \ref{tab:usage}

\begin{table}
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Switch & Info \\ \hline \hline
		-t & Number of threads \\ \hline
		-mo & Max objects to cache \\ \hline
		-ts & Total cache size \\ \hline
		-os & Object size \\ \hline
		-bs & Bucket size \\ \hline
		-bp & Blocker port \\ \hline
		-wcp & Write policy \\ \hline
	\end{tabular}
	\caption{Command line arguments of cached}
	\label{tab:usage}
\end{table}

and the cached structure that is initialized is presented in Listing 
\ref{lst:cached.h}. 

\ccode{Main cached struct}{cached.h}

Moreover, on cached initialization we also initialize xcache as well as the 
general xworkqs and xwaitqs.

Some of the above cached fields are the same with the command-line arguments 
and are self explanatory. We will briefly comment on the less obvious fields, 
which will be discussed in length in their respective sections.

\begin{description}
	\item[cache:]
		The initialized xcache struct is stored here
	\item[max\_req\_size:]
		The maximum request size that can be sent to the blocker
	\item[workq:]
		A lockless xworkq where non-critical jobs from threads who are in a 
		critical section are enqueued (More on Section ?)
	\item[pending\_waitq:]
		A waitq for jobs that need to allocate a cache entry to continue
	\item[bucket\_waitq:]
		A waitq for jobs that need to allocate a bucket to continue
	\item[req\_waitq:]
		A waitq for jobs that need to allocate a request to continue
	\item[bucket\_data:]
		This is a malloced space whose size is the total\_size of cache. This 
		space is later split in buckets and its indexes are pushed on a stack
	\item[bucket\_indexes:]
		The stack where bucket indexes are pushed
\end{description}

Furthermore, during the xcache initialization that takes place inside the 
cached initialization, the cached node initialization hook is triggered and 
cached can create its own entries, which are as many as the max objects. The 
cache entry struct (or "ce") can be seen in Listing \ref{lst:ce.h}.

\ccode{Cached entry struct}{ce.h}

The explanation of the above fields follows:

\begin{description}
	\item[status:] The ce status, as seen in Section 
		\ref{sec:cached-states-design}
	\item[lock:] The lock for the ce's and its buckets' data
	\item[workq:] The xworkq that is used for concurrency control over 
		parallel access to the ce's and its buckets' data. It uses the 
		aforementioned lock
	\item[pending\_waitq:] The xwaitq that is used when a request cannot be 
		executed due to the ce's state. It will allow job executions 
		only when the ce is not in FLUSHING state.
\end{description}

We have intentionally left out the bucket related fields that will be discussed 
in length in Section \ref{cached-bucket-imp}.

\subsection{Bucket pool}\label{sec:cached-bucket-imp}

The initialization of the bucket pool is covered in Section 
\ref{sec:cached-init-imp}. In this section, we will explain how this bucket 
pool is connected with the buckets of each ce.

When the cache node initialization hook is triggered, the ce's buckets are 
initialized. Essentially, this means we do (once only) the following:

\begin{enumerate}
	\item First, we calloc an array of struct buckets. The array has
		\texttt{buckets\_per\_object} length, which is typically 1024 
		(4MB objects / 4KB bucket size). The struct bucket is a very 
		simple struct and is presented in Listing 
		\ref{lst:bucket.h}.
		
		\ccode{Bucket implementation}{bucket.h}

	\item Second, we calloc two more arrays, the  
		\texttt{bucket\_alloc\_status\_counters} and the
		\texttt{bucket\_data\_status\_counters}, whose length is the 
		number of allocation states (2) and data states (5) 
		respectively.
	\item Third, we initialize each bucket's allocation state to FREE and 
		data state to INVALID. The allocation and data state are stored 
		in the \texttt{flags} section of struct bucket, which is 
		actually a custom bit-field with support for variable field 
		lengths.
	\item Finally, we initialize all the counters to zero, besides the 
		allocation counter for FREE buckets, which is set to 
		\texttt{buckets\_per\_object} (1024), and the data counter for 
		INVALID buckets, which is similarly to the same number.
\end{enumerate}

None of the above operations, however, interact with the bucket pool. This is 
because we don't initially attach the bucket indexes to the ce's buckets.

The way buckets are attached to the ce is analogous to the way a function maps 
to its address space a large memory chunk that has previously allocated; the 
chunk is internally divided to smaller chunks that are mapped to the function's 
address space only when the function "touches" them.

Similarly, when cached accepts a request for a target, the request's range is 
translated to bucket range. If any of the buckets within that range are not 
attached to the ce, the request is "trapped" and the needed buckets are claimed 
from the bucket pool.

Finally, the bucket claiming procedure is the following: we pop a bucket index 
from the \texttt{bucket\_indexes} stack, translate it to the actual data 
pointer and store it to the \texttt{data} field of the struct bucket. Later on, 
when the bucket is released, we translate the data pointer back to the bucket 
index and we push it to the bucket pool.

\subsection{Request handling}

Cached uses a custom loop to poll for requests. This loop follows the same 
principles as the common peer's loop (see Section ?) with the addition of:

\begin{enumerate}
	\item Checks for the state of the bucket pool. If the bucket pool has 
		been depleted, we force flush the LRU entry to acquire its 
		buckets.
	\item Periodic signals to the cached's xworkq.
\end{enumerate}

When a request is accepted/received, it is forwarded to the appropriate handle 
function based on its xcache operation type.

More specifically, for accepted (new) requests, we index the request target 
(object) and store its xcache handler on the request's cio and we proceed 
according to its operation type. For received requests, the request's cio holds 
the xcache handler for the object, so we can proceed immediately according to 
its operation type. The way the request is handled next is documented on 
Section \ref{cached-flow-design}.

\subsection{"ENOSPC" scenarios}

\fixme add out of requests, out of buckets, out of cache entries
