\chapter{Design of cached}\label{ch:cached-design}

In the previous chapters, we have addressed the need for tiering in terms of
scalability as well as performance. % Which are these chapters? Link them.

We have also evaluated current caching solutions and described why they couldn't 
be used as a cache tier in Archipelago. % Provide link to a table of comparisons

With the results of chapter ? in mind, we can provide some more strict 
requirements that our solution must have:

% List the requirements for our solution
\begin{enumerate}
	\item Requirement 1
	\item Requirement 2
	\item Requirement 3
	\item Requirement 4
\end{enumerate}

The following two chapters are the main bulk of this thesis and they present our 
own implementation that aims to fill the above requirements.

% Foretell what the following sections will contain
More specifically, this chapter provides an in-depth description of the design 
of cached.  Section ? provides a general overview of cached. Sections ? - ?  
present the building blocks of cached and their design. Section ? presents the 
interaction of cached and its building blocks. Finally, in Section ?  we 
illustrate the flow of requests for cached.

\section{Design overview}

\begin{comment}
In order to provide a caching tier for Archipelago that would blabla our 
requirements, we had to create our own implementation. Its name is simply cached 
(\textbf{cache d}aemon) and is another XSEG peer with similar structure to those 
seen in chapter ? %link to chapter
\end{comment}

Cached is a peer that operates between the vlmc and blocker. Every request that 
the vlmc peer sends to the blocker, the cached peer can \textit{intercept} it, 
so to speak, and cache it. This follows the same principle with bcache, which 
plugs its own request\_fn() function to the virtual device it creates. Unlike 
bcache however, cached can be plugged on and off at any time.

Cached has two different caching policies, \textbf{write-through} and 
\textbf{write-back}. These policies aren't new and have been discussed 
extensively in chapter ?, but let's see what these policies translate to in 
cached context.

\begin{itemize}
	\item
		In \textbf{write-back} mode, cached caches writes, immediately 
		serves the request back and marks the data as dirty. When a read 
		arrives, it either serves the request with the dirty data 
		(read-hit) or forwards the request to the storage peer and 
		caches the answer (read-miss).

		This policy is used when we want to improve read and write speed 
		and can sacrifice data safety.
	\item
		In \textbf{write-through} mode, cached forwards writes to 
		blocker, servers the request when blocker replies, caches the 
		data and marks them as valid.  When a read arrives, it either 
		serves the request with the valid data (read-hit) or forwards 
		the request to the storage peer and caches the answer 
		(read-miss).

		This policy is used when we want to improve read speed and want 
		to make sure that no data will be lost.
\end{itemize}	

\subsection{And we cache... what exactly?...}

Since Archipelago divides internally the volumes to objects (usually 4MB), the 
cached peer must operate on object level. Also, in order to know which parts of 
the cached object are actually written, or are in the process of being read etc.  
cached further divides objects to the next and final logical entity, buckets 
(typically 4KB). Each bucket consists of its data and metadata and cannot be 
half-empty, or half-allocated. You can say that the bucket is the quantum of 
cached objects.

I'll attempt to make the above a bit clearer. When cached receives a request, it 
first checks the request target (i.e. the object name and then calculates which 
bucket objects are within the request's range. It is easy to see that this is a 
1:1 mapping to the object's data.

The fact that objects are pre-allocated means two things:

\begin{enumerate}
	\item We don't need to care about memory fragmentation and system call 
		overhead
	\item We cannot index single buckets. <FILLME>
\end{enumerate}

\subsection{Cached components}

Let's see now the design of cached in detail. The cached peer consists of a 
number of building blocks.  Per Archipelago policy, most of these building 
blocks have been written in the xtypes fashion. The reasons behind this decision 
have been discussed in chapter ?
% Link to where we define xtypes
but for completeness' sake, we will mention once more the merits of xtypes:

% Mention merits briefly


The components of cached can be seen below:

% TODO: Link List to the definitions
\begin{itemize}
	\item xcache, an xtype that provides indexing support, amongst many 
		other things
	\item xworkq, an xtype that guarantees atomicity for execution of jobs 
		on the same object
	\item xwaitq, an xtype that allows conditional execution of jobs
	\item bucket pool, a pre-allocated memory pool for buckets
\end{itemize}

and their design will be discussed in-depth in the following sections.

\section{The xcache xtype}\label{sec:xcache-design}

xcache is the main component of cached. It is responsible for several key 
aspects of caching such as:

\begin{itemize}
	\item entry indexing,
	\item entry eviction, and
	\item concurrency control
\end{itemize}

Below we can see a design overview of xcache:

% Add figure for xcache with both hash tables, xq etc.

% While explaining, point to the diagram objects, a, b, c etc.
As we can see above, xcache utilizes two hash tables. One hash table is 
responsible for indexing entriess (or more generally speaking "cache entries") 
that are active in cache.  The other hash table is responsible for indexing 
evicted cache entries that have pending jobs.  Again, more generally speaking, 
evicted cache entries are entries whose refcount has not dropped to zero yet.

\subsection{Entry Preallocation}

Since xcache has a bounded number of entries that will allocate, there is no 
need to allocate them on-the fly using malloc/free. Considering that we are 
caching at RAM level and not at SSD level, the system call overhead will have a 
considerable impact on performance.

Thus, the best thing to do in our case would be to pre-allocate the necessary 
space.

\subsection{Entry indexing}

In order to index the cached entries, xcache relies on another xtype, xhash, 
which is a hash table. Moreover, it's actually the C implementation of the 
dictionary used in Python.
% explain that for sparse Python dictionaries, accesses are O(1)

We have chosen to use a hash table as our index because:
% List of pros and cons of hash tables and why cons are ok in our book

Finally, the xhash xtype gives provides us with the basic hash table functions, 
namely:

\begin{itemize}
	\item Insertion
	\item Look-up
	\item Deletion
\end{itemize}

\subsection{Entry eviction}

As we can see in figure ?, xcache has been designed to index a pre-defined 
number of entries. That means that when xcache reaches its maximum capacity and 
is requested to index a new entry, it has to resort to the eviction of a 
previously cached entry. We have chosen the LRU strategy 

Also, an added bonus is that we won't need to sacrifice speed over optimality, 
since that, our hash table approach allows us to create an O(1) LRU algorithm 
which you can see in the following figure:

% Insert figure for O(1) LRU and hash table

In a nutshell, our LRU implementation uses a doubly linked list blablabla.
This design allows us to do all of the following action in constant time:

\begin{itemize}
	\item Insert a new entry to the LRU list
	\item Evict the LRU entry
	\item Update an entry's access time (i.e. mark it as MRU)
	\item Remove an arbitrary entry
\end{itemize}

% TODO: Explain what is the interesting thing here, that evictions happen 
% implicitly
Another interesting feature of xcache is that evictions occur implicitly and not 
explicitly. The user doesn't need to interact with the LRU queue.

For example, when a user tries to insert a new entry to an already full cache, 
the insertion will succeed and the user will not be prompted to evict an entry 
manually. Also, the user will be notified via specific event hook that is 
triggered upon eviction that an entry has been evicted.

More about hooks can be seen in the following subsection.

\subsection{Concurrency control}

The concept of concurrency control has been discussed in chapter ?. The goal of 
xcache is to handle safely - and preferably fast - simultaneous accesses to 
shared memory.

In order to do so, we must first identify which are the critical sections of 
xcache, that is the sections where a thread can modify a shared structure. These 
sections are the following

\begin{itemize}
	\item
		All xhash operations: Two of the three xhash operations (inserts 
		and removals) can modify the hash table (e.g. they can resize it 
		and reallocate space for it). This means that the third one 
		(lookups) must not run concurrently with the other.
	\item
		Cache node claiming: Before an entry is inserted, it must	
		acquire one of the pre-allocated nodes and we must ensure that 
		this can happen from all threads.
	\item
		Entry migration: An entry can migrate from one hash table to 
		the other e.g. on cache eviction. This migration involves a 
		series of xhash operations; removal from one hash table and 
		subsequent insertion to the other. This a scenario that must be 
		handled properly.
	\item
		Reference counting: Every entry must have a reference counter.  
		Reference counters provide a simple way to determine when an 
		entry can be safely removed. You can see more about reference 
		counting in chapter ?
	\item
		LRU updates: Most actions that involve cache entries must 
		subsequently update the LRU queue. Being a doubly linked list, 
		if two threads update the LRU simultaneously, we can lead to 
		segfaults.
\end{itemize}

Let's see what guarantees we provide for each of the above scenarios:

\begin{itemize}
	\item
		xhash operations: We provide a lock for each hash table
	\item
		Cache node claiming: The free node queue is protected by a fast 
		lock
	\item
		Entry migration: We always take fist the lock for entries and 
		then for rm\_entries
	\item
		Reference counting: Another important guarantee is the 
		reference counting of entrys. xcache uses atomic gets and puts 
		to update the reference count of an entry.
	\item
		LRU updates: Since all LRU operations take place for entries in 
		"entries" hash table and LRU updates are blazing fast we can 
		secure our LRU with the cache->lock.
\end{itemize}

% TODO: Yada yada about refcount

\subsection{Re-insertion}

We have previously mentioned that in xcache, there can be data migration 
between hash tables. This is easy to see why in case of evictions: an entry 
that previously was in "entries" must now be migrated to "rm\_entries" until 
its reference count falls to zero and can be freed.

However, what happens when xcache receives a request for an evicted entry? 

%Explain, explain, explain the re-insertion
there is a concept called "re-insertion".  In order for an entry to
be re-inserted to the primary hash table (which will be called "entries" from
now on) it must first reside in the hash table that indexes the evicted cache
entries (which will be called "rm\_entries" from now on). As mentioned above,
an entry that is in rm\_entries has probably pending jobs that delay its
removal.

So, what happens if a lookup arrives for that entry while on this stage? In
this case, we re-insert it to entries and increase its refcount by 2, since
there is one reference by the hash table and one reference by the one who
requested the lookup.

\begin{comment}
\subsection{Hooks}


The hooks that xcache provides to users are:

\begin{itemize}
	\item on\_ init: called on cache entry initialization.
	\item on\_put: called when the last reference to the cache entry is put
	\item on\_evict: called when a cache entry is evicted.
	\item on\_node\_init: called on initial node preparation.
	\item post\_evict: called after an eviction has occurred, with cache  
		lock held.
	\item on\_free: called when a cache entry is freed.
	\item on\_finalize: called to hint the user that the cache entry's ref 
		has dropped to zero.
	\item on\_reinsert: called when a cache entry has been in cache
\end{itemize}

\end{comment}

\subsection{xcache flow}

Below we will see three important scenarios

\textbf{Insertion}

Figure

\textbf{Lookup}

Figure

\textbf{Put}

Figure

\section{The xworkq xtype}

The xworkq xtype is a useful (what?) for concurrency control on object level. It 
is important to distinguish between cache level operations and object level 
operations. Cache level operations include insertions, lookups, removals, 
allocations and refcount handling. On object level, there is a different set of 
operations that must be synchronized across threads. Namely, we have bucket 
claiming, read/write operations and object flushes.

The above distinction makes it easy to see that provided that operations on 
object level need not worry about interactions with other objects. Each object 
is "sandboxed", so to speak.

Let's see the design of the xworkq xtype. It consist of a queue where jobs (e.g.  
read from block, write to block) are enqueued. The thread that enqueues a job 
can attempt to execute it to, by acquiring a lock for the workq. If the lock is 
free, the thread will be able to execute the enqueued job. Also, other threads 
can enqueue their jobs, so the thread that has the lock can do those too. There 
is an xworkq for every object.

Every object has a workq. Whenever a new request is accepted/received for an
object, it is enqueued in the workq and we are sure that only one thread at a
time can have access to the objects data and metadata.

For more information, see the xworkq.

\section{The xwaitq xtype}

When a thread tries to insert an object in cache but fails, due to the fact
that cache is full, the request is enqueued in the xcache waitq, which is
signaled every time an object is freed.

For more information, see the xwaitq.

\section{Cached internals}

\subsection{Object states}

Every object has a state, which is set atomically by threads. The state list is
the following:

\begin{itemize}
	\item READY: the object is ready to be used
	\item FLUSHING: the object is flushing its dirty buckets
	\item DELETING: there is a delete request that has been sent to the 
		blocker for this object
	\item INVALIDATED: the object has been deleted
	\item FAILED: something went very wrong with this object
\end{itemize}

Also, object buckets have their own states too:

\begin{itemize}
	\item INVALID: the same as empty
	\item LOADING: there is a pending read to blocker for this bucket
	\item VALID: the bucket is clean and can be read
	\item DIRTY: the bucket can be read but its contents have not been
		written to the underlying storage
	\item WRITING: there is a pending write to blocker for this bucket
\end{itemize}

Finally, for every object there are bucket state counters, which are increased/
decreased when a bucket state is changed. These counters give us an O(1)
glimpse to the bucket states of an object.

\subsection{Per-object peer requests}

Reads and writes to objects are practically read/write request from other
peers, for which a peer request has been allocated. There are cases though
when an object has to allocate its own peer request e.g. due to a flushing of
its dirty buckets. Since this must be fast, there are pre-allocated requests
hard-coded in the struct of each object which can be used in such cases.

\subsection{Write policy}

The user must define beforehand what is the write policy of cache. There are
two options: write-through and write-back. On a side note, as far as reads and
cache misses are concerned, cached operates under a write-allocate policy.

\section{Cached Operation}

\subsection{Write-through mode}

Here we will see how cached operates in write-through mode.

\subsubsection{Write}

This is the flow for the write path:

\subsubsection{Read}

This is the flow for the read path:

\subsection{Write-back mode}

Here we will see how cached operates in write-back mode.

\subsubsection{Write}

This is the flow for the write path:

\subsubsection{Read}

This is the flow for the read path:

