\chapter{The synapsed peer}\label{ch:synapsed}

\fixme we probably don't care about cpu utilization but about the fact that 
cached will go down when the host goes down too.

On the previous sections, we have evaluated the design of cached and we have 
reached the conclusion that it is heavily bounded by CPU. The implications of 
this, however, are bigger, if we consider that all the Archipelago is running 
in the host machine, whose CPU's are already oversubscribed. This means that 
cached must compete for CPU time against the running VMs, essentially defeating 
the QoS purpose it serves.

\todo Better explain that if cached could run on the under-utilized RADOS 
nodes, it would be accessible from any host and thus would be a major step 
forward towards a distributed or eventually persistent cache.

\begin{comment}
On the other hand, on the RADOS nodes, the CPUs and RAM are not used to their 
full potential. Also, since these nodes are accessible from any host, it would 
be a step forward in making cached distributed if cached could run there.

Thus, it would be interesting to check how well cached (and Archipelago in 
general) would behave, if the VM's data where sent initially over network and 
then handled by the vlmcd. So what we mean is to turn the current situation 
from how it is in Figure \ref{fig:new_sxima.pdf} to the following:

\fixme add figure for network
\end{comment}

To this end, we have created a network peer called synapsed (from 
\textbf{synapse d}aemon) as a proof-of-concept, that should be able to 
accept/receive any type of requests and send them through the network to 
another Archipelago segment. We must note that this peer has not been created 
with high-performance in mind but its main aim is to provide the functionality 
needed for our purposes.  As a consequence, we haven't used tools such as 
ZeroMQ or libevent that would boost the performance of the implementation.

More specifically, on Section...

\section{Design of synapsed}

Given that currently Archipelago is not network-aware, peers from one segment 
cannot know the ports of peers of another segment. Moreover, they cannot send a 
request to the synapsed peer and simultaneously target a peer in another 
segment. So, we are faced with the problem of defining the limitations of 
synapsed.

We propose the following solution to this problem: Each of these two segments 
must have at least one synapsed peer. So, when one synapsed accepts an XSEG 
request, it will translate it to a network request, send it to the synapsed 
peer of the other segment, who will finally translate it back to an XSEG 
request.  Moreover, each synapsed peer must be attached to a peer of its 
segment, which will serve as the request target when synapsed accepts request.  

This means that synapsed does not actually connect two remote segments
but bridge two remote peers over network.

The way synapsed handles requests can be seen in Figure ?

\fixme add figure

The main difficulty is to make synapsed listen from its port and its request 
queue simultaneously. We have tackled this problem in Section 
\ref{sec:poll-synapsed}.

Moreover, synapsed sends requests using the standard TCP protocol.  This means 
that it handles the marshaling of buffers, send/receive errors as well as 
polling for new requests all by itself.

Finally, synapsed has been designed as a single-threaded peer.

\section{Implementation of synapsed}

\todo Explain what are the following chapters about.

\subsection{Synapsed initialization}

Synapsed is a much simpler peer than cached and requires only the following 
arguments.

\begin{description}
	\item[-hp] The port where it will listen for requests.
	\item[-ra] The remote address of the segment. \textbf{NOTE:} it can 
		point to the same segment, which means that synapsed provides a 
		generic way to bridge two peers who reside in the same host but 
		in a different segment.
	\item[-rp] The remote port of the other synapsed.
	\item[-txp] The XSEG port of the peer which synapsed will attach on
\end{description}

Having the above arguments, synapsed can create a socket and bing its port it.  

\subsection{Request polling}\label{sec:poll-synapsed}

Synapsed typically uses the common peer loop and Archipelago IPC methods to 
check its XSEG ports for new requests. However, it simultaneously needs to 
listen for new requests on its socket. Yet, there is no way for a process to 
block on its socket *and* sleep using sigtimedwait.

The solution seems obvious; synapsed should instead block while polling for 
requests in its socket and, if a new request arrives at its XSEG port, its 
polling will be interrupted. This is a simple solution, but there is one
detail we must take into account: Sleeping like so is unsafe because peers 
currently block the SIGIO signal in order to wake up synchronously.

Let's elaborate on it a bit: When a peer is initialized, it blocks the SIGIO 
signal. Using sigtimedwait, it can check fast and without races if a signal has 
been enqueued, and then it can sleep. This is not the case with poll() though, 
since if the peer sleeps with the SIGIO signal blocked, it will not wake up.  
On the other hand, if we unblock the SIGIO signal, we have a new set of 
problems:

Consider the case where we receive a signal before we go to sleep. In order to 
be notified that a signal has been sent so as not to sleep, we would need to 
utilize a signal handler that would increment a global variable which would be 
checked right before we go to sleep. This approach is not only racy (what 
happens if SIGIO is received after this global variable is checked) but it is 
also very slow.

For this reason, we use the ppoll alternative, which accepts a signal mask as 
an additional argument. Ppoll provides the guarantee that it will atomically 
\begin{inparaenum}[i.]
\item replace the old signal mask of the process with the one provided,
\item check if there are pending signals and
\item return if there are pending signals or block until a request has been 
	received.
\end{inparaenum}

Solving the issue of waiting for XSEG requests while blocking on a socket is 
only part of the problem. We also have to solve its counterpart, which means we 
need to find a quick way to iterate the XSEG ports of synapsed for new 
requests, while listening for new requests on its socket.

We have countered this problem by checking alternately the ports of synapsed 
and doing a poll with zero timeout, which returns immediately.

\subsection{Marshalling}

The XSEG request cannot be sent as is, since its data and target name are 
stored in two different buffers. Moreover, since these buffers can have 
variable length, the synapsed peer that reads from its socket needs a way to 
know when it has finished reading all the necessary data.

This is a common problem in computer science and its solution is to serialize 
(or "marshal") the object that must be transfered. The marshaling method we 
have chosen is the following:

\begin{enumerate}
	\item We send at the start of every transaction a fixed-length header, 
		whose size is known to all synapsed peers, and which has the 
		information about the length of the rest of the buffers.  The 
		header is presented on Listing \ref{lst:synapsed-header.h}

		\ccode{Synapsed header}{synapsed-header.h}

		The header is simply the necessary fields of the original 
		request that the remote synapsed peer needs to know. The most 
		important fields are \textbf{datalen} and \textbf{targetlen} 
		which indicate the length of the data and target buffers 
		respectively.
	\item Once the header has been sent and the remote peer has the 
		information that it needs, it allocates a new XSEG request from 
		its segment and fills it with the information provided by the 
		header.
	\item Finally, the remote peer reads the rest two buffers and stores 
		them in the XSEG request it has previously allocated.
\end{enumerate}

Marshaling however is usually not so simple. There are three caveats that one 
must take into account before attempting to serialize manually an object. They 
derive from the often overlooked fact that the host and the remote machine are 
not architecturally the same, which can lead to:

\begin{enumerate}
	\item \textbf{Different endianness}. This means that the two machines have 
		completely different byte order. This is commonly solved by converting 
		all of the data to network byte order (big endian) and then convert 
		them to the native endianess of the machine.
	\item \textbf{Different type representation}. Even if two architectures 
		have the same endianness, it is possible that they represent the same 
		types with different number of bytes. This is most common with 32-bit 
		and 64-bit architectures which, for example, use 4 bytes and 8 bytes 
		respectively to represent an int.
	\item \textbf{Padding}. Due to data alignment issues, the compiler may need 
		to pad the fields of a struct. The padding is depended on the machine's 
		architecture but also on the compiler that is used (\fixme is this 
		true?).  A common solution is to "pack" the structure, i.e. to enforce 
		that the structure will have no padding.
\end{enumerate}

For synapsed, the first two caveats do not affect us, since the machines that 
are used are both 64-bit, little endian machines. The third caveat may also not 
affect us, but just to make sure, we have packed our header structures using 
the gcc pragma directive:

\ccode{GCC pragma pack directive}{synapsed-pack.h}

\subsection{Send/Receive}

As we have mentioned in the previous chapter, in order to send the data from 
one synapsed peer to the other, we must marshal them first and then unmarshal 
them. This commonly requires to merge all buffers into one and send that one.  

In synapsed however, we have chosen a different appoach; we have employed the 
\texttt{readv()}/\texttt{writev()} functions, that allow us to do vectored I/O 
on these buffers.

\todo wrap up this section

\section{Evaluation of synapsed}

\diagram{Bandwidth performance for writes through 
	synapsed}{bw-write-synapsed.pdf}
\diagram{Latency performance for writes through 
	synapsed}{lat-write-synapsed.pdf}

\todo add a simple diagram of synapsed's performance
