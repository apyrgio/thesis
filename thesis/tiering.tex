\chapter{Tiered Storage}\label{ch:tiering}

In this chapter we will discuss the challenges of today's data storage and how 
tiered storage helps in mitigating costs and increasing performance. Moreover, 
we present the current solutions for tiered storage and we evaluate if they can 
be used in conjunction with Archipelago.

\section{Theoretical Background or What is Tiered Storage?}

1. Gap between performance of RAM and disk.
2. Tiered storage is a concept that has been taken as an analogy from 
hierarchical storage of computers.

Techniques employed by vendors: scaling up or scaling out
Caching has been used very much
3. Large vendors use storage layers (Tiers) such as memcached servers, ssds, 
hard drives etc.
4. Coherency is important as well as how the changes are propagated downwards

Tiered storage is a term that has different meaning for different contexts. We 
will approach this subject from the storage service perspective, which is an 
integral part of a cloud service.

Tiered storage is a strategy that most medium or larger deployments use in 
order to bridge the performance gap between RAM and magnetic disks, which can 
be seen in Table \ref{tab:gap}

\begin{table}
	\centering
	\begin{tabular}{ | l | l | }
		\hline
		Medium & Access time (ns) \\ \hline \hline
		CPU registers and cache & < 10 \\ \hline
		RAM & < 10\textsuperscript{2}  \\hline
		SSD & < 10\textsuperscript{5} \\ \hline
		Hard Disk & < 10\textsuperscript{7} \\ \hline
	\end{tabular}
	\caption{Access times of storage mediums}
	\label{tab:gap}
\end{table}

It is easy to see that when data are not  in RAM the performance penalty is 
x10,000.

Usually, when a small deployment makes its first steps, it doesn't use SSDs due 
to management and hardware costs and since its an investment that is actually 
needed when the deployment has proved that it will attract traffic. Instead, 
the most common setup is an array of RAID-protected commodity hard disks or 
fast SAS drives.

When the storage needs start to increase and more users use that service, the 
OS caching system of the storage nodes will soon prove ineffective and the 
randomness in the requested data will skyrocket the access times. Soon, the 
infrastructure's access times will in the spectrum of many milliseconds.

At this point, the administrators are faced with the decision of buying or not 
battery-backed array controllers with volatile storage on-board to improve 
speeds but sooner or later, the quality of hardware will not be able to 
mitigate the cost of cache misses.

This is the turning point of medium deployments and where tiered storage helps


\subsection{Caching}

In caching, there are usually the following two policies:

\begin{itemize}
	\item Write-through:
		This policy bla bla bla
	\item Write-back:
		This policy blu blu blu
\end{itemize}

\subsubsection{Eviction}

Caching generally means that you project a large address space of a slow medium 
to the smaller address space of a faster medium. That means that not everything 
can be cached as there is no 1:1 mapping. So, when a cache reaches its maximum 
capacity, it must evict one of its entries

And the big question now arises: which entry?

This is a very old and well documented problem that still troubles the research 
community. It was first faced when creating hardware caches (the L1, L2 CPU 
caches we are familiar with). In 1966, Lazlo Belady proved that the best 
strategy is to evict the entry that is going to be used more later on in the 
future\cite{Belady}.  However, the clairvoyance needed for this strategy was a 
little difficult to implement, so we had to resort to one of the following, 
well-known strategies:

% Mention ehcache approach: 
% http://ehcache.org/documentation/apis/cache-eviction-algorithms
%
% Also look at the following papers
%L. Belady, “A Study of Replacement Algorithms for a
%Virtual-Storage Computer,” IBM Systems Journal, vol.5,
%no.2, pp.78-101, 1966.
\begin{itemize}
	\item \textbf{Random:} Simply, a randomly chosen entry is evicted. This 
		strategy, although it seems simplistic at first, is sometimes chosen 
		due to the ease and speed of each. It is preferred in random workloads 
		where getting fast free space for an entry is more important than the 
		entry that will be evicted.
	\item \textbf{FIFO (First-In-First-Out):} The entry that was first inserted 
		will also be the first to evict. This is also a very simplistic 
		approach as well as easy and fast. Interestingly, although it would 
		seem to produce better results than Random eviction, it is rarely used 
		though, since it assumes that cache entries are used only once, which 
		is not common in real-life situations.
	\item \textbf{LRU (Least-Recently-Used)}
	\item \textbf{LFU (Least-Frequently-Used)}
\end{itemize}

Choosing the LRU strategy is usually a no-brainer. Not only does it 
\textit{seem} more optimal than the other algorithms, but it has also been 
proven, using a Bayesian statistic model, that no other algorithm that tracks 
the last K references to an entry can be more optimal.
%Also, check out this paper:
%An optimality proof of the LRU-K page replacement algorithm
%that proves that no algorithm that keeps track of the K most recent references 
%for a page can be more optimal than LRU.



\section{Comparison between existing methodologies}

\subsection{Block-devices}

There have been many block device tiers that have been built


\subsubsection{Bcache}

\paragraph{Overview}

Bcache has been designed by Kent Overstreet since 2011 and has been included in 
the Linux kernel (3.10) since the May of 2013.

Bcache allows one to use one or more fast mediums as a cache for slower ones.  
Typically, the slow medium is a RAID array of hard disks and the fast medium 
are SSD drives. Bcache has been specifically built for SSDs and has the 
following characteristics:

\begin{enumerate}
	\item The data are written sequentially and in erase block size 
		granularity, in order to avoid the costly read-erase-modify-write 
		cycle.
	\item It takes special care to mitigate wear-leveling by touching equally 
		all SSD cells
	\item It honors TRIM requests and uses them as hints for its garbage 
		collection.
\end{enumerate}

\paragraph{Installation}

Bcache is a kernel driver that needs a patched kernel and intrusive changes to 
the backing device, rendering previous data unreadable. On a nutshell, bcache 
edits the superblock of both the cache and backing devices in order to use 
them.  Then, it exposes to the users a virtual block device, which can be 
formatted to any filesystem.  This virtual device's request function actually 
points to the cache device and, when the cache device has reached its full 
utilization, it will flush dirty data to the backing device.

\paragraph{Features and Limitations}

The most striking bcache feature is that it uses a custom built B+tree as an 
index, which has the added benefit that dirty data can be coalesced and flushed 
sequentially to the slower spinning medium. This provides a considerable 
performance speed-up for hard disks since. Some of other noteworthy features of 
bcache are the following:

\begin{enumerate}
	\item It can be used to cache more than one devices
	\item It can operate in three modes, write-through, write-back and 
		write-around, which can be switched on/off arbitrarily during normal 
		usage or when the fast medium is congested.
	\item It utilizes a journal log of outstanding writes so that the data are 
		safe, even when an unclean shutdown occurs.
	\item It can bypass sequential IO and send it directly to the backing 
		device, since this workload is tailored for spinning disks.
\end{enumerate}

\subsubsection{Flashcache}

\paragraph{Overview}

Flashcache has been designed by Facebook and has been open-sourced in the April 
of 2010. It is a kernel module that is officially supported for kernels between  
2.6.18 and 2.6.38 and is based on the Linux Device Mapper, which is used to map 
a block device onto another.

\paragraph{Installation}

Flashcache's installation is not system-intrusive, in the sense that it needs 
only to compile the module against the kernel's source, modprobe it and then 
map the cache device upon the backing device, without making any changes to the 
latter.

\paragraph{Features and Limitations}

Flashcache uses a set-associative hash table for indexing. It has three modes 
of operation, writethrough, writeback and writearound, and some basic 
performance tuning options such eviction strategies and dirty data threshold.  
Also, it has the following limitations:

\begin{enumerate}
	\item It does not provide atomic write operations, which can lead to 
		page-tearing.
	\item It does not support the TRΙΜ command.
\end{enumerate}

\subsubsection{EnhanceIO}

\paragraph{Overview}

EnhanceIO has been developed by STEC Corp. and has been open-sourced in the 
December of 2012. It is a fork of Flashcache which does not use the Linux 
Device Mapper and has some major re-writes in parts of the code such as the 
write-back caching policy.

\paragraph{Installation}

The installation method is similar to the Flashcache's method. The source code 
is compiled again the kernel's source, which produces a module that can be 
modprobed. After that, the utilities provided can be used to map the cache 
device on the backing device.

\paragraph{Features and Limitations}

Similarly to Flashcache, EnhanceIO uses a set-associative hash table for 
indexing. It also has improvements upon the original Flashcache implementation 
in the following areas:

\begin{enumerate}
	\item The page-tearing problems have been solved.
	\item Dirty data flushing using background threads.
\end{enumerate}

\subsubsection{DM-cache}

\subsection{Key-value store}

\subsubsection{Memcached}

\paragraph{Overview}

Memcached is a distributed memory caching system that is being widely employed 
by large sites such as Youtube, Facebook, Twitter, Wikipedia. It has been 
created in 2003 by Brad Fitzpatrick while working in LiveJournal and to date 
there have been numerous forks of the code, most notably including Twitter's 
twemcache and fatcache, Facebook's implementation etc.

When memcached came into existence, many social sites like LiveJournal were 
experiencing the following problem:

User pages would often have queries that would be executed hundreds of times 
per second or would span across the database due to a big SELECT, but whose 
nature would be less critical or would not change rapidly. Queries such as "Who 
are my friends and who of them are online?", "What are the latest news in my 
feed?" etc. which could be easily cached, crippled instead the database by 
adding a lot of load to it.

To tackle this problem, administrator can instruct memcached to utilize the 
unused RAM of the site's servers to cache these kinds of queries. Ten years 
later, memcached has become the defacto scale-out solution, and has use cases 
such as Facebook's whose dedicated memcached servers serve the 95\% of their 
queries (\fixme add presentation)

\paragraph{Installation/Usage}

Memcached adheres to the client server model, with N clients connecting to M 
servers. Memcached, which is a user space daemon, runs on every server and
listens for requests typically on port 11211. The installation is very easy 
since there are packages for most known distros. Once memcached has been 
installed, the administration needs to specify only the port and several 
performance options such as cache size and number of threads.

The clients on the other hand communicate with the memcached servers using 
native libraries. There are libraries that are written for most programming 
languages such as C, PHP, Python, Haskell etc. The clients can then specify 
which queries - or keys in general - want to be cached and the actual caching 
is done in runtime.

\paragraph{Features/Limitations}

Architecturally, memcached tries to do everything in O(1) time. Each memcached 
server consists of a hash table that indexes the keys and their data. Since the 
data size can vary from 1 byte to 1MB, they are organized in SLABs in order to 
prevent memory fragmentation.  Moreover, each memcached must be able to handle 
tens of thousands connections from clients, so it relies in libevent to do the 
asynchronous polling.

What's more interesting about memcached is that its main strength is actually 
its biggest limitation. Memcached has no persistence and in fact, data can be 
evicted in numerous ways:

\begin{enumerate}
	\item Cached data have an expiration time after which they are 
		garbage-collected.
	\item Data can be evicted before their expiration time, if the cache has 
		become full.
	\item When memcached is out of SLAB pages, it must evict one in order to 
		regain space. This leads to the eviction of more than one keys.
	\item When adding or removing memcached servers, the Ketama algorithm that 
		maps keys to servers will assign a portion of the existing keys to 
		other servers. This change in mapping, however, will not actually move 
		the existing keys to these servers and the data are essentially 
		invalidated.
\end{enumerate}

To sum up, the lack of persistence means that memcached will never hit the disk 
bottleneck due to flushes and will always be very fast, as long as the cache 
hit rate is high. On the other hand, its unreliable nature means that it is not 
a general purpose software and only specific workloads will be benefited from 
it.

\subsection{NoSQL Databases}

\subsubsection{Couchbase}

\paragraph{Overview}

Couchbase, which has been under active development since the January of 2012, 
is actually the product of the merge of two independent projects, CouchDB and 
Memebase, with CouchDB continuing as an Apache funded program. Couchbase aims 
to combine the scalability of memcached with the persistence of a database such 
as CouchDB. 

\paragraph{Installation/Usage}

Moreover, it tries to add some more features such as replication
to be alive as a projects.

\subsection{Honorary mentions}

\subsubsection{Repcached}

\subsubsection{Ramcloud}

\subsection{Summary}





