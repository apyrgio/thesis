\chapter{Tiered Storage}\label{ch:tiering}

In this chapter we will discuss the challenges of today's data storage and how 
tiered storage helps in mitigating costs and increasing performance. Moreover, 
we present the current solutions for tiered storage and we evaluate if they can 
be used in conjunction with Archipelago.

\section{Theoretical Background or What is Tiered Storage}

From the genesis of the PC in ? every component was pushed to its limits to 
become as fast as possible. Take for example the CPU chip. The CPU clock is 
blabal and speed of light means 10cm. Has the same happened with disk

\subsection{Caching}

In caching, there are usually the following two policies:

\begin{itemize}
	\item Write-through:
		This policy bla bla bla
	\item Write-back:
		This policy blu blu blu
\end{itemize}

\subsubsection{Eviction}

Caching generally means that you project a large address space of a slow medium 
to the smaller address space of a faster medium. That means that not everything 
can be cached as there is no 1:1 mapping. So, when a cache reaches its maximum 
capacity, it must evict one of its entries

And the big question now arises: which entry?

This is a very old and well documented problem that still troubles the research 
community. It was first faced when creating hardware caches (the L1, L2 CPU 
caches we are familiar with). In 1966, Lazlo Belady proved that the best 
strategy is to evict the entry that is going to be used more later on in the 
future\cite{Belady}.  However, the clairvoyance needed for this strategy was a 
little difficult to implement, so we had to resort to one of the following, 
well-known strategies:

% Mention ehcache approach: 
% http://ehcache.org/documentation/apis/cache-eviction-algorithms
%
% Also look at the following papers
%L. Belady, “A Study of Replacement Algorithms for a
%Virtual-Storage Computer,” IBM Systems Journal, vol.5,
%no.2, pp.78-101, 1966.
\begin{itemize}
	\item \textbf{Random:} Simply, a randomly chosen entry is evicted. This 
		strategy, although it seems simplistic at first, is sometimes chosen 
		due to the ease and speed of each. It is preferred in random workloads 
		where getting fast free space for an entry is more important than the 
		entry that will be evicted.
	\item \textbf{FIFO (First-In-First-Out):} The entry that was first inserted 
		will also be the first to evict. This is also a very simplistic 
		approach as well as easy and fast. Interestingly, although it would 
		seem to produce better results than Random eviction, it is rarely used 
		though, since it assumes that cache entries are used only once, which 
		is not common in real-life situations.
	\item \textbf{LRU (Least-Recently-Used)}
	\item \textbf{LFU (Least-Frequently-Used)}
\end{itemize}

Choosing the LRU strategy is usually a no-brainer. Not only does it 
\textit{seem} more optimal than the other algorithms, but it has also been 
proven, using a Bayesian statistic model, that no other algorithm that tracks 
the last K references to an entry can be more optimal.
%Also, check out this paper:
%An optimality proof of the LRU-K page replacement algorithm
%that proves that no algorithm that keeps track of the K most recent references 
%for a page can be more optimal than LRU.



\section{Comparison between existing methodologies}

\subsection{Block-devices}

There have been many block device tiers that have been built


\subsubsection{Bcache}

Bcache has been designed by Kent Overstreet since 2011 and has been included in 
the Linux kernel (3.10) since the May of 2013.

Bcache allows one to use one or more fast mediums as a cache for slower ones.  
Typically, the slow medium is a RAID array of hard disks and the fast medium 
are SSD drives. Bcache has been specifically built for SSDs and has the 
following characteristics:

\begin{enumerate}
	\item The data are written sequentially and in erase block size 
		granularity, in order to avoid the costly read-erase-modify-write 
		cycle.
	\item It takes special care to mitigate wear-leveling by touching equally 
		all SSD cells
	\item It honors TRIM requests and uses them as hints for its garbage 
		collection.
\end{enumerate}

On to the technical part, bcache is a kernel driver that needs intrusive 
changes to the backing device, rendering previous data unreadable. On a 
nutshell, bcache edits the superblock of both the cache and backing devices in 
order to use them. Then, it exposes to the users a virtual block device, which 
can be formatted to any filesystem. This virtual device's request function 
actually points to the cache device and, when the cache device has reached its 
full utilization, it will flush dirty data to the backing device.

The most striking bcache feature is that it uses a custom built B+tree as an 
index, which has the added benefit that dirty data can be coalesced and flushed 
sequentially to the slower spinning medium. This provides a considerable 
performance speed-up for hard disks since. Some of other noteworthy features of 
bcache are the following:

\begin{enumerate}
	\item It can operate in three modes, write-through, write-back and 
		write-around, which can be switched on/off arbitrarily during normal 
		usage or when the fast medium is congested.
	\item It utilizes a journal log of outstanding writes so that the data are 
		safe, even when an unclean shutdown occurs.
	\item It can bypass sequential IO and send it directly to the backing 
		device, since this workload is tailored for spinning disks.
\end{enumerate}

\subsection{Key-value store}

\subsubsection{Memcached}

\subsection{NoSQL Databases}

\subsubsection{Couchbase}

\subsubsection{Redis}

\subsection{Summary}





